{
  "active_class": "cls_1764299612",
  "uploads": [],
  "recordings": [],
  "b696732e-0fee-4da3-bd19-8d593c794e47": {
    "created_at": "2025-11-28T17:11:26.583165",
    "files": [
      {
        "name": "383Final.pdf",
        "type": "document",
        "text": "\n\n\n\n\n\n\n\n\n\n\n\n",
        "added_at": "2025-11-28T17:11:26.593976"
      }
    ]
  },
  "2374118e-773f-46bb-82d0-a36a05377ac9": {
    "created_at": "2025-11-28T23:21:33.239197",
    "files": [
      {
        "name": "383Final.pdf",
        "type": "document",
        "text": "\n\n\n\n\n\n\n\n\n\n\n\n",
        "added_at": "2025-11-28T23:21:33.254253"
      }
    ]
  },
  "c1ad976b-8e00-4168-854f-a15a5cd90e1b": {
    "created_at": "2025-11-28T23:31:46.558654",
    "files": [
      {
        "name": "COMPSCI_514_HW_4.pdf",
        "type": "document",
        "text": "COMPSCI 514 HW 4\nHelektra Katsoulakis\nNovember 2025\nCore Competency Problems\n1. Rank One Matrices and Matrix Completion (10 points)\n1. (2 points) Suppose A \u2208Rn\u00d7d is a rank-1 matrix, i.e., all rows are multiples of some row\nvector y\u22a4. Without loss of generality, you may assume that y is a unit vector. Prove, without\nappealing to the existence of the SVD, that A can be written as \u03b1xy\u22a4for some unit vector\nx \u2208Rn. Specifically, express \u03b1 and each entry of x in terms of entries of A and y.\n2. (2 points) If A = \u03b1xy\u22a4, find the eigenvector of A\u22a4A with the largest eigenvalue and derive an\nexpression for this largest eigenvalue. To get full marks you need to fully explain your work.\n3. (2 points) Let B \u2208Rn\u00d7d be a partially observed matrix, i.e., we know the values of some of\nthe entries but not others. We say rows i, j \u2208[n] of B are directly comparable if there exists\nk such that Bi,k and Bj,k are both known. Prove that the unobserved entries of B can be\ndeduced from the observed entries of B given the following four assumptions:\n\u2022 B has rank 1.\n\u2022 All observed entries are non-zero.\n\u2022 There is at least one observed entry in each column.\n\u2022 For all i \u2208{1, 2, . . . , n \u22121}, the ith and (i + 1)st rows are directly comparable.\n4. (2 points) Suppose each entry of B is observed independently with probability p. Prove that\nif\np \u2265log(cd)\nn\nfor some sufficiently large constant c then with probability at least 9/10, there exists an\nobserved entry in every column.\n5. (2 points) Suppose each entry of B is observed independently with probability p. Prove that\nif\np \u2265log(cn)\n\u221a\nd\nfor some sufficiently large constant c then with probability at least 9/10, for all i \u2208{1, 2, . . . , n\u2212\n1}, the ith and (i + 1)st rows are directly comparable. Note: It is also possible to prove that\np \u2265\nr\nlog(cn)\nd\nsuffices. This is only stronger than the bound we are asking for so will receive full credit.\n1\n\nSolution:\n1. Since every row of A is a scalar multiple of y\u22a4, for each i there exists a number ci such that\nAi,: = ci y\u22a4.\nTaking the inner product of the i-th row with y gives\nAi,:y = (ciy\u22a4)y = ci (y\u22a4y) = ci,\nbecause y is a unit vector. Therefore each coefficient is\nci = Ai,:y =\nd\nX\nj=1\nAijyj,\nand the vector z = (c1, . . . , cn)\u22a4is exactly Ay. This shows that\nA = zy\u22a4= (Ay) y\u22a4.\nSince A is rank-1 and nonzero, the vector Ay is not the zero vector. Let\n\u03b1 = \u2225Ay\u22252 =\nv\nu\nu\nu\nt\nn\nX\ni=1\n\uf8eb\n\uf8ed\nd\nX\nj=1\nAijyj\n\uf8f6\n\uf8f8\n2\n,\nx =\nAy\n\u2225Ay\u22252\n.\nThen x is a unit vector by construction, and substituting into the expression for A gives\nA = (Ay) y\u22a4= \u03b1xy\u22a4.\nFinally, each entry of x is\nxi = (Ay)i\n\u2225Ay\u22252\n=\nd\nX\nj=1\nAijyj\nv\nu\nu\nt\nn\nX\nk=1\n d\nX\n\u2113=1\nAk\u2113y\u2113\n!2 .\nWith these definitions of \u03b1 and x, we have shown that\nA = \u03b1xy\u22a4\nfor some unit vector x \u2208Rn.\n2. First compute\nA\u22a4= (\u03b1xy\u22a4)\u22a4= \u03b1yx\u22a4.\nThen\nA\u22a4A = (\u03b1yx\u22a4)(\u03b1xy\u22a4) = \u03b12 y(x\u22a4x) y\u22a4.\n2\n\nSince x is a unit vector, x\u22a4x = \u2225x\u22252\n2 = 1, so this simplifies to\nA\u22a4A = \u03b12 yy\u22a4.\nThis is a rank-1 matrix in Rd\u00d7d with range equal to the span of y.\nNow check what A\u22a4A does to the vector y:\nA\u22a4A y = \u03b12 yy\u22a4y = \u03b12 (y\u22a4y) y = \u03b12 \u00b7 1 \u00b7 y = \u03b12y,\nusing again that y\u22a4y = 1. This shows that y is an eigenvector of A\u22a4A with eigenvalue \u03b12.\nNext, take any vector z \u2208Rd that is orthogonal to y, so y\u22a4z = 0. Then\nA\u22a4A z = \u03b12 yy\u22a4z = \u03b12 y \u00b7 0 = 0.\nThus every vector orthogonal to y is an eigenvector with eigenvalue 0. Therefore the eigen-\nvalues of A\u22a4A are\n\u03bbmax = \u03b12,\nand all remaining eigenvalues are 0.\nThe eigenvector corresponding to the largest eigenvalue \u03b12 is any nonzero multiple of y, and\nif we want a unit eigenvector, we can simply take y itself.\n3. Since B has rank 1, it can be written as\nB = uv\u22a4,\nso each entry factors as\nBij = uivj.\nThis means every row is a scalar multiple of the same vector v, and every column is a scalar\nmultiple of the same vector u.\nFirst, notice that under the assumptions, every row and every column must contain at least\none observed (and nonzero) entry. Because all observed entries satisfy Bij = uivj \u0338= 0, we\nimmediately get that ui \u0338= 0 and vj \u0338= 0 whenever Bij is observed. Since each row participates\nin at least one directly comparable pair, every row has at least one known, nonzero entry, and\ntherefore every ui is nonzero. Similarly, since every column has at least one observed entry,\nevery vj is nonzero as well.\nNow consider two directly comparable rows i and j. By definition, there exists a column k\nsuch that both Bik and Bjk are observed. Using the rank-1 structure,\nBik = uivk,\nBjk = ujvk.\nSince vk \u0338= 0, we divide and get\nBik\nBjk\n= uivk\nujvk\n= ui\nuj\n.\nThus one shared observed entry gives us the ratio ui/uj.\n3\n\nBecause every adjacent pair (i, i + 1) is directly comparable, we can recover all ratios\nu2\nu1\n,\nu3\nu2\n,\n. . . ,\nun\nun\u22121\n.\nMultiplying these together lets us express each ui in terms of u1:\nui\nu1\n=\ni\u22121\nY\nt=1\nut+1\nut\n.\nChoosing any nonzero value for u1 determines every ui uniquely up to an overall scale factor.\nNext, since every column j has at least one observed entry, pick any row i such that Bij is\nknown. Using Bij = uivj and the fact that ui is now known, we can solve for\nvj = Bij\nui\n.\nThis determines every vj.\nFinally, once u and v are known, all entries of B including are determined by\nBij = uivj.\nAfter solving for all of the ui and vj, every entry of B is shown to be the product uivj. Any\ndifferent choice for a missing entry would change this product and would no longer match the\nobserved values, so the missing entries are completely fixed.\n4. Fix a particular column j \u2208{1, . . . , d}.\nThere are n entries in this column, and each is\nobserved independently with probability p. The event that column j has no observed entries\nmeans that all n entries in that column are unobserved, which has probability\nP(column j has no observed entries) = (1 \u2212p)n.\nWe now bound this probability using the inequality 1\u2212p \u2264e\u2212p valid for all real p. This gives\n(1 \u2212p)n \u2264e\u2212pn.\nIf p \u2265log(cd)\nn\n, then\npn \u2265log(cd),\nso\n(1 \u2212p)n \u2264e\u2212pn \u2264e\u2212log(cd) = 1\ncd.\nThus for each fixed column j,\nP(column j has no observed entries) \u22641\ncd.\nWe now apply a union bound over all d columns. The event that at least one column has no\nobserved entries is the union of the events that column j has no observed entries, and so\nP(\u2203a column with no observed entries) \u2264\nd\nX\nj=1\nP(column j has no observed entries) \u2264d\u00b7 1\ncd = 1\nc.\n4\n\nTherefore\nP(every column has at least one observed entry) = 1\u2212P(\u2203a column with no observed entries) \u22651\u22121\nc.\nIf we choose c \u226510, then 1 \u22121\nc \u22651 \u22121\n10 = 9\n10. Hence, for such a constant c, whenever\np \u2265log(cd)\nn\n,\nwith probability at least 9/10 every column has at least one observed entry.\n5. Fix a particular pair of adjacent rows, rows i and i + 1 for some i \u2208{1, . . . , n \u22121}. For\neach column j \u2208{1, . . . , d}, consider the event that both entries Bi,j and Bi+1,j are observed.\nSince each entry is observed independently with probability p, the probability that both are\nobserved in column j is\nP(both Bi,j and Bi+1,j observed) = p2.\nDefine Xi to be the number of columns in which both entries of rows i and i+1 are observed.\nThen Xi is a binomial random variable,\nXi \u223cBinomial(d, p2).\nThe rows i and i + 1 are directly comparable exactly when Xi \u22651, that is, when there is at\nleast one column where both entries are observed. So we want to bound\nP(Xi = 0) = P(rows i and i + 1 are not directly comparable).\nSince Xi \u223cBinomial(d, p2), we have\nP(Xi = 0) = (1 \u2212p2)d.\nAs before, we use the inequality 1 \u2212x \u2264e\u2212x for x \u22650:\n(1 \u2212p2)d \u2264e\u2212p2d.\nNow assume\np \u2265log(cn)\n\u221a\nd\n.\nThen\np2 \u2265(log(cn))2\nd\n,\nand hence\np2d \u2265(log(cn))2.\nTherefore\nP(Xi = 0) \u2264e\u2212p2d \u2264e\u2212(log(cn))2.\n5\n\nSince (log(cn))2 \u2265log(cn) whenever log(cn) \u22651, we have\ne\u2212(log(cn))2 \u2264e\u2212log(cn) = 1\ncn\nfor all sufficiently large n (and fixed c > 1). Thus, for each fixed adjacent pair (i, i + 1),\nP(rows i and i + 1 are not directly comparable) = P(Xi = 0) \u22641\ncn.\nNow we apply a union bound over the n \u22121 adjacent pairs of rows. The event that there\nexists at least one pair (i, i + 1) that is not directly comparable has probability at most\nP(\u2203i \u2208{1, . . . , n\u22121} such that rows i and i+1 are not directly comparable) \u2264\nn\u22121\nX\ni=1\n1\ncn \u2264n \u22121\ncn\n\u22641\nc.\nTherefore\nP(all adjacent pairs of rows are directly comparable) \u22651 \u22121\nc.\nIf we choose c \u226510, then 1 \u22121\nc \u22659/10. Hence, for such a constant c, whenever\np \u2265log(cn)\n\u221a\nd\n,\nwith probability at least 9/10 every pair of adjacent rows (i, i + 1) is directly comparable.\n2. SVD Practice (10 points)\nConsider any matrix A \u2208Rn\u00d7d.\n1. (2 points) Let v be a unit norm eigenvector of A\u22a4A with eigenvalue \u03bb. Prove that \u03bb \u22650 and\nthat \u2225Av\u22252 =\n\u221a\n\u03bb.\n2. (2 points) Let v1, v2 be two eigenvectors of A\u22a4A that are orthogonal to each other. Prove\nthat Av1 and Av2 are also orthogonal to each other.\n3. (2 points) Let V \u2208Rd\u00d7d contain the d eigenvectors of A\u22a4A as its columns and let \u039b \u2208Rd\u00d7d\ncontain their corresponding eigenvalues. Assume A\u22a4A is full rank and so all of its eigenvalues\nare positive. Prove that U = AV \u039b\u22121/2 has orthonormal columns. Hint: Apply parts (1) and\n(2).\n4. (2 points) Prove that if V \u2208Rd\u00d7d has orthonormal columns then V V \u22a4= I. Conclude using\npart (3) that we can write\nA = U\u039b1/2V \u22a4,\nwhere U and V have orthonormal columns and \u039b is diagonal. Hint: Use the interpretation\nof V V T as a projection matrix in your proof.\n5. (2 points) Let v be an eigenvector of A\u22a4A with eigenvalue \u03bb. Prove that Av is an eigenvector\nof AA\u22a4with eigenvalue \u03bb. Conclude that the columns of U from part (4) are eigenvectors of\nAA\u22a4.\n6\n\nSolution:\n1. Let v be a unit eigenvector of A\u22a4A with eigenvalue \u03bb, so\nA\u22a4Av = \u03bbv\nand\n\u2225v\u22252 = 1.\nMultiply both sides on the left by v\u22a4:\nv\u22a4A\u22a4Av = \u03bb v\u22a4v = \u03bb.\nWe also see that\nv\u22a4A\u22a4Av = (Av)\u22a4(Av) = \u2225Av\u22252\n2.\nPutting these together,\n\u2225Av\u22252\n2 = \u03bb.\nA squared norm is always nonnegative, so \u03bb = \u2225Av\u22252\n2 \u22650. Taking square roots gives\n\u2225Av\u22252 =\n\u221a\n\u03bb.\n2. Suppose v1 and v2 are eigenvectors of A\u22a4A with\nA\u22a4Av1 = \u03bb1v1,\nA\u22a4Av2 = \u03bb2v2,\nand assume v1 and v2 are orthogonal, so v\u22a4\n1 v2 = 0.\nWe want to show that Av1 and Av2 are also orthogonal. Consider their inner product:\n\u27e8Av1, Av2\u27e9= (Av1)\u22a4(Av2) = v\u22a4\n1 A\u22a4Av2.\nSince v2 is an eigenvector of A\u22a4A with eigenvalue \u03bb2, we have\nA\u22a4Av2 = \u03bb2v2,\nso\nv\u22a4\n1 A\u22a4Av2 = v\u22a4\n1 (\u03bb2v2) = \u03bb2 v\u22a4\n1 v2 = \u03bb2 \u00b7 0 = 0.\nTherefore \u27e8Av1, Av2\u27e9= 0, which shows that Av1 and Av2 are orthogonal.\n3. Let V \u2208Rd\u00d7d have as its columns the eigenvectors of A\u22a4A, and let\nA\u22a4AV = V \u039b\nwhere \u039b is the diagonal matrix of eigenvalues. Because A\u22a4A is symmetric and full rank, its\neigenvalues are positive and its eigenvectors can be chosen to be orthonormal. This means\nthe columns of V form an orthonormal basis of Rd and\nV \u22a4V = I.\nWe are given U = AV \u039b\u22121/2 and we want to show the columns of U are orthonormal. We\nthen compute U\u22a4U as such:\nU\u22a4U = (\u039b\u22121/2)\u22a4V \u22a4A\u22a4AV \u039b\u22121/2.\n7\n\nSince \u039b is diagonal with positive entries, \u039b\u22121/2 is also diagonal and symmetric, so (\u039b\u22121/2)\u22a4=\n\u039b\u22121/2. Using A\u22a4AV = V \u039b, we get\nV \u22a4A\u22a4AV = V \u22a4(V \u039b) = (V \u22a4V )\u039b = I\u039b = \u039b.\nTherefore\nU\u22a4U = \u039b\u22121/2 \u039b \u039b\u22121/2 = I.\nSo U\u22a4U = I, which means the columns of U are orthonormal.\n4. Suppose V \u2208Rd\u00d7d has orthonormal columns. V is an orthogonal matrix and both\nV \u22a4V = I\nand\nV V \u22a4= I.\nFrom part (3), we know that\nU = AV \u039b\u22121/2\n=\u21d2\nAV = U\u039b1/2.\nMultiply both sides on the right by V \u22a4:\nAV V \u22a4= U\u039b1/2V \u22a4.\nUsing V V \u22a4= I, the left-hand side becomes simply A, so\nA = U\u039b1/2V \u22a4,\nwhere U and V both have orthonormal columns and \u039b is diagonal with positive entries. This\nis exactly the singular value decomposition of A, with \u039b1/2 containing the singular values.\n5. Let v be an eigenvector of A\u22a4A with eigenvalue \u03bb, so\nA\u22a4Av = \u03bbv.\nApply A on the left:\nAA\u22a4(Av) = A(A\u22a4Av) = A(\u03bbv) = \u03bb(Av).\nThis shows that Av is an eigenvector of AA\u22a4with the same eigenvalue \u03bb, where Av \u0338= 0. In\na full-rank setting (where all eigenvalues of A\u22a4A are positive), part (1) tells us that\n\u2225Av\u22252\n2 = \u03bb > 0,\nso Av is nonzero and is a valid eigenvector.\nNow look at the columns of U from part (4). Part (3) defined\nU = AV \u039b\u22121/2,\nwhere V = [v1 v2 \u00b7 \u00b7 \u00b7 vd] contains the eigenvectors of A\u22a4A and\n\u039b\u22121/2 = diag\n\u0012 1\n\u221a\u03bb1\n, . . . ,\n1\n\u221a\u03bbd\n\u0013\n.\n8\n\nMultiplying AV by this diagonal matrix rescales each column. Therefore the jth column of\nU is\nuj =\n1\np\n\u03bbj\nAvj.\nwhere vj is the corresponding eigenvector of A\u22a4A with eigenvalue \u03bbj. As we just showed,\nAvj is an eigenvector of AA\u22a4with eigenvalue \u03bbj, and scaling an eigenvector by a nonzero\nconstant does not change the eigenvalue or the eigenspace.\nTherefore each uj is also an\neigenvector of AA\u22a4with eigenvalue \u03bbj. This means the columns of U form an orthonormal\nset of eigenvectors of AA\u22a4.\n3. Eigendecomposition and Optimal Low-Rank Approximation (10 points)\nConsider any matrix A \u2208Rn\u00d7d.\n1. (2 points) Prove that\narg\nmin\nZ\u2208Rd\u00d7k:Z\u22a4Z=I \u2225A \u2212AZZ\u22a4\u22252\nF = arg\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ).\n2. (2 points) Let V \u2208Rd\u00d7d have orthonormal columns. Prove that for any Z \u2208Rd\u00d7k with\northonormal columns, V \u22a4Z \u2208Rd\u00d7k also has orthonormal columns. Further prove that any\nZ \u2208Rd\u00d7k with orthonormal columns can be written as Z = V \u22a4U for some U \u2208Rd\u00d7k with\northonormal columns. Hint: Use Problem 2.4.\n3. (2 points) Writing A\u22a4A = V \u039bV \u22a4in its eigendecomposition, use part (2) to prove that\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ) =\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4\u039bZ).\n4. (2 points) Prove that for Z \u2208Rd\u00d7k with orthonormal columns,\ntr(ZZ\u22a4) =\nd\nX\ni=1\n(ZZ\u22a4)ii = k,\nand 0 \u2264(ZZ\u22a4)i,i \u22641 for all i \u2208[d]. Hint: There are several ways to prove the second claim.\nOne is by using Problem 2.4 again.\n(5) (2 points) Use (3) and (4) to show that\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ) =\nk\nX\ni=1\n\u03bbi(A\u22a4A).\nConclude that Z optimizing the equation in part (1) has as its columns the k eigenvectors of\nA\u22a4A corresponding to the top eigenvalues \u03bb1(A\u22a4A), ..., \u03bbk(A\u22a4A).\n9\n\nSolution:\n1. We begin by expanding the Frobenius norm:\n\u2225A \u2212AZZ\u22a4\u22252\nF = tr\n\u0010\n(A \u2212AZZ\u22a4)\u22a4(A \u2212AZZ\u22a4)\n\u0011\n.\nDistributing the product gives\ntr(A\u22a4A) \u22122 tr(A\u22a4AZZ\u22a4) + tr(ZZ\u22a4A\u22a4AZZ\u22a4).\nUsing the cyclic property of the trace and Z\u22a4Z = I, the last term simplifies:\ntr(ZZ\u22a4A\u22a4AZZ\u22a4) = tr(Z\u22a4A\u22a4AZ).\nThus the expression becomes\n\u2225A \u2212AZZ\u22a4\u22252\nF = tr(A\u22a4A) \u2212tr(Z\u22a4A\u22a4AZ).\nThe first term does not depend on Z, so minimizing the Frobenius error is equivalent to\nmaximizing\ntr(Z\u22a4A\u22a4AZ).\nwhich proves\narg\nmin\nZ\u2208Rd\u00d7k:Z\u22a4Z=I \u2225A \u2212AZZ\u22a4\u22252\nF = arg\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ).\n2. Let V have orthonormal columns, so V \u22a4V = I. If Z also has orthonormal columns, then\n(V \u22a4Z)\u22a4(V \u22a4Z) = Z\u22a4V V \u22a4Z.\nBecause V V \u22a4= I for an orthonormal basis of Rd, we get\n(V \u22a4Z)\u22a4(V \u22a4Z) = Z\u22a4Z = I.\nSo V \u22a4Z also has orthonormal columns.\nTo see why every Z with orthonormal columns can be written as Z = V \u22a4U, start by defining\nU = V Z.\nSince V has orthonormal columns, multiplying by V does not change inner products\nU\u22a4U = Z\u22a4V \u22a4V Z = Z\u22a4Z = I,\nso U also has orthonormal columns. Solving for Z gives\nZ = V \u22a4U.\n10\n\n3. Using A\u22a4A = V \u039bV \u22a4, we substitute:\ntr(Z\u22a4A\u22a4AZ) = tr(Z\u22a4V \u039bV \u22a4Z).\nFrom part (2), whenever Z has orthonormal columns, we can write Z = V \u22a4U with U also\northonormal. Substituting,\ntr(Z\u22a4A\u22a4AZ) = tr(U\u22a4\u039bU).\nBecause the mapping Z 7\u2192U is a bijection (Z \u2190\u2192U = V Z) on matrices with orthonormal\ncolumns, we get\nmax\nZ\u22a4Z=I tr(Z\u22a4A\u22a4AZ) = max\nU\u22a4U=I tr(U\u22a4\u039bU) = max\nZ\u22a4Z=I tr(Z\u22a4\u039bZ).\n4. Since Z has orthonormal columns,\nZ\u22a4Z = I.\nUsing the cyclic property of trace,\ntr(ZZ\u22a4) = tr(Z\u22a4Z) = tr(I) = k.\nSince ZZ\u22a4is a projection matrix onto the column space of Z, each diagonal entry satisfies\n0 \u2264(ZZ\u22a4)i,i \u22641.\nThe lower bound holds because a projection cannot produce negative values on the diagonal.\nFor the upper bound, projecting any vector can only make its length shorter or keep it the\nsame, so the squared length of each coordinate direction cannot grow. This means every\ndiagonal entry of ZZ\u22a4must be at most 1.\n5. From part (3), the expression we want to maximize can be written in the diagonal basis as\ntr(Z\u22a4\u039bZ).\nSince \u039b is diagonal with entries \u03bb1 \u2265\u03bb2 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbd, we can expand its trace:\ntr(Z\u22a4\u039bZ) =\nd\nX\ni=1\n\u03bbi(ZZ\u22a4)i,i.\nPart (4) tells us two facts about the diagonal entries of ZZ\u22a4:\nd\nX\ni=1\n(ZZ\u22a4)i,i = k,\n0 \u2264(ZZ\u22a4)i,i \u22641\nfor all i.\nThis means that the diagonal of ZZ\u22a4behaves like a collection of d numbers between 0 and\n1 whose total adds up to k.\nLooking at the quantity we are trying to maximize:\nd\nX\ni=1\n\u03bbi(ZZ\u22a4)i,i.\n11\n\nEach diagonal entry (ZZ\u22a4)i,i multiplies the eigenvalue \u03bbi. Since the \u03bbi\u2019s are sorted from\nlargest to smallest, this weighted sum becomes largest when the k units of weight are placed\non the coordinates with the biggest eigenvalues.\nBecause each (ZZ\u22a4)i,i is at most 1, the best we can do is assign value 1 to the first k diagonal\nentries (corresponding to \u03bb1, . . . , \u03bbk) and assign value 0 to all remaining entries. Any other\nchoice would place some weight on a smaller eigenvalue and reduce the total.\nWith this choice, the trace becomes\nk\nX\ni=1\n\u03bbi.\nSo,\nmax\nZ\u22a4Z=I tr(Z\u22a4\u039bZ) =\nk\nX\ni=1\n\u03bbi.\nC2. More Matrix Completion (10 points)\nConsider an n\u00d7d matrix B where each entry is independently observed with probability p. Assume\nd \u2264n.\n1. (2 points) Prove that if:\n\u2022 B has rank 1.\n\u2022 All observed entries are non-zero.\n\u2022 There is at least one observed entry in each column.\n\u2022 No subset of rows is isolated.\nthen B can be reconstructed from the observed entries.\n2. (2 points) Prove that a subset of rows U is isolated with probability at most\nh\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|id\n.\n3. (2 points) Prove that\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U| \u2264exp(\u2212p|U|/2)\nassuming 1 \u2264|U| \u2264n/2 and p \u2265c ln(n)/d for sufficiently large c. Hint: First show\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U| \u2264exp(\u2212p|U| + e\u2212p(n\u22122|U|))\n4. (2 points) Prove that the probability there exists an isolated subset of nodes is at most 1/n\nif p \u2265c ln(n)/d for sufficiently large c. Hint: Use the union bound.\n5. (2 points) Describe an n\u00d7n matrix C with rank 1 such that the probability we can reconstruct\nC is at most 1/2n even if each entry is independently observed with probability 1/2 and\nobservations are independent.\nHint: C has zero entries otherwise p = c(logn)/n would\nsuffice for reconstruction with high probability.\n12\n\nSolution:\n1. Since B has rank 1, there exist vectors u \u2208Rn and v \u2208Rd such that\nB = uv\u22a4\nso\nBij = uivj.\nTo better visualize how the rows relate to each other, we can form a graph whose nodes are\nthe rows of B. Two rows i and j are connected by an edge if they are directly comparable,\nmeaning they share an observed entry in at least one column. If some subset of rows were cut\noff from the rest of the graph, that subset would be isolated, which is ruled out by assumption.\nTherefore this row-graph is connected.\nIf (i, j) is an edge, then there exists a column k with both Bik and Bjk observed. Using\nBik = uivk and Bjk = ujvk and the fact that all observed entries are nonzero, we get\nui\nuj\n= Bik\nBjk\n.\nThus each edge gives a ratio between two entries of u.\nBecause the row-graph is connected, for any row r there is a path\n1 = r0, r1, . . . , rt = r.\nMultiplying ratios along this path gives\nu1\nur\n=\nt\u22121\nY\ns=0\nurs\nurs+1\n=\nt\u22121\nY\ns=0\nBrs,ks\nBrs+1,ks\n.\nChoosing u1 = 1 determines every ur.\nEach column j has at least one observed entry Bi(j),j. Using Bi(j),j = ui(j)vj, we then get\nvj = Bi(j),j\nui(j)\n.\nAll ui and vj are now known, so every entry of B follows from\nBij = uivj.\nThus B can be reconstructed from the observed entries.\n2. Fix a subset of rows U \u2286[n]. In one column j, U is \u201cdisconnected\u201d from its complement Uc\nif either:\n\u2022 no entry in U is observed in column j, or\n\u2022 no entry in Uc is observed in column j.\nThe probability of the first event is (1 \u2212p)|U|, and of the second is (1 \u2212p)n\u2212|U|. By the union\nbound,\nPr(column j does not connect U and Uc) \u2264(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|.\nColumns are independent, so for all d columns,\nPr(U is isolated) \u2264\nh\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|id\n.\n13\n\n3. Let s = |U| with 1 \u2264s \u2264n/2. Set\nx = (1 \u2212p)s,\ny = (1 \u2212p) n\u2212s = (1 \u2212p) s+(n\u22122s) = x \u00b7 (1 \u2212p) n\u22122s.\nThen\nx + y = x\n\u00001 + (1 \u2212p) n\u22122s\u0001\n.\nUsing the inequality (1 \u2212p)k \u2264e\u2212pk for any k \u22650,\nx = (1 \u2212p)s \u2264e\u2212ps,\n(1 \u2212p) n\u22122s \u2264e\u2212p(n\u22122s),\nso\n(1 \u2212p)s + (1 \u2212p) n\u2212s \u2264e\u2212ps\u00001 + e\u2212p(n\u22122s)\u0001\n.\nFor any z \u22650, 1 + z \u2264ez, giving\n(1 \u2212p)s + (1 \u2212p) n\u2212s \u2264exp\n\u0000\u2212ps + e\u2212p(n\u22122s)\u0001\n.\nNow assume p \u2265c ln(n)/d with c large. If 1 \u2264s \u2264n/2 then n \u22122s \u22650, so e\u2212p(n\u22122s) is\nvery small. For large enough c, the quantity e\u2212p(n\u22122s) becomes smaller than ps\n2 for all such\ns. Substituting this gives\n\u2212ps + e\u2212p(n\u22122s) \u2264\u2212ps + ps\n2 = \u2212ps\n2 ,\nand therefore\n(1 \u2212p)|U| + (1 \u2212p) n\u2212|U| \u2264exp(\u2212p|U|/2).\n4. We want to bound the probability that there exists at least one isolated subset of rows. By\nsymmetry between a subset U and its complement U c, it is enough to consider subsets with\n1 \u2264|U| \u2264\u230an/2\u230b.\nFrom part (3), for any subset U with s = |U| and 1 \u2264s \u2264n/2, the probability that U is\nisolated satisfies\nPr(U is isolated) \u2264\nh\n(1 \u2212p)s + (1 \u2212p)n\u2212sid\n\u2264exp\n\u0012\n\u2212psd\n2\n\u0013\n.\nUsing the union bound over all nonempty U with |U| \u2264n/2,\nPr(\u2203U isolated) \u2264\n\u230an/2\u230b\nX\ns=1\n\u0012n\ns\n\u0013\nexp\n\u0012\n\u2212psd\n2\n\u0013\n.\nUsing\n\u0000n\ns\n\u0001\n\u2264ns and the assumption p \u2265c ln(n)/d, we get\n\u0012n\ns\n\u0013\nexp\n\u0012\n\u2212psd\n2\n\u0013\n\u2264ns exp\n\u0012\n\u2212cs ln n\n2\n\u0013\n= ns\u2212cs\n2 = n\u2212s( c\n2 \u22121).\nIf c is large enough so that c\n2 \u22121 \u22652, then each term is at most n\u22122s, and\nPr(\u2203U isolated) \u2264\n\u230an/2\u230b\nX\ns=1\nn\u22122s \u2264\n\u221e\nX\ns=1\nn\u22122s =\nn\u22122\n1 \u2212n\u22122 \u22641\nn\nfor all n \u22652. Therefore, for p \u2265c ln(n)/d with c sufficiently large, the probability that there\nexists an isolated subset of rows is at most 1/n.\n14\n\n5. Let C be an n \u00d7 n rank-1 matrix with a single nonzero row. For example, take\nu = (1, 0, 0, . . . , 0)\u22a4,\nv \u2208Rn with all entries nonzero,\nand set C = uv\u22a4. Then only the first row of C can be nonzero.\nTo reconstruct C uniquely, we need to know every entry of v, since C1j = vj and all other\nrows are zero. If even one entry in the first row is unobserved, we can change that single\nunobserved value while keeping all observed entries and zeros the same, and obtain a different\nrank-1 matrix consistent with the observations. So reconstruction is only possible if all n\nentries in the first row are observed.\nEach entry is observed independently with probability 1/2, so the probability all n entries in\nthe first row are observed is\n(1/2)n.\nTherefore, the probability we can reconstruct C is at most 1/2n.\n15\n",
        "added_at": "2025-11-28T23:31:46.617572"
      },
      {
        "name": "COMPSCI_514_HW_4.pdf",
        "type": "document",
        "text": "COMPSCI 514 HW 4\nHelektra Katsoulakis\nNovember 2025\nCore Competency Problems\n1. Rank One Matrices and Matrix Completion (10 points)\n1. (2 points) Suppose A \u2208Rn\u00d7d is a rank-1 matrix, i.e., all rows are multiples of some row\nvector y\u22a4. Without loss of generality, you may assume that y is a unit vector. Prove, without\nappealing to the existence of the SVD, that A can be written as \u03b1xy\u22a4for some unit vector\nx \u2208Rn. Specifically, express \u03b1 and each entry of x in terms of entries of A and y.\n2. (2 points) If A = \u03b1xy\u22a4, find the eigenvector of A\u22a4A with the largest eigenvalue and derive an\nexpression for this largest eigenvalue. To get full marks you need to fully explain your work.\n3. (2 points) Let B \u2208Rn\u00d7d be a partially observed matrix, i.e., we know the values of some of\nthe entries but not others. We say rows i, j \u2208[n] of B are directly comparable if there exists\nk such that Bi,k and Bj,k are both known. Prove that the unobserved entries of B can be\ndeduced from the observed entries of B given the following four assumptions:\n\u2022 B has rank 1.\n\u2022 All observed entries are non-zero.\n\u2022 There is at least one observed entry in each column.\n\u2022 For all i \u2208{1, 2, . . . , n \u22121}, the ith and (i + 1)st rows are directly comparable.\n4. (2 points) Suppose each entry of B is observed independently with probability p. Prove that\nif\np \u2265log(cd)\nn\nfor some sufficiently large constant c then with probability at least 9/10, there exists an\nobserved entry in every column.\n5. (2 points) Suppose each entry of B is observed independently with probability p. Prove that\nif\np \u2265log(cn)\n\u221a\nd\nfor some sufficiently large constant c then with probability at least 9/10, for all i \u2208{1, 2, . . . , n\u2212\n1}, the ith and (i + 1)st rows are directly comparable. Note: It is also possible to prove that\np \u2265\nr\nlog(cn)\nd\nsuffices. This is only stronger than the bound we are asking for so will receive full credit.\n1\n\nSolution:\n1. Since every row of A is a scalar multiple of y\u22a4, for each i there exists a number ci such that\nAi,: = ci y\u22a4.\nTaking the inner product of the i-th row with y gives\nAi,:y = (ciy\u22a4)y = ci (y\u22a4y) = ci,\nbecause y is a unit vector. Therefore each coefficient is\nci = Ai,:y =\nd\nX\nj=1\nAijyj,\nand the vector z = (c1, . . . , cn)\u22a4is exactly Ay. This shows that\nA = zy\u22a4= (Ay) y\u22a4.\nSince A is rank-1 and nonzero, the vector Ay is not the zero vector. Let\n\u03b1 = \u2225Ay\u22252 =\nv\nu\nu\nu\nt\nn\nX\ni=1\n\uf8eb\n\uf8ed\nd\nX\nj=1\nAijyj\n\uf8f6\n\uf8f8\n2\n,\nx =\nAy\n\u2225Ay\u22252\n.\nThen x is a unit vector by construction, and substituting into the expression for A gives\nA = (Ay) y\u22a4= \u03b1xy\u22a4.\nFinally, each entry of x is\nxi = (Ay)i\n\u2225Ay\u22252\n=\nd\nX\nj=1\nAijyj\nv\nu\nu\nt\nn\nX\nk=1\n d\nX\n\u2113=1\nAk\u2113y\u2113\n!2 .\nWith these definitions of \u03b1 and x, we have shown that\nA = \u03b1xy\u22a4\nfor some unit vector x \u2208Rn.\n2. First compute\nA\u22a4= (\u03b1xy\u22a4)\u22a4= \u03b1yx\u22a4.\nThen\nA\u22a4A = (\u03b1yx\u22a4)(\u03b1xy\u22a4) = \u03b12 y(x\u22a4x) y\u22a4.\n2\n\nSince x is a unit vector, x\u22a4x = \u2225x\u22252\n2 = 1, so this simplifies to\nA\u22a4A = \u03b12 yy\u22a4.\nThis is a rank-1 matrix in Rd\u00d7d with range equal to the span of y.\nNow check what A\u22a4A does to the vector y:\nA\u22a4A y = \u03b12 yy\u22a4y = \u03b12 (y\u22a4y) y = \u03b12 \u00b7 1 \u00b7 y = \u03b12y,\nusing again that y\u22a4y = 1. This shows that y is an eigenvector of A\u22a4A with eigenvalue \u03b12.\nNext, take any vector z \u2208Rd that is orthogonal to y, so y\u22a4z = 0. Then\nA\u22a4A z = \u03b12 yy\u22a4z = \u03b12 y \u00b7 0 = 0.\nThus every vector orthogonal to y is an eigenvector with eigenvalue 0. Therefore the eigen-\nvalues of A\u22a4A are\n\u03bbmax = \u03b12,\nand all remaining eigenvalues are 0.\nThe eigenvector corresponding to the largest eigenvalue \u03b12 is any nonzero multiple of y, and\nif we want a unit eigenvector, we can simply take y itself.\n3. Since B has rank 1, it can be written as\nB = uv\u22a4,\nso each entry factors as\nBij = uivj.\nThis means every row is a scalar multiple of the same vector v, and every column is a scalar\nmultiple of the same vector u.\nFirst, notice that under the assumptions, every row and every column must contain at least\none observed (and nonzero) entry. Because all observed entries satisfy Bij = uivj \u0338= 0, we\nimmediately get that ui \u0338= 0 and vj \u0338= 0 whenever Bij is observed. Since each row participates\nin at least one directly comparable pair, every row has at least one known, nonzero entry, and\ntherefore every ui is nonzero. Similarly, since every column has at least one observed entry,\nevery vj is nonzero as well.\nNow consider two directly comparable rows i and j. By definition, there exists a column k\nsuch that both Bik and Bjk are observed. Using the rank-1 structure,\nBik = uivk,\nBjk = ujvk.\nSince vk \u0338= 0, we divide and get\nBik\nBjk\n= uivk\nujvk\n= ui\nuj\n.\nThus one shared observed entry gives us the ratio ui/uj.\n3\n\nBecause every adjacent pair (i, i + 1) is directly comparable, we can recover all ratios\nu2\nu1\n,\nu3\nu2\n,\n. . . ,\nun\nun\u22121\n.\nMultiplying these together lets us express each ui in terms of u1:\nui\nu1\n=\ni\u22121\nY\nt=1\nut+1\nut\n.\nChoosing any nonzero value for u1 determines every ui uniquely up to an overall scale factor.\nNext, since every column j has at least one observed entry, pick any row i such that Bij is\nknown. Using Bij = uivj and the fact that ui is now known, we can solve for\nvj = Bij\nui\n.\nThis determines every vj.\nFinally, once u and v are known, all entries of B including are determined by\nBij = uivj.\nAfter solving for all of the ui and vj, every entry of B is shown to be the product uivj. Any\ndifferent choice for a missing entry would change this product and would no longer match the\nobserved values, so the missing entries are completely fixed.\n4. Fix a particular column j \u2208{1, . . . , d}.\nThere are n entries in this column, and each is\nobserved independently with probability p. The event that column j has no observed entries\nmeans that all n entries in that column are unobserved, which has probability\nP(column j has no observed entries) = (1 \u2212p)n.\nWe now bound this probability using the inequality 1\u2212p \u2264e\u2212p valid for all real p. This gives\n(1 \u2212p)n \u2264e\u2212pn.\nIf p \u2265log(cd)\nn\n, then\npn \u2265log(cd),\nso\n(1 \u2212p)n \u2264e\u2212pn \u2264e\u2212log(cd) = 1\ncd.\nThus for each fixed column j,\nP(column j has no observed entries) \u22641\ncd.\nWe now apply a union bound over all d columns. The event that at least one column has no\nobserved entries is the union of the events that column j has no observed entries, and so\nP(\u2203a column with no observed entries) \u2264\nd\nX\nj=1\nP(column j has no observed entries) \u2264d\u00b7 1\ncd = 1\nc.\n4\n\nTherefore\nP(every column has at least one observed entry) = 1\u2212P(\u2203a column with no observed entries) \u22651\u22121\nc.\nIf we choose c \u226510, then 1 \u22121\nc \u22651 \u22121\n10 = 9\n10. Hence, for such a constant c, whenever\np \u2265log(cd)\nn\n,\nwith probability at least 9/10 every column has at least one observed entry.\n5. Fix a particular pair of adjacent rows, rows i and i + 1 for some i \u2208{1, . . . , n \u22121}. For\neach column j \u2208{1, . . . , d}, consider the event that both entries Bi,j and Bi+1,j are observed.\nSince each entry is observed independently with probability p, the probability that both are\nobserved in column j is\nP(both Bi,j and Bi+1,j observed) = p2.\nDefine Xi to be the number of columns in which both entries of rows i and i+1 are observed.\nThen Xi is a binomial random variable,\nXi \u223cBinomial(d, p2).\nThe rows i and i + 1 are directly comparable exactly when Xi \u22651, that is, when there is at\nleast one column where both entries are observed. So we want to bound\nP(Xi = 0) = P(rows i and i + 1 are not directly comparable).\nSince Xi \u223cBinomial(d, p2), we have\nP(Xi = 0) = (1 \u2212p2)d.\nAs before, we use the inequality 1 \u2212x \u2264e\u2212x for x \u22650:\n(1 \u2212p2)d \u2264e\u2212p2d.\nNow assume\np \u2265log(cn)\n\u221a\nd\n.\nThen\np2 \u2265(log(cn))2\nd\n,\nand hence\np2d \u2265(log(cn))2.\nTherefore\nP(Xi = 0) \u2264e\u2212p2d \u2264e\u2212(log(cn))2.\n5\n\nSince (log(cn))2 \u2265log(cn) whenever log(cn) \u22651, we have\ne\u2212(log(cn))2 \u2264e\u2212log(cn) = 1\ncn\nfor all sufficiently large n (and fixed c > 1). Thus, for each fixed adjacent pair (i, i + 1),\nP(rows i and i + 1 are not directly comparable) = P(Xi = 0) \u22641\ncn.\nNow we apply a union bound over the n \u22121 adjacent pairs of rows. The event that there\nexists at least one pair (i, i + 1) that is not directly comparable has probability at most\nP(\u2203i \u2208{1, . . . , n\u22121} such that rows i and i+1 are not directly comparable) \u2264\nn\u22121\nX\ni=1\n1\ncn \u2264n \u22121\ncn\n\u22641\nc.\nTherefore\nP(all adjacent pairs of rows are directly comparable) \u22651 \u22121\nc.\nIf we choose c \u226510, then 1 \u22121\nc \u22659/10. Hence, for such a constant c, whenever\np \u2265log(cn)\n\u221a\nd\n,\nwith probability at least 9/10 every pair of adjacent rows (i, i + 1) is directly comparable.\n2. SVD Practice (10 points)\nConsider any matrix A \u2208Rn\u00d7d.\n1. (2 points) Let v be a unit norm eigenvector of A\u22a4A with eigenvalue \u03bb. Prove that \u03bb \u22650 and\nthat \u2225Av\u22252 =\n\u221a\n\u03bb.\n2. (2 points) Let v1, v2 be two eigenvectors of A\u22a4A that are orthogonal to each other. Prove\nthat Av1 and Av2 are also orthogonal to each other.\n3. (2 points) Let V \u2208Rd\u00d7d contain the d eigenvectors of A\u22a4A as its columns and let \u039b \u2208Rd\u00d7d\ncontain their corresponding eigenvalues. Assume A\u22a4A is full rank and so all of its eigenvalues\nare positive. Prove that U = AV \u039b\u22121/2 has orthonormal columns. Hint: Apply parts (1) and\n(2).\n4. (2 points) Prove that if V \u2208Rd\u00d7d has orthonormal columns then V V \u22a4= I. Conclude using\npart (3) that we can write\nA = U\u039b1/2V \u22a4,\nwhere U and V have orthonormal columns and \u039b is diagonal. Hint: Use the interpretation\nof V V T as a projection matrix in your proof.\n5. (2 points) Let v be an eigenvector of A\u22a4A with eigenvalue \u03bb. Prove that Av is an eigenvector\nof AA\u22a4with eigenvalue \u03bb. Conclude that the columns of U from part (4) are eigenvectors of\nAA\u22a4.\n6\n\nSolution:\n1. Let v be a unit eigenvector of A\u22a4A with eigenvalue \u03bb, so\nA\u22a4Av = \u03bbv\nand\n\u2225v\u22252 = 1.\nMultiply both sides on the left by v\u22a4:\nv\u22a4A\u22a4Av = \u03bb v\u22a4v = \u03bb.\nWe also see that\nv\u22a4A\u22a4Av = (Av)\u22a4(Av) = \u2225Av\u22252\n2.\nPutting these together,\n\u2225Av\u22252\n2 = \u03bb.\nA squared norm is always nonnegative, so \u03bb = \u2225Av\u22252\n2 \u22650. Taking square roots gives\n\u2225Av\u22252 =\n\u221a\n\u03bb.\n2. Suppose v1 and v2 are eigenvectors of A\u22a4A with\nA\u22a4Av1 = \u03bb1v1,\nA\u22a4Av2 = \u03bb2v2,\nand assume v1 and v2 are orthogonal, so v\u22a4\n1 v2 = 0.\nWe want to show that Av1 and Av2 are also orthogonal. Consider their inner product:\n\u27e8Av1, Av2\u27e9= (Av1)\u22a4(Av2) = v\u22a4\n1 A\u22a4Av2.\nSince v2 is an eigenvector of A\u22a4A with eigenvalue \u03bb2, we have\nA\u22a4Av2 = \u03bb2v2,\nso\nv\u22a4\n1 A\u22a4Av2 = v\u22a4\n1 (\u03bb2v2) = \u03bb2 v\u22a4\n1 v2 = \u03bb2 \u00b7 0 = 0.\nTherefore \u27e8Av1, Av2\u27e9= 0, which shows that Av1 and Av2 are orthogonal.\n3. Let V \u2208Rd\u00d7d have as its columns the eigenvectors of A\u22a4A, and let\nA\u22a4AV = V \u039b\nwhere \u039b is the diagonal matrix of eigenvalues. Because A\u22a4A is symmetric and full rank, its\neigenvalues are positive and its eigenvectors can be chosen to be orthonormal. This means\nthe columns of V form an orthonormal basis of Rd and\nV \u22a4V = I.\nWe are given U = AV \u039b\u22121/2 and we want to show the columns of U are orthonormal. We\nthen compute U\u22a4U as such:\nU\u22a4U = (\u039b\u22121/2)\u22a4V \u22a4A\u22a4AV \u039b\u22121/2.\n7\n\nSince \u039b is diagonal with positive entries, \u039b\u22121/2 is also diagonal and symmetric, so (\u039b\u22121/2)\u22a4=\n\u039b\u22121/2. Using A\u22a4AV = V \u039b, we get\nV \u22a4A\u22a4AV = V \u22a4(V \u039b) = (V \u22a4V )\u039b = I\u039b = \u039b.\nTherefore\nU\u22a4U = \u039b\u22121/2 \u039b \u039b\u22121/2 = I.\nSo U\u22a4U = I, which means the columns of U are orthonormal.\n4. Suppose V \u2208Rd\u00d7d has orthonormal columns. V is an orthogonal matrix and both\nV \u22a4V = I\nand\nV V \u22a4= I.\nFrom part (3), we know that\nU = AV \u039b\u22121/2\n=\u21d2\nAV = U\u039b1/2.\nMultiply both sides on the right by V \u22a4:\nAV V \u22a4= U\u039b1/2V \u22a4.\nUsing V V \u22a4= I, the left-hand side becomes simply A, so\nA = U\u039b1/2V \u22a4,\nwhere U and V both have orthonormal columns and \u039b is diagonal with positive entries. This\nis exactly the singular value decomposition of A, with \u039b1/2 containing the singular values.\n5. Let v be an eigenvector of A\u22a4A with eigenvalue \u03bb, so\nA\u22a4Av = \u03bbv.\nApply A on the left:\nAA\u22a4(Av) = A(A\u22a4Av) = A(\u03bbv) = \u03bb(Av).\nThis shows that Av is an eigenvector of AA\u22a4with the same eigenvalue \u03bb, where Av \u0338= 0. In\na full-rank setting (where all eigenvalues of A\u22a4A are positive), part (1) tells us that\n\u2225Av\u22252\n2 = \u03bb > 0,\nso Av is nonzero and is a valid eigenvector.\nNow look at the columns of U from part (4). Part (3) defined\nU = AV \u039b\u22121/2,\nwhere V = [v1 v2 \u00b7 \u00b7 \u00b7 vd] contains the eigenvectors of A\u22a4A and\n\u039b\u22121/2 = diag\n\u0012 1\n\u221a\u03bb1\n, . . . ,\n1\n\u221a\u03bbd\n\u0013\n.\n8\n\nMultiplying AV by this diagonal matrix rescales each column. Therefore the jth column of\nU is\nuj =\n1\np\n\u03bbj\nAvj.\nwhere vj is the corresponding eigenvector of A\u22a4A with eigenvalue \u03bbj. As we just showed,\nAvj is an eigenvector of AA\u22a4with eigenvalue \u03bbj, and scaling an eigenvector by a nonzero\nconstant does not change the eigenvalue or the eigenspace.\nTherefore each uj is also an\neigenvector of AA\u22a4with eigenvalue \u03bbj. This means the columns of U form an orthonormal\nset of eigenvectors of AA\u22a4.\n3. Eigendecomposition and Optimal Low-Rank Approximation (10 points)\nConsider any matrix A \u2208Rn\u00d7d.\n1. (2 points) Prove that\narg\nmin\nZ\u2208Rd\u00d7k:Z\u22a4Z=I \u2225A \u2212AZZ\u22a4\u22252\nF = arg\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ).\n2. (2 points) Let V \u2208Rd\u00d7d have orthonormal columns. Prove that for any Z \u2208Rd\u00d7k with\northonormal columns, V \u22a4Z \u2208Rd\u00d7k also has orthonormal columns. Further prove that any\nZ \u2208Rd\u00d7k with orthonormal columns can be written as Z = V \u22a4U for some U \u2208Rd\u00d7k with\northonormal columns. Hint: Use Problem 2.4.\n3. (2 points) Writing A\u22a4A = V \u039bV \u22a4in its eigendecomposition, use part (2) to prove that\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ) =\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4\u039bZ).\n4. (2 points) Prove that for Z \u2208Rd\u00d7k with orthonormal columns,\ntr(ZZ\u22a4) =\nd\nX\ni=1\n(ZZ\u22a4)ii = k,\nand 0 \u2264(ZZ\u22a4)i,i \u22641 for all i \u2208[d]. Hint: There are several ways to prove the second claim.\nOne is by using Problem 2.4 again.\n(5) (2 points) Use (3) and (4) to show that\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ) =\nk\nX\ni=1\n\u03bbi(A\u22a4A).\nConclude that Z optimizing the equation in part (1) has as its columns the k eigenvectors of\nA\u22a4A corresponding to the top eigenvalues \u03bb1(A\u22a4A), ..., \u03bbk(A\u22a4A).\n9\n\nSolution:\n1. We begin by expanding the Frobenius norm:\n\u2225A \u2212AZZ\u22a4\u22252\nF = tr\n\u0010\n(A \u2212AZZ\u22a4)\u22a4(A \u2212AZZ\u22a4)\n\u0011\n.\nDistributing the product gives\ntr(A\u22a4A) \u22122 tr(A\u22a4AZZ\u22a4) + tr(ZZ\u22a4A\u22a4AZZ\u22a4).\nUsing the cyclic property of the trace and Z\u22a4Z = I, the last term simplifies:\ntr(ZZ\u22a4A\u22a4AZZ\u22a4) = tr(Z\u22a4A\u22a4AZ).\nThus the expression becomes\n\u2225A \u2212AZZ\u22a4\u22252\nF = tr(A\u22a4A) \u2212tr(Z\u22a4A\u22a4AZ).\nThe first term does not depend on Z, so minimizing the Frobenius error is equivalent to\nmaximizing\ntr(Z\u22a4A\u22a4AZ).\nwhich proves\narg\nmin\nZ\u2208Rd\u00d7k:Z\u22a4Z=I \u2225A \u2212AZZ\u22a4\u22252\nF = arg\nmax\nZ\u2208Rd\u00d7k:Z\u22a4Z=I tr(Z\u22a4A\u22a4AZ).\n2. Let V have orthonormal columns, so V \u22a4V = I. If Z also has orthonormal columns, then\n(V \u22a4Z)\u22a4(V \u22a4Z) = Z\u22a4V V \u22a4Z.\nBecause V V \u22a4= I for an orthonormal basis of Rd, we get\n(V \u22a4Z)\u22a4(V \u22a4Z) = Z\u22a4Z = I.\nSo V \u22a4Z also has orthonormal columns.\nTo see why every Z with orthonormal columns can be written as Z = V \u22a4U, start by defining\nU = V Z.\nSince V has orthonormal columns, multiplying by V does not change inner products\nU\u22a4U = Z\u22a4V \u22a4V Z = Z\u22a4Z = I,\nso U also has orthonormal columns. Solving for Z gives\nZ = V \u22a4U.\n10\n\n3. Using A\u22a4A = V \u039bV \u22a4, we substitute:\ntr(Z\u22a4A\u22a4AZ) = tr(Z\u22a4V \u039bV \u22a4Z).\nFrom part (2), whenever Z has orthonormal columns, we can write Z = V \u22a4U with U also\northonormal. Substituting,\ntr(Z\u22a4A\u22a4AZ) = tr(U\u22a4\u039bU).\nBecause the mapping Z 7\u2192U is a bijection (Z \u2190\u2192U = V Z) on matrices with orthonormal\ncolumns, we get\nmax\nZ\u22a4Z=I tr(Z\u22a4A\u22a4AZ) = max\nU\u22a4U=I tr(U\u22a4\u039bU) = max\nZ\u22a4Z=I tr(Z\u22a4\u039bZ).\n4. Since Z has orthonormal columns,\nZ\u22a4Z = I.\nUsing the cyclic property of trace,\ntr(ZZ\u22a4) = tr(Z\u22a4Z) = tr(I) = k.\nSince ZZ\u22a4is a projection matrix onto the column space of Z, each diagonal entry satisfies\n0 \u2264(ZZ\u22a4)i,i \u22641.\nThe lower bound holds because a projection cannot produce negative values on the diagonal.\nFor the upper bound, projecting any vector can only make its length shorter or keep it the\nsame, so the squared length of each coordinate direction cannot grow. This means every\ndiagonal entry of ZZ\u22a4must be at most 1.\n5. From part (3), the expression we want to maximize can be written in the diagonal basis as\ntr(Z\u22a4\u039bZ).\nSince \u039b is diagonal with entries \u03bb1 \u2265\u03bb2 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03bbd, we can expand its trace:\ntr(Z\u22a4\u039bZ) =\nd\nX\ni=1\n\u03bbi(ZZ\u22a4)i,i.\nPart (4) tells us two facts about the diagonal entries of ZZ\u22a4:\nd\nX\ni=1\n(ZZ\u22a4)i,i = k,\n0 \u2264(ZZ\u22a4)i,i \u22641\nfor all i.\nThis means that the diagonal of ZZ\u22a4behaves like a collection of d numbers between 0 and\n1 whose total adds up to k.\nLooking at the quantity we are trying to maximize:\nd\nX\ni=1\n\u03bbi(ZZ\u22a4)i,i.\n11\n\nEach diagonal entry (ZZ\u22a4)i,i multiplies the eigenvalue \u03bbi. Since the \u03bbi\u2019s are sorted from\nlargest to smallest, this weighted sum becomes largest when the k units of weight are placed\non the coordinates with the biggest eigenvalues.\nBecause each (ZZ\u22a4)i,i is at most 1, the best we can do is assign value 1 to the first k diagonal\nentries (corresponding to \u03bb1, . . . , \u03bbk) and assign value 0 to all remaining entries. Any other\nchoice would place some weight on a smaller eigenvalue and reduce the total.\nWith this choice, the trace becomes\nk\nX\ni=1\n\u03bbi.\nSo,\nmax\nZ\u22a4Z=I tr(Z\u22a4\u039bZ) =\nk\nX\ni=1\n\u03bbi.\nC2. More Matrix Completion (10 points)\nConsider an n\u00d7d matrix B where each entry is independently observed with probability p. Assume\nd \u2264n.\n1. (2 points) Prove that if:\n\u2022 B has rank 1.\n\u2022 All observed entries are non-zero.\n\u2022 There is at least one observed entry in each column.\n\u2022 No subset of rows is isolated.\nthen B can be reconstructed from the observed entries.\n2. (2 points) Prove that a subset of rows U is isolated with probability at most\nh\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|id\n.\n3. (2 points) Prove that\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U| \u2264exp(\u2212p|U|/2)\nassuming 1 \u2264|U| \u2264n/2 and p \u2265c ln(n)/d for sufficiently large c. Hint: First show\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U| \u2264exp(\u2212p|U| + e\u2212p(n\u22122|U|))\n4. (2 points) Prove that the probability there exists an isolated subset of nodes is at most 1/n\nif p \u2265c ln(n)/d for sufficiently large c. Hint: Use the union bound.\n5. (2 points) Describe an n\u00d7n matrix C with rank 1 such that the probability we can reconstruct\nC is at most 1/2n even if each entry is independently observed with probability 1/2 and\nobservations are independent.\nHint: C has zero entries otherwise p = c(logn)/n would\nsuffice for reconstruction with high probability.\n12\n\nSolution:\n1. Since B has rank 1, there exist vectors u \u2208Rn and v \u2208Rd such that\nB = uv\u22a4\nso\nBij = uivj.\nTo better visualize how the rows relate to each other, we can form a graph whose nodes are\nthe rows of B. Two rows i and j are connected by an edge if they are directly comparable,\nmeaning they share an observed entry in at least one column. If some subset of rows were cut\noff from the rest of the graph, that subset would be isolated, which is ruled out by assumption.\nTherefore this row-graph is connected.\nIf (i, j) is an edge, then there exists a column k with both Bik and Bjk observed. Using\nBik = uivk and Bjk = ujvk and the fact that all observed entries are nonzero, we get\nui\nuj\n= Bik\nBjk\n.\nThus each edge gives a ratio between two entries of u.\nBecause the row-graph is connected, for any row r there is a path\n1 = r0, r1, . . . , rt = r.\nMultiplying ratios along this path gives\nu1\nur\n=\nt\u22121\nY\ns=0\nurs\nurs+1\n=\nt\u22121\nY\ns=0\nBrs,ks\nBrs+1,ks\n.\nChoosing u1 = 1 determines every ur.\nEach column j has at least one observed entry Bi(j),j. Using Bi(j),j = ui(j)vj, we then get\nvj = Bi(j),j\nui(j)\n.\nAll ui and vj are now known, so every entry of B follows from\nBij = uivj.\nThus B can be reconstructed from the observed entries.\n2. Fix a subset of rows U \u2286[n]. In one column j, U is \u201cdisconnected\u201d from its complement Uc\nif either:\n\u2022 no entry in U is observed in column j, or\n\u2022 no entry in Uc is observed in column j.\nThe probability of the first event is (1 \u2212p)|U|, and of the second is (1 \u2212p)n\u2212|U|. By the union\nbound,\nPr(column j does not connect U and Uc) \u2264(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|.\nColumns are independent, so for all d columns,\nPr(U is isolated) \u2264\nh\n(1 \u2212p)|U| + (1 \u2212p)n\u2212|U|id\n.\n13\n\n3. Let s = |U| with 1 \u2264s \u2264n/2. Set\nx = (1 \u2212p)s,\ny = (1 \u2212p) n\u2212s = (1 \u2212p) s+(n\u22122s) = x \u00b7 (1 \u2212p) n\u22122s.\nThen\nx + y = x\n\u00001 + (1 \u2212p) n\u22122s\u0001\n.\nUsing the inequality (1 \u2212p)k \u2264e\u2212pk for any k \u22650,\nx = (1 \u2212p)s \u2264e\u2212ps,\n(1 \u2212p) n\u22122s \u2264e\u2212p(n\u22122s),\nso\n(1 \u2212p)s + (1 \u2212p) n\u2212s \u2264e\u2212ps\u00001 + e\u2212p(n\u22122s)\u0001\n.\nFor any z \u22650, 1 + z \u2264ez, giving\n(1 \u2212p)s + (1 \u2212p) n\u2212s \u2264exp\n\u0000\u2212ps + e\u2212p(n\u22122s)\u0001\n.\nNow assume p \u2265c ln(n)/d with c large. If 1 \u2264s \u2264n/2 then n \u22122s \u22650, so e\u2212p(n\u22122s) is\nvery small. For large enough c, the quantity e\u2212p(n\u22122s) becomes smaller than ps\n2 for all such\ns. Substituting this gives\n\u2212ps + e\u2212p(n\u22122s) \u2264\u2212ps + ps\n2 = \u2212ps\n2 ,\nand therefore\n(1 \u2212p)|U| + (1 \u2212p) n\u2212|U| \u2264exp(\u2212p|U|/2).\n4. We want to bound the probability that there exists at least one isolated subset of rows. By\nsymmetry between a subset U and its complement U c, it is enough to consider subsets with\n1 \u2264|U| \u2264\u230an/2\u230b.\nFrom part (3), for any subset U with s = |U| and 1 \u2264s \u2264n/2, the probability that U is\nisolated satisfies\nPr(U is isolated) \u2264\nh\n(1 \u2212p)s + (1 \u2212p)n\u2212sid\n\u2264exp\n\u0012\n\u2212psd\n2\n\u0013\n.\nUsing the union bound over all nonempty U with |U| \u2264n/2,\nPr(\u2203U isolated) \u2264\n\u230an/2\u230b\nX\ns=1\n\u0012n\ns\n\u0013\nexp\n\u0012\n\u2212psd\n2\n\u0013\n.\nUsing\n\u0000n\ns\n\u0001\n\u2264ns and the assumption p \u2265c ln(n)/d, we get\n\u0012n\ns\n\u0013\nexp\n\u0012\n\u2212psd\n2\n\u0013\n\u2264ns exp\n\u0012\n\u2212cs ln n\n2\n\u0013\n= ns\u2212cs\n2 = n\u2212s( c\n2 \u22121).\nIf c is large enough so that c\n2 \u22121 \u22652, then each term is at most n\u22122s, and\nPr(\u2203U isolated) \u2264\n\u230an/2\u230b\nX\ns=1\nn\u22122s \u2264\n\u221e\nX\ns=1\nn\u22122s =\nn\u22122\n1 \u2212n\u22122 \u22641\nn\nfor all n \u22652. Therefore, for p \u2265c ln(n)/d with c sufficiently large, the probability that there\nexists an isolated subset of rows is at most 1/n.\n14\n\n5. Let C be an n \u00d7 n rank-1 matrix with a single nonzero row. For example, take\nu = (1, 0, 0, . . . , 0)\u22a4,\nv \u2208Rn with all entries nonzero,\nand set C = uv\u22a4. Then only the first row of C can be nonzero.\nTo reconstruct C uniquely, we need to know every entry of v, since C1j = vj and all other\nrows are zero. If even one entry in the first row is unobserved, we can change that single\nunobserved value while keeping all observed entries and zeros the same, and obtain a different\nrank-1 matrix consistent with the observations. So reconstruction is only possible if all n\nentries in the first row are observed.\nEach entry is observed independently with probability 1/2, so the probability all n entries in\nthe first row are observed is\n(1/2)n.\nTherefore, the probability we can reconstruct C is at most 1/2n.\n15\n",
        "added_at": "2025-11-28T23:32:36.192645"
      }
    ]
  },
  "ad59a5e4-b6a1-4a61-a071-842521b26368": {
    "created_at": "2025-11-29T01:16:53.511454",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T01:16:53.513042"
      }
    ]
  },
  "98efd3b4-c3fd-49c1-83be-9c20e9810b08": {
    "created_at": "2025-11-29T01:35:35.925225",
    "files": [
      {
        "name": "LostArtofMathModeling.pdf",
        "type": "document",
        "text": "The Lost Art of Mathematical Modelling\nLinn\u00b4ea Gyllingberga, Abeba Birhaneb, David J. T. Sumpterc\naDepartment of Mathematics, Box 480, Uppsala, SE-751 06, Sweden\nbMozilla Foundation, 2 Harrison Street, Suite 175, San Francisco, CA 94105, USA\ncDepartment of Information Technology, Uppsala University, Box 337, Uppsala, SE-751\n05, Sweden\nAbstract\nWe provide a critique of mathematical biology in light of rapid developments\nin modern machine learning. We argue that out of the three modelling ac-\ntivities \u2014 (1) formulating models; (2) analysing models; and (3) fitting or\ncomparing models to data \u2014 inherent to mathematical biology, researchers\ncurrently focus too much on activity (2) at the cost of (1). This trend, we\npropose, can be reversed by realising that any given biological phenomenon\ncan be modelled in an infinite number of different ways, through the adoption\nof an pluralistic approach, where we view a system from multiple, different\npoints of view. We explain this pluralistic approach using fish locomotion\nas a case study and illustrate some of the pitfalls \u2014 universalism, creating\nmodels of models, etc. \u2014 that hinder mathematical biology. We then ask\nhow we might rediscover a lost art: that of creative mathematical modelling.\nThis article is dedicated to the memory of Edmund Crampin.\nKeywords:\nmathematical biology, hybrid models, critical complexity,\nmachine learning, equation-free approaches\n1. Introduction\nThe challenges in mathematical biology can be roughly broken down in to\nthree activities: (1) formulating models; (2) analysing models; and (3) fitting\nor comparing models to data. These activities are part of a larger modelling\ncycle \u2014 where modellers work together with biologists to try to better un-\nderstand the study system \u2014 but within that cycle, most of the time, the\nmodeller will be found conducting one of these three activities. Research\nin mathematical biology has evolved a great deal over the last decades, in\nPreprint submitted to Mathematical Biosciences\nJune 5, 2023\narXiv:2301.08559v2  [q-bio.OT]  2 Jun 2023\n\nparticular in response to the rise of machine learning (ML). Indeed, the ML\napproach \u2014 with its emphasis very clearly on activity (3), that of predicting\nfuture data \u2014 can be seen as a challenge to the essence of the research area.\nWe need to find ways of reconciling a mathematical biology approach, largely\nbuilt on describing biological mechanisms, with rapid progress in predicting\npatterns in data [1].\nWe argue that in response to the rise of ML, mathematical biology needs\nto refocus on activity (1), the formulation of new models. We start, in the\nnext section, by defining an inherent feature of biological systems, that they\nare complex. Our definition of complexity differs from (is more radical than)\nthose most often provided by modellers, in that it emphasises the open-\nended nature of biological systems. In section 3, we critique one approach to\ncomplex systems, that of unification. This leads us, in section 4, to propose\nanother approach to modelling biological systems; one which emphasises a\nplurality of models.\nWe then argue, in section 5, that (whether researchers are aware of it\nor not) the unification and pluralistic approaches emphasise different values.\nUnification emphasises activity (2), that of analysing models, while plurality\nemphasises activity (1). We argue that currently, the universalist approach\ndominates and creation of new models, which is inherent to pluralism, is not\nsufficiently emphasised. This brings us to, in sections 6 and 7, a discussion of\nhow mathematical biology has responded with the rise of machine learning.\nWe argue that ML, which emphasises prediction (activity 3), is ill-prepared to\ndeal with complexity without incorporating some form of mechanistic model\nbuilding. But we also, more controversially for those working in mathematical\nbiology, emphasise how some of the responses to the rise of ML have fallen in\nto the trap of making models of models (or fitting models to data generated\nby models) rather than innovating by creating new models of biology itself.\nWe conclude that mathematical biology needs less unification and less\nanalysis of existing models, and more creativity and more creation of new\nmodels. We should be creative without fear of them being wrong or produc-\ning ideas that are mathematically intractable, with an aim of providing a\nmultitude of tools for better understanding of biological systems.\n2. What is complexity?\nBiological systems are complex systems. This statement is so often made,\nthat it can obscure just how radical the consequences of complexity are for\n2\n\nFigure 1: The agents (circles) in a complex biological system interact (straight arrows)\nwith each other, their environment (wavy lines), which is partially open and ever-changing,\nand these interactions are continually adjusted (curved arrows) by the agents themselves.\nAdapted from Di Paolo et al. (2018) [2].\nthe life sciences. To explain why we say radical, consider one of the most\ncommon uses of the term complex systems. In physics and applied mathe-\nmatics, complex systems science has become a name for a set of modelling\ntools: networks, power laws, phase transitions and the like which purport to\ncapture general properties of systems. This is explicitly not what we mean by\ncomplexity. Although complex systems models will come up in this article,\nwe do not consider them useful in defining complexity itself.\nInstead, the radical definition of complex systems comes from, what is\nknown as, critical complexity. Work by Paul Cilliers and Alicia Juarrero\nwarned against aggrandising models (even supposedly complex systems mod-\nels) [3, 4].\nThey emphasise the need to embrace the ambiguous, messy,\nfluid, non-determinable, contextual, and historical nature of complex sys-\ntems. They describe complex phenomena as unfinalizible and inexhaustible,\nwhich means that we can never capture any given biological system entirety\nwith models [5]. Figure 1, adapted from Di Paolo et al. (2018), captures\nthe interdependence, fluidly and interactivity of agents and environments in\na complex system [2]. Complex systems are open-ended, which means there\nis no uncontested way of telling whether what we have included in a model\nis crucial or what we have omitted as irrelevant is indeed so. Models can, ac-\ncording to the critical complexity approach, be contradictory: we can accept\ntwo incompatible predictions as both describing the same system.\nThis approach views a model as a snapshot of a system and no single\nsnapshot tells the whole story. For modelling the human body, for example,\n\u201ca portrait of a person, a store mannequin, and a pig can all be models\u201d\n3\n\n[6]. None is a perfect representation, but each can be the best model for\na human, depending on whether one wants to remember an old friend, to\nbuy clothes, or to study anatomy. The critical complexity view suggests that\ntheoreticians should avoid specialising in any one modelling approach and\ntry to find the right set of models to understand a particular system in a\ngiven context.\nThere can, of course, be more than one definition of complex systems.\nIndeed, Cilliers and Juarrero\u2019s approach to complexity encourages a plural-\nity of definitions (after all, there is no single view of a system). We would,\nthough, emphasise that it is the radical definition of complexity \u2014 in which\nsystems always resist a complete description, are open and unfinalisable \u2014\nwhich is least well understood by mathematical biologists today. It is there-\nfore important to investigate how complexity should be approached in the\nstudy of biological systems.\n3. The allure of unification\nPrecisely because most biological systems are more complex than physical\nsystems, they are also more difficult to model.\nIn another article in this\ncollection, Vittadello and Stumpf outline two broad approaches that might\nbe adopted [7]. The first of these approaches builds on the motto put forward\nby Philip Anderson, that \u2019More is Different\u2019 [8]: suggesting that each level of\nbiological organisation requires different types of approaches. The second of\nthese, which we will critique in this section, suggests that the way forward is\ngreater unification and increased mathematical rigour.\nIn presenting the second approach, that of unification, Vittadello and\nStumpf argue that the success of mathematical modelling of physical systems\nsuggest that further progress in mathematical biology can be best made with\neven more advanced mathematics [7].\nWith the complexity of biological\nsystems comes a need for rigorous definitions of biological concepts, and they\npropose a definition-theorem-proof style as a way forward. Accompanying\nthis idea, comes a focus on unification. In the same way as there are unifying\ntheories in physics \u2014 relating to energy conservation, entropy, etc. \u2014 there\nought to be unifying models for biological systems.\nUnder this view, an\nincrease in rigour is supposed to tame the complexity of biology. Vitadelli\nand Stumpf suggest that unification and rigour could lead to avoidance of\nexcessive incrementalism in model development, as well as avoidance of a\nfocus on development of simple models of simple systems.\n4\n\nThe idea of unification in biology has been echoed by many others [9, 10,\n11, 12]. For example, van Hemmen claims that some of the universal laws\nof biology might have already been discovered in neurobiology [12]. Since\nmathematical models can describe the behaviour of biological systems at\ncertain scales, the equations of the models, van Hemmen argues, could be\nseen as \u2018universal laws\u2019. The question is how to find the appropriate scale\nfor these universal laws.\nWe should, van Hemmen argues, be patient: it\nhas taken humanity hundreds of years to discover the physical laws of the\nUniverse formulated through mathematics; with enough time we will discover\nthe universal laws of biology too [12].\nFinding the appropriate scales and determining unifying laws is certainly\npart of modelling biological systems. For example, a fundamental difference\nbetween most biological and physical systems is the conservation of momen-\ntum.\nFor self-propelled particle models \u2014 which are used for modelling\nbiology on scales ranging from cells to flocks \u2014 momentum is not conserved\n[13], and thus classical kinetic theory used in physics to derive macroscopic\nequations is not applicable. To get around this problem, Degond introduced\nthe Generalised Collision Invariant [14], from which it is possible to derive\nmacroscopic equations for many self-propelled particle systems used in mod-\nelling flocks and other systems in biology [15, 16, 17, 18].\nThis method\nallows us to show convergence between self-propelled particle systems on mi-\ncroscopic and macroscopic scale, i.e. between the movement of e.g. flocks of\nbirds described by local interactions between a few individuals, and PDEs\ndescribing the angular and velocity distributions of the flock as a group.\nIn the example above, an approach like the Generalised Collision Invari-\nant, when applied to the collective motion of real biological systems, usually\nfails to provide the answers biologists are looking for. Indeed, the very point\nof self-propelled particle models is to capture the rich, varying dynamics of\ndifferent schools, swarms and flocks. Derivations of continuum equations for\ninfinitely large populations provide little insight into these questions. While\nall of these systems may well share a common invariant, this is not the key\nissue at hand. Biologists want to find the details of shapes of schooling [19],\nunderstand how a wave of fish escape from a predator [20], find the mech-\nanism behind the V-shape of migrating birds [21], measure the sociability\nof fish schooling based on their movement [22], study leadership in flocks of\npigeons [23, 24], or understand the mechanisms behind shepherding sheep\n[25]; to give just a few examples. We have illustrated this point with a spe-\ncific example, but see the point as applying more widely. While unification\n5\n\nin mathematical biology is tempting, it often neglects the complex nature of\nbiological systems.\nOur discussion of the Generalised Collision Invariant is meant to give\none concrete example of how a mathematically appealing universal idea fails\nto give insight into range of complexity seen in biology 1. Such approaches\nmay well lead to new, interesting and beautiful mathematics [26], but there\nis no reason (a-priori) that they will give deeper biological insight. In bi-\nology, experimental results are noisy, non-stationary and often differ across\nspecies and scales. Studies of self-propelled particles span species from spi-\nders through fish to humans, as well as sperm and cell interactions. When\nformalising such models, we have to ask the question, exactly which biolog-\nical entity or species is it that is being formalised? Is the relevant scale the\nmolecules, the cells, the organs, the animal or the collective? These questions\nare not amenable to a universal approach or reducible to a small number of\nequations.\nA focus on rigour in biology is an example of over-mathematization, a\nphenomena frequently discussed in economics [27, 28, 29, 30, 31]. The cri-\ntique has been summarized by the Nobel prize laureate Paul Krugman, who\ndescribed that \u201cthe economics profession went astray because economists, as\na group, mistook beauty, clad in impressive-looking mathematics, for truth.\u201d\n[30]. A similar phenomena has occurred in theoretical physics, where the fo-\ncus on developing beautiful mathematical theories has taken precedent over\ngenuine insight into physics [32]. There is a danger, that in trying to find\nunification, mathematical biology gets stuck at analysing/unifying simple\nmodels, none of which are appropriate for any specific system. Moreover, in\nsearch for unification and general methods in biology, we might neglect to\nstudy actual systems because they are too complicated or detailed.\nThe allure of unification often centres the idea of deriving properties of\nthe collective from interactions between individuals.\nCountering the pos-\nsibility of unifying biology through such an approach, Sandra Mitchell ar-\ngues that microscopic phenomena (cells, molecules, atoms) are not always\nsuited for capturing the rich variety of relations found in biological sciences\n[33].\nScientific representations are abstractions or idealizations, and thus\n1In this article we take our examples from fish locomotion. We choose an area we\nunderstand well in order to illustrate our points in a concrete way. We encourage the\nreader to imagine similar examples in their own specialised research area.\n6\n\nonly represent partial features of individuals or a system [34]. As such, the\nabstractions/idealizations do not constitute identical representations across\nthe two levels. Thus, even if the descriptions at each level is accurate, they\nmay, by being partial, not represent the same features of nature. As a re-\nsult, there is no straightforward derivability or intertranslability relationship\nbetween levels [33].\nUnification is a reductionist approach [35]. Multilevel, multi-component,\ncomplex systems that populate the domain of biology cannot be reduced to\na simple, unified picture of scientific theorizing [33]. And even though con-\ntributions to mathematical biology can be made by unification approaches,\nthey do not account for all the explanations that biologists seek [35]. This\nview echoes that of Dupr\u00b4e, who explains unification as another way of argu-\ning for the (flawed) reductionist hypothesis that all of science can be reduced\nto a description based on simple building blocks [36]. Reductionism fails\nto account for what Noble calls the \u2018relativity principle\u2019, that there is no\n\u201cprivileged scale at which biological functions are determined\u201d [37].\nIn summary, there are both philosophical (universalism is another form\nof flawed reductionism) and practical (supposedly general equations don\u2019t\ncapture the type of questions biologists ask) arguments against a unification\napproach to biology. The question now is what is the alternative?\n4. The pluralistic approach\nGiven that biological knowledge is fragmented and that biological systems\nare complex, we have argued that it is not useful to build a general theoreti-\ncal framework or to strive for unification. We now outline, in this section, an\nalternative: the pluralistic approach to modelling biological systems. Plu-\nralism embraces complexity by never seeking to close a biological system in\na single or a small number of formalisms, but endlessly endeavours to find\nnew ways of looking at the world. Pluralism reflects the open, unfinalisable\nnature of complex systems described in section 2.\nWe illustrate the pluralistic approach by focusing on one specific area\nof biology, modelling fish locomotion, before broadening out (in the next\nsections) to explore the lessons we can learn from this approach when looking\nat other systems.\nFigure 2 illustrates various ways that fish locomotion\ncan be modelled. When modelling the behaviour of fish shoals and schools,\nself-propelled particle models are widely used [38, 39, 40, 41, 42, 43, 44]\n(figure 2a). The most prominent model, called the Vicsek model [42], assumes\n7\n\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 2: Under the Pluralistic Approach to Biology, fish locomotion can be understood\nand modelled in many different ways. When modelling the collective motion of fish schools,\nself-propelled particle models can be used to capture and explain their motion (a). How-\never, most fish use burst and glide movement, where they alternate between accelerated\nmotion and powerless gliding (b). This type of motion can be understood from a hy-\ndrodynamical perspective (c), or a behavioural ecology perspective (d), as there are both\nenergetic (c) and perceptual benefits (d) of this type of motion. When swimming in pairs,\nthe fish influence each others burst and glide motion (e). Intermittent locomotion can\nalso be understood and modelled through neurobiological perspective, where the activity\nin the brain affects the motion (f).\nthat each fish (or particle) moves with constant speed, while its direction is\nupdated at each time step to be closer to the average direction of individuals\nwithin its neighbourhood. A noise term is added to model uncertainty or\nerror in the fish\u2019s direction. These type of models have proven useful when\nexplaining many aspects of schooling behaviour, for example, how body size\ninfluences shoaling patterns [45], how large schools switch between different\norganisational states [46] and the spread of escape waves in response to a\npredator [20].\nAlthough self-propelled particles have been central to the study of collec-\ntive motion, both in fish and other species, there are a several, fundamental\nways in which they do not capture the locomotion of fish. For example, rather\nthan updating their angle to turn towards other fish, as assumed in many\nmodels [39, 40, 41, 43], between-fish attraction and repulsion is mainly me-\ndiated by speed changes [19]. Even in the absence of interactions, constant\n8\n\nspeed is not typical swimming behaviour for most fish species. Zebrafish\n[47], koi carps [48], guppies [49], cod [50], red nose tetra fish [51] and many\nother species swim by alternating between accelerated motion and powerless\ngliding [52] (figure 2b).\nThe discrepancy between how fish are modelled collectively and individ-\nually can be seen as an example of logical incompatibility between models\n[33, 53]. After all, fish cannot simultaneously swim both at a constant speed\nas in the Viscek model and according to a varying speed, as in a burst and\nglide model! Yet the models in figures 2a and 2b might view the same fish\nin both of these (contradictory) ways.\nFrom a unification point of view, logical incompatibility might be seen as\nan indication that the models are, at least in some aspects, wrong or that\nwe should try to find ways to unify them to make them both approximately\ncorrect (possibly on different time scales).\nBuilding on our definition of\ncomplexity and open systems, however, we would downplay the importance\nof investigating relationships between models.\nFrom the point of view of\ncomplexity, since each model is a different snapshot of a system, taken from\na different point of view, using a different camera and lens, we should not\nbe surprised to find an element of incompatibility between models. If we\naccept the idea of complex phenomena as unfinalizible and inexhaustible\n(as we outlined in section 2), we cannot expect our models to be logically\ncompatible on every level. Moreover, lack of compatibility should not concern\nus: it is simply a consequence of taking a different view, of using a different\nlens, which bends the light of observation in a different way.\nEven within the context of intermittent burst and glide motion, we can\nfind several useful and correct, yet logically incompatible approaches to fish\nlocomotion. For example, a range of biomechanical models [52, 50, 54, 55,\n56, 57, 58, 59] have been proposed for fish locomotion (figure 2c). These\nhave been used to show that there are energetic advantages of burst and\nglide behaviour, when compared to constant swimming speed. The energetic\ncost of swimming is minimized during the glide phase, where the body is\nrigid and the fish decelerate due to water resistance [52]. Other models have\nfocused on a behavioural ecology perspective, since intermittent motion is\nassociated with many ecologically relevant behaviours, e.g. foraging, mating,\nexploration and predator evasion [60, 61]. There are also perceptual benefits\nthat can arise from the pauses in locomotion, such as the sensory system\u2019s\ncapacity to detect relevant stimuli increases, [62, 63] (figure 2d).\nYet another level to the understanding of intermittent locomotion of fish,\n9\n\nis to look at why fish use burst and glide when swimming in pairs. Theoreti-\ncal studies from hydromechanical perspectives show that there are energetic\nadvantages to intermittent motion in this setting [64, 65].\nHowever, the\nhydromechanical models neglect the social aspects of intermittent locomo-\ntion when swimming in pairs (figure 2e). In fact, burst and glide swimming\nhave been shown pivotal to detect and quantify social interactions between\nindividual fish [49]. Moreover, high burst speed in response to neighbours\nevolves when subjected to artificial selection [66]. Also, when studying in-\ntermittent locomotion of pairs of fish, leadership can become apparent [67].\nFor example, in pairs of freely exploring eastern mosquitofish, it is possible\nto categorize the fish into leaders and followers [68].\nThe locomotion of fish can also be studied in neurobiological settings\n(figure 2f), leading to questions about how different nerve cells affect the\ninitiation of motion seen in fish [69, 70, 71, 72]. This can also be linked to\ngenetical aspects of fish locomotion. For example, studies on zebrafish show\nthat genetically encoded calcium-indicators provide a direct link between\nsignalling at a cellular level and functional output in the form of swimming\nbehaviour [72, 73].\nAbove we have listed many different approaches to modelling fish loco-\nmotion. Depending on what question we want to answer, the model used is\ndifferent. It is this creation of many different viewpoints which is pluralism.\nThis multiplicity (or plurality) of ways of seeing a system is even stronger\nthan suggested by Philip Anderson\u2019s \u2018More is Different\u2019 approach [8]. An-\nderson emphasised that \u201cpsychology is not applied biology, nor is biology\napplied chemistry\u201d: each level of organisation requires completely new ap-\nproaches. We would go further, arguing that even within a single biological\nphenomena, at only one organisational level, we need a whole range of dif-\nferent explanations and models. We simultaneously engage many different\nframeworks and views of a system, each designed to answer a different sub-\nquestion. We take different snapshots of the system and then use each of\nthem to construct a bigger picture of the system. The more snapshots we\ninclude, the more comprehensive the bigger picture.\nOur approach follows, what Sandra Mitchell calls, integrative pluralism\n[34, 33, 35]. Like us, she argues that complexity in nature, particularly in\nbiology, has direct implications for our scientific theories, models, and expla-\nnations. To quote, \u201cnature is complex and so, too, should be our representa-\ntions of it\u201d [33]. Mitchell\u2019s own examples build on how social insect biologists\nexplained the emergence of division of labour, by focusing on the effects of\n10\n\ncauses at one level (genetics, single organisms, and colonies), while idealizing\naway the other potentially relevant factors [53]. She argues that scientists\ndo not need unified theories to provide causal explanations. Nor is there a\nrequirement for logical compatibility between explanations. Mitchell\u2019s char-\nacterisation of biological research as a whole, we believe, translates even to\nthe mathematical biologist\u2019s approach to creating formal models. To para-\nphrase her: \u201cbiology is complex and so, too, should be our mathematical\nmodels of it\u201d.\nComprehensive, integrated understanding of biology doesn\u2019t come from\none universal model, but rather the synergy of many different, potentially\ncontradictory models. Having many models helps us understand more.\n5. How modelling approaches shape mathematical biology\nOur rejecting unification and embracing a pluralistic approach, which\nemphasises the open nature of biological systems, might be viewed as a purely\nphilosophical exercise. It could also be seen as a question of personal taste.\nAfter all, practicing mathematical biologists seldom discuss whether they see\nthe world in terms of unification or integrative pluralism, they just get on\nwith their job... don\u2019t they?\nIn this section we argue that the formalism and unification approach has\na strong influence on what is valued in mathematical biology research. To\nunderstand this point, let\u2019s return to the three activities outlined in the in-\ntroduction: (1) formulating models; (2) analysing models; and (3) fitting or\ncomparing models to data. Under a unification approach, which emphasises\nthe importance of mathematical formalism, activity (1) is about finding a\nsmall number of universal models which explain as many biological phenom-\nena as possible; and activity (2) requires great care in developing a precise\nformalism to be clear about the universal properties of the model. Under an\nintegrative pluralism approach activity (1) is about producing lots of differ-\nent models which view a system through different lenses; and activity (2) is\nimportant to get right initially, but details are less important, since we are\nhappy to discard the model once it has told us something useful.\nUnification and integrative pluralism thus emphasise very different values\nand practices. Whether or not these values are acknowledged by researchers,\nwe can look at the type of activities carried out by mathematical biology\nresearchers and see which approach is more predominant. This is what we\ndo now.\n11\n\nWe have already described a pluralistic approach to fish locomotion (fig-\nure 2) and emphasised the success of self-propelled particle models in describ-\ning schooling patterns. However, while there are valuable empirical studies\nof bird flocks and fish schools, where variations of these models are used\nto understand the details, these are outnumbered by articles reporting on\nsimulations and investigations of theoretical properties of flocking models\n[74, 75, 76]. A similar pattern is seen in evolutionary game theory, which\nprovides insight into how spatial and genetic structure is important to the\nevolution of co-operation [77, 78, 79]. This approach has produced countless\ntheoretical questions about how co-operation evolved in different (artificial)\nsettings [80, 81, 82, 83], which are detached from observations in the natural\nworld. This is not to say that evolutionary game theory is not useful in biol-\nogy, it is just to point out that testable predictions have been accompanied\nby a massive sub-literature simply analysing model properties.\nSimilarly, complex systems tools \u2014 such as networks [84, 85], power laws\n[86], phase transitions [87] etc. \u2014 often purport to capture general proper-\nties of systems and suggest that studying these models will give very general\ninsight, in terms of scaling laws or unifying rules. In making claims of univer-\nsality, modellers sometimes suggest that biology will succumb, like physics,\nto an understanding based on one or a small number of models. Under the\ndefinition of complexity we use here, the more radical definition, such univer-\nsality is impossible: complex systems are not complex if they can be reduced\nto a small number of universal rules.\nFocusing on network science, Fox-Keller argues that claims that scale-free\nnetworks and power law distributions are universal laws of life are problem-\natic on two counts [88]. Firstly, power laws are not as ubiquitous as was\noriginally supposed [89]. Secondly, and more importantly to Fox-Keller, the\nexistence of these distributions tells us nothing about the mechanisms that\ngive rise to them [88]. Many reported power laws lack either (or both of)\nstatistical and mechanistic support [90] There are, at least, a dozen distinct\nways to derive power laws from theoretical models [91], making them far from\nuniversal. And asking questions about how power laws should be measured\nhas led to better practices for model fitting (and identifying cases in which\nthey don\u2019t fit) [92]. Again, it is the details that matter in biology. Power\nlaws don\u2019t fit everywhere.\nWe note that it is activity (1), rather than activity (2), which typically\nprovide the biggest steps forward in Science. Specifically, it is the initial\nmodel (prisoners dilemma, chaos theory [93], Turing\u2019s work on morphogenesis\n12\n\n[94], the Fitzhugh-Nagumo model [95], Yule\u2019s power law model [86], Viscek\u2019s\nSPP model [42]...)\nwhich provides the most inspiration and insight into\nthe biological world, rather than analyses of small variations of these initial\nmodels. Yet, universalism, with its emphasis on formalism, prizing activity\n(2) over activity (1), remains a dominant force in shaping what constitutes\nmathematical biology.\nWe believe that the emphasis on model analyses, rather than creating\nnew models, is caused by a tendency towards universalism. The result is\nmathematical analysis of small variations of existing models (activity 2) at\nthe expense of creating very novel and different models (activity 1). Admit-\ntedly, the observations we make above are qualitative. We have not carried\nout a comprehensive literature review comparing universalism and plural-\nism approaches, but instead we appeal to the active mathematical biologist\nto consider their own research field and think about the theory/application\nbalance. We would imagine for most (leaving activity 3 aside for now) the\nbalance is towards 2, rather than 1.\nThis imbalance, we believe, is wrong. Small variations of already existing\nmodels seldom provide additional insight into biological systems. We would\nfollow Reed, who wrote in in his 2004 Essay Why is mathematical biology so\nhard [96]: \u201dDon\u2019t do mathematical biology to satisfy a desire to find universal\nstructural relationships; you\u2019ll be disappointed. Don\u2019t waste time developing\n\u201cmethods of mathematical biology\u201d, the problems are too diverse for central\nmethods.\nWhat\u2019s left is the biology.\nYou should only do mathematical\nbiology if you are deeply interested in the science itself.\u201d\n6. Machine learning can\u2019t replace modelling\nThe distinction between universalism and pluralism has become partic-\nularly important with the move away from activities (1) and (2) \u2013\u2013\u2013 model\nbuilding and analysis \u2013\u2013\u2013 towards methods broadly described as machine\nlearning (ML), which emphasise activity (3).\nInitially, machine learning\nmethods were proposed primarily for data collection \u2014 for example, com-\nputer vision was proposed to track movements of fish and cells \u2014 but it later\nbecame clear that these methods could also be used to pick out patterns in\nthe data. In an early example, Berman and colleagues showed how different\ntypes of fruit fly behaviour (grooming with different legs, running, moving\nof wings etc.) could be categorised without the need for human definitions\nof these activities [97]. This work has evolved in to a field of computational\n13\n\nethology, which Pereira recently claimed will \u201cin the near future, make it pos-\nsible to quantify in near completeness what an animal is doing as it navigates\nits environment\u201d [98].\nSuch claims place activity (3), that of fitting or comparing models to\ndata, as central to the scientific endeavour. The proponents of this approach\n[99, 100, 101] sometimes even go as far as to suggest that activity (1), that of\ncreating models, is now redundant. For example, Rackauckas and colleagues\nclaim that traditional mathematical models are only required because of\ntoo small training datasets and that models which are not learnt directly\nfrom data have an inductive bias, because they use assumptions about the\nunderlying system being modelled [101].\nWe reject such claims as yet another form of universalism.\nAs Nurse\nhas recently argued, data should be a means to knowledge, not an end in\nthemselves [102].\nNurse emphasises that the hypothesis free approach of\ncollecting data is just the first step when doing biological research. In order\nto make advancements in biological sciences, new hypotheses and theories\nneed to be formulated. Reed follows a similar line of argument: \u201cdata itself\nis not understanding. Understanding requires a conceptual framework (that\nis, a theory) that identifies the fundamental variables and their causative\ninfluences on each other\u201d [96].\nIn the context of machine learning, Birhane and Sumpter make a dis-\ntinction between closed systems \u2014 such as games like Chess and Go, some\nimage analysis tasks and short term nowcasting of the weather \u2014 that are\nentirely defined by the available data, and open systems \u2014 like fish schools\nand other biological systems \u2014 similar to those we discuss above, which can\nbe viewed in multiple ways [103]. The only systems which can modelled by\ndata alone, in the way Rackauckas and colleagues propose, are those which\nare fully closed. Indeed, data-driven models, are just representations of the\ndata itself, rather than insights in to that data. To take an example given\nby Nurse, what if Darwin had just fed in the data of size and shapes of finch\nbeaks into a neural network? The deep learning algorithm would have found\nclusters and patterns and might have predicted the future development of the\nbeaks on different islands, but would never formulated the theory of natural\nselection. An ML approach, when employed in the absence of activities (1)\nand (2), can be characterised as a view from nowhere [104]: without con-\ntext data analysis becomes meaningless to us, the scientist interpreting the\nresults.\nNot only are there (as we have already emphasised) many ways of see-\n14\n\ning a system, there are also many other reasons for doing modelling, over\nand above making predictions.\nFor example, models can also be used to\nguide data collection for future experiments, or to capture qualitative be-\nhaviors of overarching interest and lead to new scientific questions being\nposed [105]. While pure machine learning models are focused on prediction,\na large part of biological understanding focuses on mechanistic explanations\n[106, 107]. There is no consensus definition of mechanisms, but Illari and\nWilliamson offers the following: \u2018a mechanism for a phenomenon consists of\nentities and activities organized in such a way that they are responsible for\nthe phenomenon\u2019 [108]. Thus, for a mathematical model to give a mech-\nanistic explanation to a phenomenon, the model cannot merely summarize\nand describe the data, but rather the model should encode a mechanism\ngenerating the observed phenomenon [109].\nBreidenmoser and Wolkenhauer make a distinction between mechanistic\nmodels, which explain a system by describing underlying biological processes,\nand phenomenological models, which only \u201csave the phenomena\u201d by fitting a\ncurve to the data [106, 109]. The key problem with a purely machine learning\nbased approach is that it says little about the processes, i.e. mechanisms,\nbehind the data [110], and instead focuses on \u201csaving the phenomena\u201d, to\nuse Breidenhoser and Wolkenhauer\u2019s term. Phenomenological models, like\nmany \u2018view from nowhere\u2019 machine learning models, might be a good start\nin understanding statistical relationships between variables, and thus a first\nstep towards modelling a phenomena, but these models do not contribute to\na deeper understanding [109].\n7. Are hybrid models the answer?\nMoving away from the idea that machine learning can fully replace math-\nematical modelling, several authors have proposed integration of mechanis-\ntic models and machine learning methods in the form of hybrid models\n[1, 7]. These come in many forms, from neural ordinary differential equations\n[111, 100] and biologically informed neural networks (BINNs) [112, 113], to\nsymbolic regression [114, 115] and equation learning [116, 117]. Hybrid mod-\nels can both have an underlying specified mechanistic model and then use\nmachine learning methods to infer parts of the equations (as in BINNs [112])\nor aim to find analytical expressions directly from data (as in equation learn-\ning [101]). The end product is thus a mechanistic model, in form of, e.g., a\ndynamical system [117].\n15\n\nTo give a concrete mathematical example, consider a reaction diffusion\nequation of the following form\nut = (D(u)ux)x + R(u),\n(1)\nwhere D is a function of u describing the diffusion process, and R, also\na function of u, describing the reaction process. This equation is used in\na range of situations in mathematical biology, from pattern formation, to\ninsect dispersal, to spread of epidemics, to tissue growth. Depending on the\napplication, D(u) and R(u) take different forms. For example, in the Fisher-\nKPP equation, which was originally used to describe the spatial spread of a\nfavoured gene, D is a constant and R(u) = ru(1 \u2212u) [118]. Choosing the\ncorrect form of D and R is an open question and the focus of many research\nefforts [112]. The traditional approach for solving this task is to choose an\nappropriate form of D(u) and R(u) based on first principles and then try to\nfit the parameters of the model to the data.\nAnother approach, is to not state the form of D(u) and R(u) explic-\nitly, but instead learn the functions directly from data. There are different\nmethods for achieving this. For example, sparse regression [119, 120] and\ntheory-informed neural networks [112] are two of them.\nAlso, D(u) and\nR(u) do not need to be explicitly defined, but can instead just be modelled\nwith data [121]. This approach is sometimes referred to as an equation-free\napproach as parts of the equations do not need to be explicitly formulated\n[121]. Other applications of so-called equation free approaches can be found\nin, for example, in ecosystem forecasting [122].\nHybrid models are sometimes presented as defining a complete modelling\ncycle: doing all three activities \u2013 (1) formulating a model, (2) analysing\na model and (3) fitting the model to data. But even though parts of the\nequations do not need to be explicitly formulated in this approach and are\ndirectly learnt from the data, the underlying model (in our case the reaction\ndiffusion equation in equation (1)) is already specified. Thus, equation-free\napproaches like these are useful for model selection, validation, and analysis,\nwhich is part of activity (2) and (3), but they do not replace activity (1).\nEven methods that infer analytical expressions directly from data, with\nno underlying model like the reaction-diffusion example, cannot be used to\nreplace activity (1). Building mathematical models is more than choosing a\nset of differential equations that describe data. For example, equation learn-\ning would never have formulated inclusive fitness theory in the way Hamilton\n16\n\ndid after careful across species observations [123]. Nor would symbolic re-\ngression produce the Vicsek equations and the idea of self-propelled particles\nsolely from analysing videos of bird flocks.\nThus, while approaches combining machine learning and mechanisms are\ncertainly part of the way forward for mathematical biology, we also need to\nlook critically at whether they are a genuine step away from the universalist\nthinking, which we have criticised in earlier sections of this article.\nOne important critique in this direction arises when researchers apply\nmachine learning methods to models, rather than to data from natural sys-\ntems. For example, parameters and properties of agent-based models can\nbe learnt using machine learning methods [124, 125]. In such settings, sim-\nulations are seen as a way of generating data on which to test methods for\nfitting models (i.e to improve the way we perform activity 3). The limitation,\nfrom the perspective of a complexity approach, is that simulated data from\nknown models does not come from a complex system (as defined in section\n2). Models are not in themselves open-ended or unconstrained, in the same\nway a biological system is. Instead, an already well-established view of a\nsystem in the form of one model is studied in the context of another model.\nThis approach implicitly avoids the challenge of formulating new models.\nFor example, Roesch and colleagues, apply a collocation based method\nfor training neural ODE:s, i.e. ODE:s where the derivative is learnt directly\nby a neural network [100]. To demonstrate the applicability to biology, the\nneural ODE is trained on data generated from the classical Van der Pol\noscillator with added Gaussian noise.\nThe authors view the model as a\npromising hybrid method for biological applications, because it uses machine\nlearning methods to infer a (mechanistic) dynamical system. However, using\nBreidenmoser and Wolkenhauer\u2019s distinction between phenomenological and\nmechanistic models, we would argue that, even though coated in terms like\n\u201cmechanistic\u201d, this method is nothing more than a phenomenological model:\na curve is fitted to a derivative, instead of a time series.\nA similar critique can be applied to studies in which differential equa-\ntions are learnt from model simulations [125]. For example, in one paper\non hybrid models, Nardini et al.\nshow how differential equations can be\nlearned from agent-based simulations in order to predict how the latter type\nof model responds to parameter changes [117]. One justification for this ap-\nproach is that agent-based models might be computationally expensive to\nsimulate. The approach also allows researchers to compare three approaches\n\u2013 an agent based model, mean field equations derived from the agent based\n17\n\nmodel, and a differential equation model learnt from the data provided by\nthe agent based model\u2014 in terms of the insights they offer and how well\nthe methods approximate each other [117]. In terms of the pluaralistic ap-\nproach (which we advocate) the limitation of such a study is that it gives the\n(false) impression of generating three different views of a system, while it is\nprimarily an exercise in deriving and estimating one model from another. In\nmany studies describing hybrid approaches, data from real-world systems is\nnot included.\nThe danger here is that hybrid and equation-free modelling has a veneer\nof doing activities (1) through (3) but are, in fact, limited to activity (2).\nReal biological data is noisy, non-stationary and can be viewed in a multi-\ntude of ways. Understanding biology requires an openness to adopt different\nview points, rather than an attempt to close our approach down to one self-\nconsistent framework. Fundamentally, any mathematical model or method\nshould provide additional insights to a phenomenon, rather than cement a\nrelationship between models. Hybrid approaches are not a substitute for,\nand are in can run counter to, an approach based onintegrative pluralism.\n8. How we can do better?\nThe title of this paper is The Lost Art of Mathematical Modelling. In\norder to demonstrate why we think that something is lost, we have been crit-\nical of an emphasis of universalism and formalism in mathematical biology.\nIt is in activity (2), we believe the subject can get lost. Our solution is to\nrefocus on activity (1), creating new models. Smaldino takes a similar stand\npoint but in social science [110]. He argues that \u201dit is time to focus on better\npractices for hypothesis generation. We need training programmes in model\nbuilding and critique, plus consortia-building and funding programmes to\ninvent and test measurements that make models tractable. Better methods\nwill help us get the right answers; models and measurements will ensure we\nask the right questions\u201d [126]. We fully agree with this position. We need to\ncreate more mathematical models in biology.\nThe way forward, we believe, is to view mathematical modelling in a\nmore open way, one that admits biology is complex. This involves, as we saw\nin section 4, creating lots of different models of a system in order to build\nup a broader understanding. There is a need, as Mitchell emphasises, for\nan approach based on integrative pluralism [35], which emphasises creating\n18\n\nmany different types of models (as is done in modelling fish locomotion; see\nfigure 2).\nCreating new models does not have to be a grandiose activity. For ex-\nample, in a recent article we have developed a model of social burst and\nglide motion by combining a well-studied model of neuronal dynamics, the\nFitzHugh-Nagumo model, with a self-propelled model of fish motion [127].\nThe FitzHugh-Nagumo equations model the membrane potential in a neu-\nron, V , and the recovery variable, W .[95]. We found a way to couple this\nmodel to the velocity of the fish v and a burst potential b, with the following\nequations:\ndb\ndt = b \u2212b3\n3 \u2212v + c\ndv\ndt = g(b) \u2212kv,\nwhere, g(b) is given by\ng(b) = a\n\u0010arctan(z1(b \u2212b0))\n\u03c0\n+ 1\n2\n\u0011\n.\nFigure 3(a) shows the burst and glide dynamics of two fish, interacting ac-\ncording to these dynamics, with their interactions coupled as a function of\nthe distance between them. The model captures the responses of fish to each\nother, where one fish speeds up when the other fish moves ahead of it.\nThe challenge in this research was investigating the plausibility of a mech-\nanism, i.e.\nwhether the bursting of neurons, captured by the FitzHugh-\nNagumo model, could be successfully related to burst and glide motion. At\nthis stage of the research, comparison to data is less important [128]. Figure\n3(b) shows that data collected from pairs of interacting guppies (as part of\na study on effects of predation [49]) is not entirely (or even very closely)\ndescribed by the model, but there is something worth pursuing. Specifically,\nfish do respond to each other in their burst and glides and there is a need\nto relate this motion to neuronal mechanisms. The model we propose is cer-\ntainly not the best way of predicting the time series in figure 3(b), but it\ncould potentially be a way to identify a key mechanism in fish locomotion.\nOur point here is not to argue that this specific model is in itself a break-\nthrough, it is rather to give a feeling for what a refocussing on models might\nlook like. It is OK to play around with different sets of equations and look to\n19\n\nFigure 3: Example of model compared with data from [49]. Simulation of (a) a model of\ncoupled differential equations describing the interaction of fish and (b) the speed of two\nfish over time for a period of 15 seconds of two guppies swimming in an arena.\nsee, in a very loose way, whether they capture aspects of our understanding\nof a system.\nCreating new models can mean leaving our comfort zone. Some pointers\nin this direction include research in artificial life, developing online games\nwhere humans interact with simulations and investigating novel cellular au-\ntomata (see figure 4 for examples). The common theme is an open-ended\nattempt to identify emergent phenomena, without ever trying to close the\nsystem with an exhaustive mathematical analysis. Instead of stifling theo-\nretical development, we believe that mathematical biology should push to be\nmore creative, to take risks and allow ourselves to be spectacularly wrong.\nIt is the small insights, of how things fit together, which have constituted\nthe biggest steps forward in mathematical biology. From Turing\u2019s work on\nmorphogenesis [94] and Hodgkin and Huxley\u2019s modelling of neuronal firing\n[133], through Hamilton\u2019s proposal of inclusive fitness [123] and May\u2019s appli-\ncation of chaos theory in ecology [93], to Vicsek\u2019s model of collective motion\n[42] and Hogeweg\u2019s models of multi-level selection [134], it is the formulation\nof models (rather than their in-depth analysis) which has led to progress.\nInstead, of treating these existing models as sacred Platonic forms, which\nshould be respected with deeper analysis, we should not be scared to look for\n20\n\nFigure 4: Sketch examples of creating models of complex biological systems. (a) Langton\u2019s\n(1984) loop is one of the first examples of a self-reproducing cellular automata [129]; (b)\nArtificial chemistry gives insight in to both the origin of life and how complex components\narise from simpler units [130]; (c) Nicky Case\u2019s \u2018We come what we behold\u2019 is one example\nof an explorable game that allows the user to actively explore complexity [131]; (d) Inigo\nQuilez work shows how realistic complex, fractal landscapes can be generated by a few\nlines of code [132].\nnew ideas and approaches. The inconvenient truth \u2014 that biology is itself\nendlessly rich and varied and never subject to a final analysis \u2014 is sometimes\ndismissed as an unrigorous approach. Such a situation is wrong. We need to\nrediscover the lost art of creating mathematical models.\n9. Epilogue\nThis article is part of a special collection in memory of Edmund Crampin.\nEdmund was a close friend of one of the authors (David Sumpter) when they\nwere PhD students. What David remembers most fondly about Edmund was\nhis ability to be critical, both in a very deep way about his own work and\nconstructively of the work of others. At that time, at the turn of the new\nmillennium, Edmund was working on reaction-diffusion equations [135, 136],\nbut was always torn as to whether the technical work he did truly contributed\nto biological insight. He felt that the experimental results collected at the\ntime [137], although supporting reaction-diffusion as a mechanism, did not\n21\n\nentirely justify the extended theory he was working on.\nRather than staying safely within the confines of one model, after his PhD,\nEdmund made sure he created new approaches to a variety of mathematical\nproblems. When we look at his contributions \u2014 ranging from multi-cellular\nmodelling of the heart [138, 139], through systems biology [140] to biochem-\nical reactions [141, 142] and fundamentals of biophysics [143, 144]\u2014we see\nan approach grounded in all three of the activities (modelling, anlaysis, com-\nparison to data) we have discussed here.\nEdmund\u2019s self-insight has always stayed with David. And it is this spirit\nwe have tried to adopt in this paper. Edmund may not have agreed with\neverything we have written \u2014 we have taken a very strong position on what\nmathematical biology should be \u2014 but he would have understood the need to\nbe critical. Most of all, he would have enjoyed, over a long lunch or a vigorous\nwalk, talking (and arguing) about the merits of different approaches to the\nsubject he loved.\nReferences\n[1] R. E. Baker, J.-M. Pena, J. Jayamohan, A. J\u00b4erusalem, Mechanistic\nmodels versus machine learning, a fight worth fighting for the biological\ncommunity?, Biology letters 14 (5) (2018) 20170660.\n[2] E. A. Di Paolo, E. C. Cuffari, H. De Jaegher, Linguistic bodies: The\ncontinuity between life and language, MIT press, 2018.\n[3] P. Cilliers, Complexity and postmodernism: Understanding complex\nsystems, routledge, 2002.\n[4] A. Juarrero, Dynamics in action: Intentional behavior as a complex\nsystem, Emergence 2 (2) (2000) 24\u201357.\n[5] P. Cilliers, Why we cannot know complex things completely, Critical\ncomplexity: Collected essays 6 (2016) 97\u2013105.\n[6] P. Blanchard, R. L. Devaney, G. R. Hall, Differential equations, Cen-\ngage Learning, 2012.\n[7] S. T. Vittadello, M. P. Stumpf, Open problems in mathematical biol-\nogy, arXiv preprint arXiv:2206.09516 (2022).\n22\n\n[8] P. W. Anderson, More is different: broken symmetry and the nature\nof the hierarchical structure of science., Science 177 (4047) (1972) 393\u2013\n396.\n[9] N. Rashevsky, Outline of a unified approach to physics, biology and\nsociology, The bulletin of mathematical biophysics 31 (1) (1969) 159\u2013\n198.\n[10] P. Kitcher, Explanatory unification, Philosophy of science 48 (4) (1981)\n507\u2013531.\n[11] A. Borovik, A mathematician\u2019s view of the unreasonable ineffectiveness\nof mathematics in biology, Biosystems 205 (2021) 104410.\n[12] J. L. van Hemmen, Biology and mathematics: A fruitful merger of two\ncultures, Biological cybernetics 97 (1) (2007) 1\u20133.\n[13] A. Czir\u00b4ok, T. Vicsek, Collective behavior of interacting self-propelled\nparticles, Physica A: Statistical Mechanics and its Applications 281 (1-\n4) (2000) 17\u201329.\n[14] P. Degond, S. Motsch, Continuum limit of self-driven particles with\norientation interaction, Mathematical Models and Methods in Applied\nSciences 18 (supp01) (2008) 1193\u20131215.\n[15] P. Degond, T. Yang, Diffusion in a continuum model of self-propelled\nparticles with alignment interaction, Mathematical Models and Meth-\nods in Applied Sciences 20 (supp01) (2010) 1459\u20131490.\n[16] P. Degond, A. Frouvelle, J.-G. Liu, Macroscopic limits and phase tran-\nsition in a system of self-propelled particles, Journal of nonlinear sci-\nence 23 (3) (2013) 427\u2013456.\n[17] P. Degond, A. Frouvelle, S. Merino-Aceituno, A. Trescases, Alignment\nof self-propelled rigid bodies: from particle systems to macroscopic\nequations, in: International workshop on Stochastic Dynamics out of\nEquilibrium, Springer, 2017, pp. 28\u201366.\n[18] P. Degond, S. Merino-Aceituno, Nematic alignment of self-propelled\nparticles: From particle to macroscopic dynamics, Mathematical Mod-\nels and Methods in Applied Sciences 30 (10) (2020) 1935\u20131986.\n23\n\n[19] J. E. Herbert-Read, A. Perna, R. P. Mann, T. M. Schaerf, D. J.\nSumpter, A. J. Ward, Inferring the rules of interaction of shoaling\nfish, Proceedings of the National Academy of Sciences 108 (46) (2011)\n18726\u201318731.\n[20] J. E. Herbert-Read, J. Buhl, F. Hu, A. J. Ward, D. J. Sumpter, Initi-\nation and spread of escape waves within animal groups, Royal Society\nopen science 2 (4) (2015) 140355.\n[21] S. J. Portugal, T. Y. Hubel, J. Fritz, S. Heese, D. Trobe, B. Voelkl,\nS. Hailes, A. M. Wilson, J. R. Usherwood, Upwash exploitation and\ndownwash avoidance by flap phasing in ibis formation flight, Nature\n505 (7483) (2014) 399\u2013402.\n[22] D. J. Sumpter, A. Szorkovszky, A. Kotrschal, N. Kolm, J. E. Herbert-\nRead, Using activity and sociability to characterize collective motion,\nPhilosophical Transactions of the Royal Society B: Biological Sciences\n373 (1746) (2018) 20170015.\n[23] M. Nagy, Z. \u00b4Akos, D. Biro, T. Vicsek, Hierarchical group dynamics in\npigeon flocks, Nature 464 (7290) (2010) 890\u2013893.\n[24] M. Nagy, G. V\u00b4as\u00b4arhelyi, B. Pettit, I. Roberts-Mariani, T. Vicsek,\nD. Biro, Context-dependent hierarchies in pigeons, Proceedings of the\nNational Academy of Sciences 110 (32) (2013) 13049\u201313054.\n[25] D. Str\u00a8ombom, R. P. Mann, A. M. Wilson, S. Hailes, A. J. Morton, D. J.\nSumpter, A. J. King, Solving the shepherding problem: heuristics for\nherding autonomous, interacting agents, Journal of the royal society\ninterface 11 (100) (2014) 20140719.\n[26] M. C. Reed, Mathematical biology is good for mathematics, Notices of\nthe AMS 62 (10) (2015) 1172\u20136.\n[27] C. Beed, O. Kane, What is the critique of the mathematization of\neconomics?, Kyklos 44 (4) (1991) 581\u2013612.\n[28] N. Bouleau, On excessive mathematization, symptoms, diagnosis and\nphilosophical bases for real world knowledge, Real World Economics\n57 (2011) 90\u2013105.\n24\n\n[29] N. Bouleau, Can there be excessive mathematization of the world?, in:\nSeminar on Stochastic Analysis, Random Fields and Applications VII,\nSpringer, 2013, pp. 453\u2013469.\n[30] P. Krugman, How did economists get it so wrong?, New York Times\n2 (9) (2009) 2009.\n[31] I. Moosa, The mathematization of economics: Useful, inevitable, in-\ndispensable or simply extravaganza, Manag Econ Res J 7 (1) (2021)\n19036.\n[32] S. Hossenfelder, Lost in math: How beauty leads physics astray, Ha-\nchette UK, 2018.\n[33] S. D. Mitchell, Biological complexity and integrative pluralism, Cam-\nbridge University Press, 2003.\n[34] S. D. Mitchell, Integrative pluralism, Biology and Philosophy 17 (1)\n(2002) 55\u201370.\n[35] S. D. Mitchell, M. R. Dietrich, Integration without unification: An ar-\ngument for pluralism in the biological sciences, the american naturalist\n168 (S6) (2006) S73\u2013S79.\n[36] J. Dupr\u00b4e, The disunity of science, Mind 92 (367) (1983) 321\u2013346.\n[37] D. Noble, A theory of biological relativity: no privileged level of cau-\nsation, Interface focus 2 (1) (2012) 55\u201364.\n[38] B. L. Partridge, The structure and function of fish schools, Scientific\namerican 246 (6) (1982) 114\u2013123.\n[39] I. Aoki, A simulation study on the schooling mechanism in fish, Bulletin\nof the Japanese Society of Scientific Fisheries (Japan) (1982).\n[40] C. W. Reynolds, Flocks, herds and schools: A distributed behavioral\nmodel, in: Proceedings of the 14th annual conference on Computer\ngraphics and interactive techniques, 1987, pp. 25\u201334.\n[41] A. Huth, C. Wissel, The simulation of the movement of fish schools,\nJournal of theoretical biology 156 (3) (1992) 365\u2013385.\n25\n\n[42] T. Vicsek, A. Czir\u00b4ok, E. Ben-Jacob, I. Cohen, O. Shochet, Novel type\nof phase transition in a system of self-driven particles, Physical review\nletters 75 (6) (1995) 1226.\n[43] I. D. Couzin, J. Krause, R. James, G. D. Ruxton, N. R. Franks, Collec-\ntive memory and spatial sorting in animal groups, Journal of theoretical\nbiology 218 (1) (2002) 1\u201311.\n[44] D. Str\u00a8ombom, Collective motion from local attraction, Journal of the-\noretical biology 283 (1) (2011) 145\u2013151.\n[45] M. Romenskyy, J. E. Herbert-Read, A. J. Ward, D. J. Sumpter, Body\nsize affects the strength of social interactions and spatial organization\nof a schooling fish (pseudomugil signifer), Royal Society open science\n4 (4) (2017) 161056.\n[46] K. Tunstr\u00f8m, Y. Katz, C. C. Ioannou, C. Huepe, M. J. Lutz, I. D.\nCouzin, Collective states, multistability and transitional behavior in\nschooling fish, PLoS Comput Biol 9 (2) (2013) e1002915.\n[47] A. V. Kalueff, M. Gebhardt, A. M. Stewart, J. M. Cachat, M. Brimmer,\nJ. S. Chawla, C. Craddock, E. J. Kyzar, A. Roth, S. Landsman, et al.,\nTowards a comprehensive catalog of zebrafish behavior 1.0 and beyond,\nZebrafish 10 (1) (2013) 70\u201386.\n[48] G. Wu, Y. Yang, L. Zeng, Kinematics, hydrodynamics and energetic\nadvantages of burst-and-coast swimming of koi carps (cyprinus carpio\nkoi), Journal of Experimental Biology 210 (12) (2007) 2181\u20132191.\n[49] J. E. Herbert-Read, E. Ros\u00b4en, A. Szorkovszky, C. C. Ioannou, B. Ro-\ngell, A. Perna, I. W. Ramnarine, A. Kotrschal, N. Kolm, J. Krause,\net al., How predation shapes the social interaction rules of shoaling\nfish, Proceedings of the Royal Society B: Biological Sciences 284 (1861)\n(2017) 20171126.\n[50] J. J. Videler, D. Weihs, Energetic advantages of burst-and-coast swim-\nming of fish at high speeds, Journal of Experimental Biology 97 (1)\n(1982) 169\u2013178.\n26\n\n[51] G. Li, I. Ashraf, B. Fran\u00b8cois, D. Kolomenskiy, F. Lechenault, R. Godoy-\nDiana, B. Thiria, Burst-and-coast swimmers optimize gait by adapting\nunique intrinsic cycle, Communications biology 4 (1) (2021) 1\u20137.\n[52] D. Weihs, Energetic advantages of burst swimming of fish, Journal of\nTheoretical Biology 48 (1) (1974) 215\u2013229.\n[53] S. Cusimano, B. Sterner, Integrative pluralism for biological function,\nBiology & Philosophy 34 (6) (2019) 1\u201321.\n[54] R. Blake, Functional design and burst-and-coast swimming in fishes,\nCanadian Journal of Zoology 61 (11) (1983) 2491\u20132494.\n[55] F. E. Fish, J. F. Fegely, C. J. Xanthopoulos, Burst-anc-coast swim-\nming in schooling fish (notemigonus cr ysoleucas) with implications for\nenergy economy, Comparative Biochemistry and Physiology Part A:\nPhysiology (1991).\n[56] E. G. Drucker, The use of gait transition speed in comparative studies\nof fish locomotion, American Zoologist 36 (6) (1996) 555\u2013566.\n[57] E. Akoz, K. W. Moored, Unsteady propulsion by an intermittent swim-\nming gait, arXiv preprint arXiv:1703.06185 (2017).\n[58] D. Floryan, T. Van Buren, A. J. Smits, Forces and energetics of inter-\nmittent swimming, Acta Mechanica Sinica 33 (4) (2017) 725\u2013732.\n[59] P. Paoletti, L. Mahadevan, Intermittent locomotion as an optimal con-\ntrol strategy, Proceedings of the Royal Society A: Mathematical, Phys-\nical and Engineering Sciences 470 (2164) (2014) 20130535.\n[60] A. D. Wilson, J.-G. J. Godin, Boldness and intermittent locomotion\nin the bluegill sunfish, lepomis macrochirus, Behavioral Ecology 21 (1)\n(2010) 57\u201362.\n[61] M. Andersson, On optimal predator search, Theoretical Population\nBiology 19 (1) (1981) 58\u201386.\n[62] D. L. Kramer, R. L. McLaughlin, The behavioral ecology of intermit-\ntent locomotion, American Zoologist 41 (2) (2001) 137\u2013153.\n27\n\n[63] O. B\u00b4enichou, C. Loverdo, M. Moreau, R. Voituriez, Intermittent search\nstrategies, Reviews of Modern Physics 83 (1) (2011) 81.\n[64] A. P. Maertens, A. Gao, M. S. Triantafyllou, Optimal undulatory swim-\nming for a single fish-like body and for a pair of interacting swimmers,\nJournal of Fluid Mechanics 813 (2017) 301\u2013345.\n[65] G. Li, D. Kolomenskiy, H. Liu, B. Thiria, R. Godoy-Diana, On the\nenergetics and stability of a minimal fish school, PLoS One 14 (8)\n(2019) e0215265.\n[66] A. Kotrschal, A. Szorkovszky, J. Herbert-Read, N. I. Bloch, M. Romen-\nskyy, S. D. Buechel, A. F. Eslava, L. S. Al`os, H. Zeng, A. Le Foll, et al.,\nRapid evolution of coordinated and collective movement in response to\nartificial selection, Science advances 6 (49) (2020) eaba3148.\n[67] S. Nakayama, J. L. Harcourt, R. A. Johnstone, A. Manica, Initiative,\npersonality and leadership in pairs of foraging fish, PLoS One 7 (5)\n(2012) e36606.\n[68] T. Schaerf, J. Herbert-Read, A. Ward, A statistical method for iden-\ntifying different rules of interaction between individuals in moving an-\nimal groups, Journal of the Royal Society Interface 18 (176) (2021)\n20200925.\n[69] T. D. Wiggin, T. M. Anderson, J. Eian, J. H. Peck, M. A. Masino,\nEpisodic swimming in the larval zebrafish is generated by a spatially\ndistributed spinal network with modular functional organization, Jour-\nnal of neurophysiology 108 (3) (2012) 925\u2013934.\n[70] E. A. Naumann, J. E. Fitzgerald, T. W. Dunn, J. Rihel, H. Sompolin-\nsky, F. Engert, From whole-brain data to functional circuit models: the\nzebrafish optomotor response, Cell 167 (4) (2016) 947\u2013960.\n[71] X. Chen, Y. Mu, Y. Hu, A. T. Kuan, M. Nikitchenko, O. Randlett,\nA. B. Chen, J. P. Gavornik, H. Sompolinsky, F. Engert, et al., Brain-\nwide organization of neuronal activity and convergent sensorimotor\ntransformations in larval zebrafish, Neuron 100 (4) (2018) 876\u2013890.\n28\n\n[72] A. Del Pozo, R. Manuel, A. B. I. Gonzalez, H. K. Koning, J. Habicher,\nH. Zhang, A. Allalou, K. Kullander, H. Boije, Behavioral characteri-\nzation of dmrt3a mutant zebrafish reveals crucial aspects of vertebrate\nlocomotion through phenotypes related to acceleration, ENeuro 7 (3)\n(2020).\n[73] H. K. Koning, A. Ahemaiti, H. Boije, A deep-dive into fictive\nlocomotion\u2013a strategy to probe cellular activity during speed transi-\ntions in fictively swimming zebrafish larvae, Biology Open 11 (3) (2022)\nbio059167.\n[74] H. Chat\u00b4e, F. Ginelli, G. Gr\u00b4egoire, F. Raynaud, Collective motion of\nself-propelled particles interacting without cohesion, Physical Review\nE 77 (4) (2008) 046113.\n[75] R. Gro\u00dfmann, P. Romanczuk, M. B\u00a8ar, L. Schimansky-Geier, Vortex\narrays and mesoscale turbulence of self-propelled particles, Physical\nreview letters 113 (25) (2014) 258104.\n[76] P. Szab\u00b4o, M. Nagy, T. Vicsek, Transitions in a self-propelled-particles\nmodel with coupling of accelerations, Physical Review E 79 (2) (2009)\n021908.\n[77] R. Axelrod, W. D. Hamilton, The evolution of cooperation, science\n211 (4489) (1981) 1390\u20131396.\n[78] M. A. Nowak, R. M. May, Evolutionary games and spatial chaos, Na-\nture 359 (6398) (1992) 826\u2013829.\n[79] J. Hofbauer, K. Sigmund, et al., Evolutionary games and population\ndynamics, Cambridge university press, 1998.\n[80] M. A. Nowak, Five rules for the evolution of cooperation, science\n314 (5805) (2006) 1560\u20131563.\n[81] G. Szab\u00b4o, G. Fath, Evolutionary games on graphs, Physics reports\n446 (4-6) (2007) 97\u2013216.\n[82] M. Perc, A. Szolnoki, Coevolutionary games\u2014a mini review, BioSys-\ntems 99 (2) (2010) 109\u2013125.\n29\n\n[83] Z. Wang, L. Wang, A. Szolnoki, M. Perc, Evolutionary games on multi-\nlayer networks: a colloquium, The European physical journal B 88 (5)\n(2015) 1\u201315.\n[84] D.\nJ.\nWatts,\nS.\nH.\nStrogatz,\nCollective\ndynamics\nof\n\u2018small-\nworld\u2019networks, nature 393 (6684) (1998) 440\u2013442.\n[85] A.-L. Barab\u00b4asi, R. Albert, Emergence of scaling in random networks,\nscience 286 (5439) (1999) 509\u2013512.\n[86] M. E. Newman, Power laws, pareto distributions and zipf\u2019s law, Con-\ntemporary physics 46 (5) (2005) 323\u2013351.\n[87] C. Domb, Phase transitions and critical phenomena, Elsevier, 2000.\n[88] E. Fox Keller, A clash of two cultures, Nature 445 (7128) (2007) 603\u2013\n603.\n[89] A. D. Broido, A. Clauset, Scale-free networks are rare, Nature commu-\nnications 10 (1) (2019) 1\u201310.\n[90] M. P. Stumpf, M. A. Porter, Critical truths about power laws, Science\n335 (6069) (2012) 665\u2013666.\n[91] D. Sornette, Critical phenomena in natural sciences: chaos, fractals,\nselforganization and disorder: concepts and tools, Springer Science &\nBusiness Media, 2006.\n[92] A. Clauset, C. R. Shalizi, M. E. Newman, Power-law distributions in\nempirical data, SIAM review 51 (4) (2009) 661\u2013703.\n[93] R. M. May, Biological populations with nonoverlapping generations:\nstable points, stable cycles, and chaos, Science 186 (4164) (1974) 645\u2013\n647.\n[94] A. M. Turing, The chemical basis of morphogenesis, Bulletin of math-\nematical biology 52 (1) (1990) 153\u2013197.\n[95] R. FitzHugh, Impulses and physiological states in theoretical models\nof nerve membrane, Biophysical journal 1 (6) (1961) 445\u2013466.\n30\n\n[96] M. C. Reed, Why is mathematical biology so hard, Notices of the AMS\n51 (3) (2004) 338\u2013342.\n[97] G. J. Berman, D. M. Choi, W. Bialek, J. W. Shaevitz, Mapping the\nstereotyped behaviour of freely moving fruit flies, Journal of The Royal\nSociety Interface 11 (99) (2014) 20140672.\n[98] T. D. Pereira, J. W. Shaevitz, M. Murthy, Quantifying behavior to\nunderstand the brain, Nature neuroscience 23 (12) (2020) 1537\u20131549.\n[99] C. T. Perretti, S. B. Munch, G. Sugihara, Model-free forecasting out-\nperforms the correct mechanistic model for simulated and experimental\ndata, Proceedings of the National Academy of Sciences 110 (13) (2013)\n5253\u20135257.\n[100] E. Roesch, C. Rackauckas, M. P. Stumpf, Collocation based training of\nneural ordinary differential equations, Statistical Applications in Ge-\nnetics and Molecular Biology 20 (2) (2021) 37\u201349.\n[101] C. Rackauckas, Y. Ma, J. Martensen, C. Warner, K. Zubov, R. Supekar,\nD. Skinner, A. Ramadhan, A. Edelman, Universal differential equations\nfor scientific machine learning, arXiv preprint arXiv:2001.04385 (2020).\n[102] P. Nurse, et al., Biology must generate ideas as well as data, Nature\n597 (7876) (2021) 305\u2013305.\n[103] A. Birhane, D. J. Sumpter, The games we play: critical complexity\nimproves machine learning, arXiv preprint arXiv:2205.08922 (2022).\n[104] A. Birhane, Algorithmic injustice: a relational ethics approach, Pat-\nterns 2 (2) (2021) 100205.\n[105] J. M. Epstein, Why model?, Journal of artificial societies and social\nsimulation 11 (4) (2008) 12.\n[106] P.-A. Braillard, C. Malaterre, Explanation in biology: An introduction,\nin: Explanation in biology, Springer, 2015, pp. 1\u201328.\n[107] C. Mekios, Explanation in systems biology: Is it all about mecha-\nnisms?, in: Explanation in Biology, Springer, 2015, pp. 47\u201372.\n31\n\n[108] P. M. Illari, J. Williamson, What is a mechanism?\nthinking about\nmechanisms across the sciences, European Journal for Philosophy of\nScience 2 (1) (2012) 119\u2013135.\n[109] T. Breidenmoser, O. Wolkenhauer, Explanation and organizing princi-\nples in systems biology, in: Explanation in Biology, Springer, 2015, pp.\n249\u2013264.\n[110] P. E. Smaldino, Models are stupid, and we need more of them, in:\nComputational social psychology, Routledge, 2017, pp. 311\u2013331.\n[111] W. Bonnaff\u00b4e, B. C. Sheldon, T. Coulson, Neural ordinary differential\nequations for ecological and evolutionary time-series analysis, Methods\nin Ecology and Evolution 12 (7) (2021) 1301\u20131315.\n[112] J. H. Lagergren, J. T. Nardini, R. E. Baker, M. J. Simpson, K. B. Flo-\nres, Biologically-informed neural networks guide mechanistic modeling\nfrom sparse experimental data, PLoS computational biology 16 (12)\n(2020) e1008462.\n[113] M. Daneker, Z. Zhang, G. E. Karniadakis, L. Lu, Systems biology:\nIdentifiability analysis and parameter identification via systems-biology\ninformed neural networks, arXiv preprint arXiv:2202.01723 (2022).\n[114] S. Gaucel, M. Keijzer, E. Lutton, A. Tonda, Learning dynamical sys-\ntems using standard symbolic regression, in: European Conference on\nGenetic Programming, Springer, 2014, pp. 25\u201336.\n[115] B. T. Martin, S. B. Munch, A. M. Hein, Reverse-engineering ecolog-\nical theory from data, Proceedings of the Royal Society B: Biological\nSciences 285 (1878) (2018) 20180422.\n[116] G. Martius, C. H. Lampert, Extrapolation and learning equations,\narXiv preprint arXiv:1610.02995 (2016).\n[117] J. T. Nardini, R. E. Baker, M. J. Simpson, K. B. Flores, Learning differ-\nential equation models from stochastic agent-based model simulations,\nJournal of the Royal Society Interface 18 (176) (2021) 20200987.\n[118] J. D. Murray, Mathematical biology: I. An introduction, Springer,\n2002.\n32\n\n[119] S. L. Brunton, J. L. Proctor, J. N. Kutz, Discovering governing equa-\ntions from data by sparse identification of nonlinear dynamical systems,\nProceedings of the national academy of sciences 113 (15) (2016) 3932\u2013\n3937.\n[120] S. H. Rudy, S. L. Brunton, J. L. Proctor, J. N. Kutz, Data-driven\ndiscovery of partial differential equations, Science advances 3 (4) (2017)\ne1602614.\n[121] C. A. Yates, R. Erban, C. Escudero, I. D. Couzin, J. Buhl, I. G.\nKevrekidis, P. K. Maini, D. J. Sumpter, Inherent noise can facili-\ntate coherence in collective swarm motion, Proceedings of the National\nAcademy of Sciences 106 (14) (2009) 5464\u20135469.\n[122] H. Ye, R. J. Beamish, S. M. Glaser, S. C. Grant, C.-h. Hsieh,\nL. J. Richards, J. T. Schnute, G. Sugihara, Equation-free mechanistic\necosystem forecasting using empirical dynamic modeling, Proceedings\nof the National Academy of Sciences 112 (13) (2015) E1569\u2013E1576.\n[123] W. D. Hamilton, The genetical evolution of social behaviour. ii, Journal\nof theoretical biology 7 (1) (1964) 17\u201352.\n[124] G. Ten Broeke, G. van Voorn, A. Ligtenberg, J. Molenaar, The use of\nsurrogate models to analyse agent-based models, Journal of Artificial\nSocieties and Social Simulation 24 (2) (2021).\n[125] D. G. Patsatzis, L. Russo, I. G. Kevrekidis, C. Siettos, Data-driven con-\ntrol of agent-based models: An equation/variable-free machine learning\napproach, Journal of Computational Physics 478 (2023) 111953.\n[126] P. Smaldino, Better methods can\u2019t make up for mediocre theory, Nature\n575 (7783) (2019) 9\u201310.\n[127] L. Gyllingberg, A. Szorkovszky, D. J. Sumpter, Using neuronal models\nto capture burst and glide motion and leadership in fish, arXiv preprint\narXiv:2304.00727 (2023).\n[128] M. A. Bedau, Can unrealistic computer models illuminate theoretical\nbiology, in: Proceedings of the 1999 genetic and evolutionary compu-\ntation conference workshop program, Citeseer, 1999, pp. 20\u201323.\n33\n\n[129] C. G. Langton, Self-reproduction in cellular automata, Physica D: Non-\nlinear Phenomena 10 (1-2) (1984) 135\u2013144.\n[130] Y. Liu, D. J. Sumpter, Mathematical modeling reveals spontaneous\nemergence of self-replication in chemical reaction systems, Journal of\nBiological Chemistry 293 (49) (2018) 18854\u201318863.\n[131] N. Case, We become what we behold.\nURL https://ncase.itch.io/wbwwb\n[132] I. Quilez, Value noise derivatives.\nURL\nhttps://www.iquilezles.org/www/articles/morenoise/\nmorenoise.htm\n[133] A. L. Hodgkin, A. F. Huxley, A quantitative description of membrane\ncurrent and its application to conduction and excitation in nerve, The\nJournal of physiology 117 (4) (1952) 500\u2013544.\n[134] P. Hogeweg, N. Takeuchi, Multilevel selection in models of prebiotic\nevolution: compartments and spatial self-organization, Origins of Life\nand Evolution of the Biosphere 33 (4) (2003) 375\u2013403.\n[135] E. J. Crampin, E. A. Gaffney, P. K. Maini, Reaction and diffusion on\ngrowing domains: scenarios for robust pattern formation, Bulletin of\nmathematical biology 61 (6) (1999) 1093\u20131120.\n[136] E. Crampin, E. Gaffney, P. Maini, Mode-doubling and tripling in\nreaction-diffusion patterns on growing domains: A piecewise linear\nmodel, Journal of mathematical biology 44 (2) (2002) 107\u2013128.\n[137] S. Kondo, R. Asai, A reaction\u2013diffusion wave on the skin of the marine\nangelfish pomacanthus, Nature 376 (6543) (1995) 765\u2013768.\n[138] M. Fink, S. A. Niederer, E. M. Cherry, F. H. Fenton, J. T. Koivum\u00a8aki,\nG. Seemann, R. Thul, H. Zhang, F. B. Sachse, D. Beard, et al., Cardiac\ncell modelling: observations from the heart of the cardiac physiome\nproject, Progress in biophysics and molecular biology 104 (1-3) (2011)\n2\u201321.\n[139] N. Smith, D. Nickerson, E. Crampin, P. Hunter, Multiscale computa-\ntional modelling of the heart, Acta Numerica 13 (2004) 371\u2013431.\n34\n\n[140] P. Kohl, E. J. Crampin, T. Quinn, D. Noble, Systems biology: an\napproach, Clinical Pharmacology & Therapeutics 88 (1) (2010) 25\u201333.\n[141] I. Siekmann, J. Sneyd, E. J. Crampin, Mcmc can detect nonidentifiable\nmodels, Biophysical journal 103 (11) (2012) 2275\u20132286.\n[142] E. J. Crampin, S. Schnell, P. E. McSharry, Mathematical and computa-\ntional techniques to deduce complex biochemical reaction mechanisms,\nProgress in biophysics and molecular biology 86 (1) (2004) 77\u2013112.\n[143] P. Cudmore, M. Pan, P. J. Gawthrop, E. J. Crampin, Analysing and\nsimulating energy-based models in biology using bondgraphtools, The\nEuropean Physical Journal E 44 (12) (2021) 1\u201320.\n[144] N. Shahidi, M. Pan, K. Tran, E. J. Crampin, D. P. Nickerson, A seman-\ntics, energy-based approach to automate biomodel composition, PloS\none 17 (6) (2022) e0269497.\n35\n",
        "added_at": "2025-11-29T01:35:36.057272"
      }
    ]
  },
  "bc2e60b3-f186-4d8a-9024-ec84bee622be": {
    "created_at": "2025-11-29T03:13:56.694983",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T03:13:56.697029"
      }
    ]
  },
  "70356fa8-7f5d-4385-9ffb-f12b80fe264f": {
    "created_at": "2025-11-29T03:15:03.992623",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n\n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n\n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n\n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n\n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \n\nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n\n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n\n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=HTMLResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/summary.html\", \n\n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/keyterms/{session_id}\", response_class=HTMLResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/keyterms.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/questions/{session_id}\", response_class=HTMLResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/questions.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/resources/{session_id}\", response_class=HTMLResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/resources.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n\nPython\nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n\n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n\nPython\n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n\n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n\n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n\nHTML\n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n    <!-- LEFT: Upload + Materials --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n\n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n            <form \n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\" \n            > \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <div id=\"materials-panel\"> \n            <!-- filled by upload_status fragment --> \n            <div id=\"materials-list\"> \n                <em>No materials uploaded yet.</em> \n            </div> \n        </div> \n    </div> \n \n    <!-- RIGHT: Study Tools --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555; margin-top:0;\"> \n            Once you've uploaded all relevant materials, choose what you want \nthe AI to generate. \n\n        </p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n    </div> \n</div> \n \n\n<script> \nlet currentSessionId = null; \n \n// Called from upload_status fragment \nfunction setSessionId(id) { \n    currentSessionId = id; \n    document.getElementById(\"session_id_input\").value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn = document.getElementById(\"btn-keyterms\"); \n    const qBtn = document.getElementById(\"btn-questions\"); \n    const rBtn = document.getElementById(\"btn-resources\"); \n \n    summaryBtn.disabled = keyBtn.disabled = qBtn.disabled = rBtn.disabled = \nfalse; \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \n// -------- Live recording with MediaRecorder -------- \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        // Start recording \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n\n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        // Stop recording \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n</script> \n \n{% endblock %} \n\nHTML\nHTML\n \n2.\u200b base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n    <script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</head> \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b upload_status.html  \n \n\nHTML\nHTML\nHTML\n<h2>{{ title }}</h2> \n<div class=\"output-content\"> \n    {{ content | safe }} \n</div> \n \nc.\u200b upload_list.html  \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li>{{ f.name }} ({{ f.size_kb }} KB)</li> \n    {% endfor %} \n</ul> \n{% else %} \n<p>No files uploaded yet.</p> \n{% endif %} \n \nd.\u200b output.html  \n \n<div id=\"materials-list\"> \n    {% if files %} \n    <ul> \n        {% for f in files %} \n        <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n        {% endfor %} \n    </ul> \n    {% else %} \n    <em>No materials uploaded yet.</em> \n    {% endif %} \n</div> \n \n<script> \n    setSessionId(\"{{ session_id }}\"); \n</script> \n \n \n \n\nCSS\nStatic:  \n1.\u200b styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n\n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n\n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n2.\u200b  \n",
        "added_at": "2025-11-29T03:15:04.054880"
      },
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T03:15:11.772472"
      }
    ]
  },
  "40fa6a62-58ef-481f-9b8c-54d4a41c7c61": {
    "created_at": "2025-11-29T03:30:44.943551",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n\n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n\n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n\n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n\n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \n\nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n\n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n\n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=HTMLResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/summary.html\", \n\n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/keyterms/{session_id}\", response_class=HTMLResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/keyterms.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/questions/{session_id}\", response_class=HTMLResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/questions.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/resources/{session_id}\", response_class=HTMLResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/resources.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n\nPython\nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n\n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n\nPython\n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n\n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n\n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n\nHTML\n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n    <!-- LEFT: Upload + Materials --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n\n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n            <form \n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\" \n            > \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <div id=\"materials-panel\"> \n            <!-- filled by upload_status fragment --> \n            <div id=\"materials-list\"> \n                <em>No materials uploaded yet.</em> \n            </div> \n        </div> \n    </div> \n \n    <!-- RIGHT: Study Tools --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555; margin-top:0;\"> \n            Once you've uploaded all relevant materials, choose what you want \nthe AI to generate. \n\n        </p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n    </div> \n</div> \n \n\n<script> \nlet currentSessionId = null; \n \n// Called from upload_status fragment \nfunction setSessionId(id) { \n    currentSessionId = id; \n    document.getElementById(\"session_id_input\").value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn = document.getElementById(\"btn-keyterms\"); \n    const qBtn = document.getElementById(\"btn-questions\"); \n    const rBtn = document.getElementById(\"btn-resources\"); \n \n    summaryBtn.disabled = keyBtn.disabled = qBtn.disabled = rBtn.disabled = \nfalse; \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \n// -------- Live recording with MediaRecorder -------- \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        // Start recording \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n\n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        // Stop recording \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n</script> \n \n{% endblock %} \n\nHTML\nHTML\n \n2.\u200b base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n    <script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</head> \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b upload_status.html  \n \n\nHTML\nHTML\nHTML\n<h2>{{ title }}</h2> \n<div class=\"output-content\"> \n    {{ content | safe }} \n</div> \n \nc.\u200b upload_list.html  \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li>{{ f.name }} ({{ f.size_kb }} KB)</li> \n    {% endfor %} \n</ul> \n{% else %} \n<p>No files uploaded yet.</p> \n{% endif %} \n \nd.\u200b output.html  \n \n<div id=\"materials-list\"> \n    {% if files %} \n    <ul> \n        {% for f in files %} \n        <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n        {% endfor %} \n    </ul> \n    {% else %} \n    <em>No materials uploaded yet.</em> \n    {% endif %} \n</div> \n \n<script> \n    setSessionId(\"{{ session_id }}\"); \n</script> \n \n \n \n\nCSS\nStatic:  \n1.\u200b styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n\n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n\n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n2.\u200b  \n",
        "added_at": "2025-11-29T03:30:44.993174"
      }
    ]
  },
  "90aca4a1-1840-419c-acd4-26d335afe042": {
    "created_at": "2025-11-29T03:32:22.526683",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T03:32:22.528309"
      },
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T03:53:01.561922"
      },
      {
        "name": "live_recording.webm",
        "type": "audio-live",
        "text": "[Error transcribing live audio: [Errno 2] No such file or directory: 'ffmpeg']",
        "added_at": "2025-11-29T03:53:23.920150"
      }
    ]
  },
  "b855a422-3707-47a7-a82d-bfdc7a0870d6": {
    "created_at": "2025-11-29T03:55:59.963133",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n\n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n\n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n\n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n\n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \n\nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n\n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n\n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=HTMLResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/summary.html\", \n\n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/keyterms/{session_id}\", response_class=HTMLResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/keyterms.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/questions/{session_id}\", response_class=HTMLResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/questions.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/resources/{session_id}\", response_class=HTMLResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/resources.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n\nPython\nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n\n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n\nPython\n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n\n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n\n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n\nHTML\n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n    <!-- LEFT: Upload + Materials --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n\n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n            <form \n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\" \n            > \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <div id=\"materials-panel\"> \n            <!-- filled by upload_status fragment --> \n            <div id=\"materials-list\"> \n                <em>No materials uploaded yet.</em> \n            </div> \n        </div> \n    </div> \n \n    <!-- RIGHT: Study Tools --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555; margin-top:0;\"> \n            Once you've uploaded all relevant materials, choose what you want \nthe AI to generate. \n\n        </p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n    </div> \n</div> \n \n\n<script> \nlet currentSessionId = null; \n \n// Called from upload_status fragment \nfunction setSessionId(id) { \n    currentSessionId = id; \n    document.getElementById(\"session_id_input\").value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn = document.getElementById(\"btn-keyterms\"); \n    const qBtn = document.getElementById(\"btn-questions\"); \n    const rBtn = document.getElementById(\"btn-resources\"); \n \n    summaryBtn.disabled = keyBtn.disabled = qBtn.disabled = rBtn.disabled = \nfalse; \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \n// -------- Live recording with MediaRecorder -------- \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        // Start recording \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n\n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        // Stop recording \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n</script> \n \n{% endblock %} \n\nHTML\nHTML\n \n2.\u200b base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n    <script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</head> \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b upload_status.html  \n \n\nHTML\nHTML\nHTML\n<h2>{{ title }}</h2> \n<div class=\"output-content\"> \n    {{ content | safe }} \n</div> \n \nc.\u200b upload_list.html  \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li>{{ f.name }} ({{ f.size_kb }} KB)</li> \n    {% endfor %} \n</ul> \n{% else %} \n<p>No files uploaded yet.</p> \n{% endif %} \n \nd.\u200b output.html  \n \n<div id=\"materials-list\"> \n    {% if files %} \n    <ul> \n        {% for f in files %} \n        <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n        {% endfor %} \n    </ul> \n    {% else %} \n    <em>No materials uploaded yet.</em> \n    {% endif %} \n</div> \n \n<script> \n    setSessionId(\"{{ session_id }}\"); \n</script> \n \n \n \n\nCSS\nStatic:  \n1.\u200b styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n\n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n\n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n2.\u200b  \n",
        "added_at": "2025-11-29T03:56:00.013374"
      }
    ]
  },
  "04d60d50-887e-4c4e-b436-ecfb4c8067c0": {
    "created_at": "2025-11-29T03:57:55.326043",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n\n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n\n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n\n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n\n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \n\nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n\n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n\n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=HTMLResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/summary.html\", \n\n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/keyterms/{session_id}\", response_class=HTMLResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/keyterms.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/questions/{session_id}\", response_class=HTMLResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/questions.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/resources/{session_id}\", response_class=HTMLResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/resources.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n\nPython\nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n\n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n\nPython\n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n\n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n\n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n\nHTML\n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n    <!-- LEFT: Upload + Materials --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n\n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n            <form \n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\" \n            > \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <div id=\"materials-panel\"> \n            <!-- filled by upload_status fragment --> \n            <div id=\"materials-list\"> \n                <em>No materials uploaded yet.</em> \n            </div> \n        </div> \n    </div> \n \n    <!-- RIGHT: Study Tools --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555; margin-top:0;\"> \n            Once you've uploaded all relevant materials, choose what you want \nthe AI to generate. \n\n        </p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n    </div> \n</div> \n \n\n<script> \nlet currentSessionId = null; \n \n// Called from upload_status fragment \nfunction setSessionId(id) { \n    currentSessionId = id; \n    document.getElementById(\"session_id_input\").value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn = document.getElementById(\"btn-keyterms\"); \n    const qBtn = document.getElementById(\"btn-questions\"); \n    const rBtn = document.getElementById(\"btn-resources\"); \n \n    summaryBtn.disabled = keyBtn.disabled = qBtn.disabled = rBtn.disabled = \nfalse; \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \n// -------- Live recording with MediaRecorder -------- \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        // Start recording \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n\n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        // Stop recording \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n</script> \n \n{% endblock %} \n\nHTML\nHTML\n \n2.\u200b base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n    <script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</head> \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b upload_status.html  \n \n\nHTML\nHTML\nHTML\n<h2>{{ title }}</h2> \n<div class=\"output-content\"> \n    {{ content | safe }} \n</div> \n \nc.\u200b upload_list.html  \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li>{{ f.name }} ({{ f.size_kb }} KB)</li> \n    {% endfor %} \n</ul> \n{% else %} \n<p>No files uploaded yet.</p> \n{% endif %} \n \nd.\u200b output.html  \n \n<div id=\"materials-list\"> \n    {% if files %} \n    <ul> \n        {% for f in files %} \n        <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n        {% endfor %} \n    </ul> \n    {% else %} \n    <em>No materials uploaded yet.</em> \n    {% endif %} \n</div> \n \n<script> \n    setSessionId(\"{{ session_id }}\"); \n</script> \n \n \n \n\nCSS\nStatic:  \n1.\u200b styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n\n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n\n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n2.\u200b  \n",
        "added_at": "2025-11-29T03:57:55.375153"
      }
    ]
  },
  "a506c8fb-a6dc-48f7-95d5-c0fc528b07b7": {
    "created_at": "2025-11-29T04:08:21.231246",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T04:08:21.233831"
      }
    ]
  },
  "cf14c745-0de4-4a7c-83e7-3fee2bb99cf2": {
    "created_at": "2025-11-29T04:23:35.927255",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T04:23:35.930332"
      }
    ]
  },
  "b918876a-6fc4-4bee-a064-c55c5c049324": {
    "created_at": "2025-11-29T04:30:24.644089",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T04:30:24.647233"
      }
    ]
  },
  "3e1f71d6-e4e3-4be3-b108-fc00d6b253a9": {
    "created_at": "2025-11-29T04:30:49.950667",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n\n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n\n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n\n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n\n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \n\nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n\n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n\n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=HTMLResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/summary.html\", \n\n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/keyterms/{session_id}\", response_class=HTMLResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/keyterms.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/questions/{session_id}\", response_class=HTMLResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/questions.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/resources/{session_id}\", response_class=HTMLResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/resources.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n\nPython\nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n\n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n\nPython\n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n\n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n\n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n\nHTML\n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n    <!-- LEFT: Upload + Materials --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n\n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n            <form \n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\" \n            > \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <div id=\"materials-panel\"> \n            <!-- filled by upload_status fragment --> \n            <div id=\"materials-list\"> \n                <em>No materials uploaded yet.</em> \n            </div> \n        </div> \n    </div> \n \n    <!-- RIGHT: Study Tools --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555; margin-top:0;\"> \n            Once you've uploaded all relevant materials, choose what you want \nthe AI to generate. \n\n        </p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n    </div> \n</div> \n \n\n<script> \nlet currentSessionId = null; \n \n// Called from upload_status fragment \nfunction setSessionId(id) { \n    currentSessionId = id; \n    document.getElementById(\"session_id_input\").value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn = document.getElementById(\"btn-keyterms\"); \n    const qBtn = document.getElementById(\"btn-questions\"); \n    const rBtn = document.getElementById(\"btn-resources\"); \n \n    summaryBtn.disabled = keyBtn.disabled = qBtn.disabled = rBtn.disabled = \nfalse; \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \n// -------- Live recording with MediaRecorder -------- \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        // Start recording \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n\n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        // Stop recording \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n</script> \n \n{% endblock %} \n\nHTML\nHTML\n \n2.\u200b base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n    <script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</head> \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b upload_status.html  \n \n\nHTML\nHTML\nHTML\n<h2>{{ title }}</h2> \n<div class=\"output-content\"> \n    {{ content | safe }} \n</div> \n \nc.\u200b upload_list.html  \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li>{{ f.name }} ({{ f.size_kb }} KB)</li> \n    {% endfor %} \n</ul> \n{% else %} \n<p>No files uploaded yet.</p> \n{% endif %} \n \nd.\u200b output.html  \n \n<div id=\"materials-list\"> \n    {% if files %} \n    <ul> \n        {% for f in files %} \n        <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n        {% endfor %} \n    </ul> \n    {% else %} \n    <em>No materials uploaded yet.</em> \n    {% endif %} \n</div> \n \n<script> \n    setSessionId(\"{{ session_id }}\"); \n</script> \n \n \n \n\nCSS\nStatic:  \n1.\u200b styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n\n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n\n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n2.\u200b  \n",
        "added_at": "2025-11-29T04:30:50.001591"
      }
    ]
  },
  "7e9862b2-6d67-4353-9101-7bf56900bbf9": {
    "created_at": "2025-11-29T04:46:39.550723",
    "files": [
      {
        "name": "Helektron.txt",
        "type": "document",
        "text": "WebApp:\n\nHelektron Web App Structure\n\u251c\u2500\u2500 flask_app.py\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 setup.sh\n\u251c\u2500\u2500 transcription_cache.json\n\u251c\u2500\u2500 transcription_hash_cache.json\n\u251c\u2500\u2500 whisper.tar.gz\n\u251c\u2500\u2500 whisper-main/\n\u2502   \u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 build/\n\u2502   \u2502   \u2514\u2500\u2500 bin/\n\u2502   \u2514\u2500\u2500 whisper-main/\n\u251c\u2500\u2500 whisper.cpp/\n\u251c\u2500\u2500 static/\n\u2502   \u251c\u2500\u2500 build_vector_store.py\n\u2502   \u2514\u2500\u2500 rag_utils.py\n\u251c\u2500\u2500 templates/\n\u2502   \u251c\u2500\u2500 base.html\n\u2502   \u2514\u2500\u2500 index.html\n\u2514\u2500\u2500 upload/\n    \u251c\u2500\u2500 data/\n    \u2514\u2500\u2500 vs/\n        \u2514\u2500\u2500 docs.json\n\nflask_app.py: \n#!/usr/bin/env python3\nfrom flask import Flask, request, jsonify, render_template, send_from_directory, redirect, url_for\nfrom flask_cors import CORS\nimport os\nimport subprocess\nimport tempfile\nimport uuid\nfrom datetime import datetime\nfrom typing import Dict, List, Optional\nimport json\nimport requests\nimport hashlib  \nimport threading\nfrom static.rag_utils import retrieveChunksFull\nfrom static.build_vector_store import build_vector_store\nfrom langchain.schema import Document\nimport random\n\napp = Flask(__name__, \n           template_folder='templates',\n           static_folder='static')\nCORS(app)\n\nRAG_DATA_DIR = 'upload/data'\nos.makedirs(RAG_DATA_DIR, exist_ok=True)\n\n# Configuration\nUPLOAD_FOLDER = 'upload'\nMAX_CONTENT_LENGTH = 500 * 1024 * 1024  # 500MB max file size\nALLOWED_EXTENSIONS = {'mp4', 'm4a'}  # MODIFIED\n\napp.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\napp.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH\n\nos.makedirs(UPLOAD_FOLDER, exist_ok=True)\nos.makedirs('templates', exist_ok=True)\nos.makedirs('static', exist_ok=True)\n\n# Global cache for file index\ntranscription_files: Dict[int, dict] = {}\ncurrent_file_id = 1\n\n# Path to save transcription data\nTRANSCRIPTION_CACHE_FILE = 'transcription_cache.json'\n\n# Global cache for file content hashes\nTRANSCRIPTION_HASH_CACHE_FILE = 'transcription_hash_cache.json'\nif os.path.exists(TRANSCRIPTION_HASH_CACHE_FILE):\n    with open(TRANSCRIPTION_HASH_CACHE_FILE, 'r') as f:\n        transcription_hash_cache = json.load(f)\nelse:\n    transcription_hash_cache = {}\n\n# Load from file-index cache\ndef load_existing_files():\n    global transcription_files, current_file_id\n    if os.path.exists('transcription_cache.json'):\n        with open('transcription_cache.json', 'r') as f:\n            transcription_files.update(json.load(f))\n        current_file_id = max(map(int, transcription_files.keys())) + 1\n    else:\n        current_file_id = 1\n\n# Save transcription data to the JSON file\ndef save_transcription_data():\n    with open(TRANSCRIPTION_CACHE_FILE, 'w') as f:\n        json.dump(transcription_files, f, indent=2, default=str)\n    \n# Call the loader at startup\nload_existing_files()\n\n# Use a stable key for caching\ndef get_file_hash(file_path):\n    with open(file_path, 'rb') as f:\n        return hashlib.sha256(f.read()).hexdigest()\n\ndef allowed_file(filename):\n    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\ndef run_whisper_transcription(file_id: int, input_path: str) -> str:\n    \"\"\"Run whisper transcription on the audio file\"\"\"\n    print(f\"Starting transcription for file {file_id}: {input_path}\")\n    \n    # Locate whisper binary \n    whisper_paths = [\n        \"./whisper-main/build/bin/whisper-cli\",\n        \"./whisper.cpp/build/bin/main\",\n        \"./whisper.cpp/main\",\n        \"./whisper-main/build/bin/main\"\n    ]\n\n    for path in whisper_paths:\n        print(f\"Checking: {path}, exists: {os.path.exists(path)}\")\n\n    whisper_binary = next((path for path in whisper_paths if os.path.exists(path)), None)\n    model_path = \"./whisper.cpp/models/ggml-base.en.bin\"\n    \n    if not whisper_binary:\n        raise RuntimeError(\"Whisper binary not found in expected paths.\")\n    print(f\"Found whisper binary: {whisper_binary}\")\n    \n    if not os.path.exists(model_path):\n        available = os.listdir(\"./whisper.cpp/models/\") if os.path.exists(\"./whisper.cpp/models/\") else \"models directory not found\"\n        raise RuntimeError(f\"Model not found at {model_path}. Available: {available}\")\n\n    # Convert video to audio first using ffmpeg\n    audio_path = input_path + '.wav'\n    print(f\"Converting video to audio: {input_path} -> {audio_path}\")\n    \n    try:\n        # Convert video to audio with whisper-compatible parameters\n        ffmpeg_cmd = [\n            \"ffmpeg\", \"-i\", input_path,\n            \"-acodec\", \"pcm_s16le\",\n            \"-ar\", \"16000\",\n            \"-ac\", \"1\",\n            \"-y\", audio_path\n        ]\n        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise Exception(f\"Audio conversion failed: {result.stderr}\")\n        print(f\"Audio conversion completed: {audio_path}\")\n    except Exception as e:\n        raise RuntimeError(f\"Audio conversation failed: {e}\")\n\n    # Run whisper transcription\n    output_file = input_path + \".txt\"\n    whisper_cmd = [\n        whisper_binary,\n        \"-m\", model_path,\n        \"-f\", audio_path,\n        \"--output-txt\",\n        \"--output-file\", output_file.replace('.txt', '')\n    ]\n        \n    try:\n        print(f\"Running whisper: {' '.join(whisper_cmd)}\")\n        result = subprocess.run(whisper_cmd, capture_output=True, text=True)\n        if result.returncode != 0:\n            raise RuntimeError(f\"Whisper transcription failed: {result.stderr.strip()}\")\n\n        if not os.path.exists(output_file):\n            raise FileNotFoundError(f\"Output file not found: {output_file}\")\n\n        with open(output_file, 'r') as f:\n            transcription = f.read().strip()\n\n        # Clean up\n        os.unlink(audio_path)\n        os.unlink(output_file)\n\n        return transcription\n\n    except Exception as e:\n        print(f\"[ERROR] Transcription failed: {e}\")\n        raise\n\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n@app.template_filter('to_datetime')\ndef to_datetime(value):\n    return datetime.fromisoformat(value).strftime('%b %d, %Y %H:%M')\n\n@app.route('/transcription/<int:file_id>')\ndef view_transcription(file_id):\n    if file_id not in transcription_files:\n        return redirect(url_for('index'))\n\n    file_data = transcription_files[file_id]\n    return render_template('index.html', file_data=file_data)\n\n@app.route('/api/files', methods=['GET'])\ndef get_files():\n    completed = [f for f in transcription_files.values() if f.get(\"status\") == \"completed\"]\n    recent = sorted(completed, key=lambda x: x['uploadedAt'], reverse=True)[:5]\n    return jsonify(recent)\n\n@app.route('/api/upload', methods=['POST'])\ndef upload_file():\n    global current_file_id, transcription_hash_cache\n\n    if 'file' not in request.files:\n        return jsonify({'error': 'No file provided'}), 400\n\n    file = request.files['file']\n    if file.filename == '':\n        return jsonify({'error': 'No file selected'}), 400\n\n    if not allowed_file(file.filename):\n        return jsonify({'error': 'Only MP4 and M4A files are allowed'}), 400\n\n    # Generate hashed filename from original + date\n    original_name = file.filename\n    today_str = datetime.now().strftime('%Y-%m-%d')\n    base_string = f\"{original_name}_{today_str}\"\n    hashed_name = hashlib.md5(base_string.encode()).hexdigest()\n    extension = original_name.rsplit('.', 1)[-1]\n    filename = f\"{hashed_name}.{extension}\"\n    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)\n    file.save(file_path)\n\n    file_record = {\n        'id': current_file_id,\n        'filename': filename,\n        'originalName': original_name,\n        'status': 'uploaded',\n        'uploadedAt': datetime.now().isoformat(),\n        'transcription': None,\n        'summary': None,\n        'issues': None\n    }\n\n    transcription_files[current_file_id] = file_record\n    file_id = current_file_id\n    current_file_id += 1\n\n    # Save file index immediately\n    save_transcription_data()\n\n    # Compute hash of uploaded file\n    file_hash = get_file_hash(file_path)\n\n    # If hash is in transcription cache, reuse the transcription\n    if file_hash in transcription_hash_cache:\n        print(f\"[CACHE HIT] Using cached transcription for: {original_name}\")\n        transcription = transcription_hash_cache[file_hash]\n\n        transcription_files[file_id]['transcription'] = transcription\n        transcription_files[file_id]['status'] = 'completed'\n\n        # Save updated file index and hash cache\n        save_transcription_data()\n        with open('transcription_hash_cache.json', 'w') as f:\n            json.dump(transcription_hash_cache, f, indent=2)\n\n        return jsonify(file_record)\n\n    # Transcribe in background if not cached\n    def transcribe_async():\n        try:\n            # Set status to 'transcribing'\n            transcription_files[file_id]['status'] = 'transcribing'\n            \n            # Run transcription\n            transcription = run_whisper_transcription(file_id, file_path)\n            transcription_files[file_id]['transcription'] = transcription\n            transcription_files[file_id]['status'] = 'completed'\n            print(f\"[DONE] Transcription completed for file {file_id}\")\n\n            # Save the transcription as a RAG-compatible .txt file\n            rag_filename = f\"{file_id}_{file_record['originalName'].rsplit('.', 1)[0]}.txt\"\n            rag_path = os.path.join(RAG_DATA_DIR, rag_filename)\n            with open(rag_path, 'w', encoding='utf-8') as f:\n                f.write(transcription)\n            \n            # Rebuild the FAISS vector store\n            build_vector_store()\n\n            #  Save transcription hash (to avoid recomputation)\n            transcription_hash_cache[file_hash] = transcription\n\n        except Exception as e:\n            print(f\"[ERROR] Transcription failed for file {file_id}: {str(e)}\")\n            transcription_files[file_id]['status'] = 'error'\n            transcription_files[file_id]['error'] = str(e)\n\n        finally:\n            # Save transcription data and hash cache\n            save_transcription_data()\n            with open('transcription_hash_cache.json', 'w') as f:\n                json.dump(transcription_hash_cache, f, indent=2)\n\n        # Return the file record for the front-end\n        return jsonify(file_record)\n\n    # Start the transcription in a background thread\n    thread = threading.Thread(target=transcribe_async)\n    thread.daemon = True\n    thread.start()\n\n    return jsonify(file_record)\n\n\n@app.route('/api/transcription/<int:file_id>', methods=['GET'])\ndef get_transcription(file_id):\n    if file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n    return jsonify(transcription_files[file_id])\n\n# Call Ollama API\ndef call_ollama_api(prompt: str, model: str = \"llama3:latest\", options: Optional[dict] = None) -> Optional[str]:\n    if options is None:\n        options = {\n            \"temperature\": 0.3,\n            \"top_p\": 0.95,\n            \"num_ctx\": 128000\n        }\n    try:\n        response = requests.post(\"http://localhost:11434/api/generate\", json={\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False,\n            \"options\": options\n        }, timeout=1800)\n\n        print(\"Ollama status code:\", response.status_code)\n        print(\"Ollama raw response:\", response.text)\n\n        if response.status_code == 200:\n            return response.json().get(\"response\", \"\").strip()\n        else:\n            print(f\"Ollama API error {response.status_code}: {response.text}\")\n            return None\n    except Exception as e:\n        print(f\"Exception calling Ollama API: {e}\")\n        return None\n\ndef get_summary_prompt(context: str) -> str:\n    return f\"\"\"Based on the following meeting transcription, create a **structured and detailed bullet-point summary**. Organize the output into clear sections:\n\n    - **Meeting Overview**: Purpose and context.\n    - **Key Discussion Topics**: Group related ideas, decisions, or suggestions.\n    - **Decisions Made**: Any agreed-upon plans or conclusions.\n    - **Action Items**: Clearly assigned tasks with responsible parties (or 'TBD') and deadlines if mentioned.\n    - **Open Questions / Follow-Ups**: Any unresolved points or items needing further clarification.\n\n    Keep bullet points **concise but precise**. Use professional language. If information is unclear or missing, note it as `TBD`.\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n    \"\"\"\n\ndef generate_gitlab_issues_prompt(context: str) -> str:\n    return f\"\"\"You are an expert technical writer and a strict formatter. Your ONLY job is to extract individual, self-contained technical issues from the transcript and output each one using the exact GitLab Issue Template format.\n\n    DO NOT:\n    - Do NOT skip any template section.\n    - Do NOT modify section headers or formatting.\n    - Do NOT write anything before or after the issue blocks.\n\n    STRICT REQUIREMENTS:\n    - Each issue must start with this line exactly: `**Pelagic Issue**`\n    - Each issue must use proper markdown, including `---` separators and `**Bolded headers**` exactly as shown.\n    - If a section is missing from the transcript, write `To be defined` for that field.\n    - Be specific and descriptive in the **Description** and **Definition of Done**.\n    - Do NOT merge multiple issues into one ticket. You MUST extract one issue per distinct technical idea or task.\n    - In **Issue Type** and **Priority**, mark only one box with an `x` and leave the rest blank.\n\n    ---\n\n     Below is the actual transcription. Use it to generate GitLab issues using the template that follows\n\n    ---BEGIN TRANSCRIPTION---\n    {context}\n    ---END TRANSCRIPTION---\n\n    GitLab Issue Template (follow exactly for each issue). Use the full template below for each issue you extract. Repeat the entire block for every issue:\n\n    ---\n    **Pelagic Issue**\n    ---\n    **Description:**\n    [Provide a clear and concise description of the issue or task.]\n\n    Text Here\n\n    ---\n    **Definition of Done:** [Must be measurable.]\n\n    DoD:\n    Text Here\n\n    ---\n    **Requestor:**\n    [Contact information for who is calling for the issue.]\n\n\n    ---\n    **Rationale:**\n    [Why is this a requirement?]\n\n    Text Here\n\n    ---\n    **Issue Type:**\n\n    - [ ] Bug\n    - [ ] Feature Request\n    - [ ] Task\n    - [ ] Improvement\n    - [ ] Other\n    ---\n    **Job Number:**\n    To be defined\n\n    ---\n    **Priority:**\n\n    - [ ] High\n    - [ ] Medium\n    - [ ] Low\n\n    ---\n    **Developer Notes:**\n    [Developers are welcome to add notes here instead of comments.]\n\n    Text Here\n\n    ---\n    \"\"\"\n\n@app.route('/api/summary', methods=['POST'])\ndef generate_summary():\n    \"\"\"Generate AI summary from transcription\"\"\"\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('summary'):  # already generated\n        return jsonify({'summary': file_data['summary']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'No transcription text provided'}), 400\n\n    prompt = get_summary_prompt(transcription_text)\n\n    summary = call_ollama_api(prompt)\n    if summary is None:\n        return jsonify({'error': 'Failed to generate summary from Ollama'}), 500\n\n    transcription_files[file_id]['summary'] = summary\n    save_transcription_data()\n    return jsonify({'summary': summary}) # Make sure summaries aren't regenerated with every click to \"Meeting Summary\" \n\n@app.route('/api/issues', methods=['POST'])\ndef generate_gitlab_issues():\n    data = request.get_json()\n    file_id = data.get('file_id')\n\n    if not file_id or file_id not in transcription_files:\n        return jsonify({'error': 'File not found'}), 404\n\n    file_data = transcription_files[file_id]\n\n    if file_data.get('issues'):  # Already generated\n        return jsonify({'issues': file_data['issues']})\n\n    transcription_text = file_data.get('transcription', '')\n    if not transcription_text:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    # RAG retrieval + include current transcript using randomization \n    vector_store_path = \"upload/vs\"\n\n    try:\n        # Get top 3 relevant chunks \n        top_k = 3\n        query = transcription_text[:1000] # Use first 1000 characters of the transcription\n        top_results = retrieveChunksFull(vector_store_path, query, num_chunks=top_k) # Pass query to retrieveChunksFull(..) to perform a vector similarity search against past transcript chunks\n\n        # Load full corpus from docs.json\n        docs_path = os.path.join(vector_store_path, \"docs.json\")\n        with open(docs_path, \"r\") as f:\n            all_docs = [Document(**d) for d in json.load(f)]\n        \n        # Remove any top_results to avoid duplication F\n        used_set = set(doc.page_content for doc in top_results)\n        random_pool = [doc for doc in all_docs if doc.page_content not in used_set]\n\n        # Sample 2 random chunks\n        random_chunks = random.sample(random_pool, min(2, len(random_pool)))\n\n        # Combine context\n        selected_chunks = top_results + random_chunks\n        context_chunks = [doc.page_content for doc in selected_chunks]\n        context_chunks.append(transcription_text) # add current transcript\n        context = \"\\n\\n\".join(context_chunks)\n\n        # logging\n        print(\"[RAG] Using top 3 + 2 random chunks from vector store:\")\n        for doc in selected_chunks:\n            print(\" -\", doc.metadata.get(\"source\"))\n\n    except Exception as e:\n        return jsonify({'error': f'RAG retrieval failed: {str(e)}'}), 500\n\n\n    # Build prompt with context \n    prompt = generate_gitlab_issues_prompt(context)\n\n    issues = call_ollama_api(prompt)\n    if issues is None:\n        return jsonify({'error': 'Failed to generate GitLab issues from Ollama'}), 500\n\n    transcription_files[file_id]['issues'] = issues\n    save_transcription_data()\n    return jsonify({'issues': issues})\n\n@app.route('/static/<path:filename>')\ndef serve_static(filename):\n    return send_from_directory('static', filename)\n\nif __name__ == '__main__':\n    print(\"Starting Flask server with Jinja templates...\")\n    print(\"Server ready on http://localhost:5000\")\n    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True)\n\nstatic/build_vector_store.py: \nimport os\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain.docstore.document import Document\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nBASE_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\nDATA_DIR = os.path.join(BASE_DIR, \"upload\", \"data\")\nVECTOR_STORE_PATH = os.path.join(BASE_DIR, \"upload\", \"vs\")\nINDEX_NAME = 'vs'\n\ndef build_vector_store():\n    print(f\"Building vector store from {DATA_DIR}...\")\n\n    all_chunks = []\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,     # ~750 tokens\n        chunk_overlap=200    # contextual overlap\n    )\n\n    for filename in os.listdir(DATA_DIR):\n        if filename.endswith(\".txt\"):\n            filepath = os.path.join(DATA_DIR, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                text = f.read().strip()\n                if text:\n                    chunks = splitter.split_text(text)\n                    for chunk in chunks:\n                        all_chunks.append(Document(page_content=chunk, metadata={'source': filename}))\n\n    if not all_chunks:\n        print(\"No documents to index.\")\n        return\n\n    embeddings = HuggingFaceEmbeddings()\n    vectorstore = FAISS.from_documents(all_chunks, embeddings)\n    vectorstore.save_local(VECTOR_STORE_PATH, index_name=INDEX_NAME)\n\n    # Save doc metadata separately as JSON\n    doc_dump_path = os.path.join(VECTOR_STORE_PATH, \"docs.json\")\n    with open(doc_dump_path, \"w\") as f:\n        json.dump([doc.dict() for doc in all_chunks], f)\n\n    print(f\"Vector store saved to {VECTOR_STORE_PATH}\")\n\nif __name__ == '__main__':\n    os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)\n    build_vector_store()\n\nstatic/rag_utils.py: \nimport os\nimport faiss\nimport json\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.docstore.in_memory import InMemoryDocstore\nfrom langchain.schema import Document\n\ndef retrieveChunksFull(vector_store_path, query, num_chunks=5):\n    if not os.path.exists(vector_store_path):\n        raise FileNotFoundError(f\"Vector store not found at: {vector_store_path}\")\n\n    index_path = os.path.join(vector_store_path, \"vs.faiss\")\n    docs_path = os.path.join(vector_store_path, \"docs.json\")\n\n    if not os.path.exists(index_path):\n        raise FileNotFoundError(\"Missing FAISS index: vs.faiss\")\n    if not os.path.exists(docs_path):\n        raise FileNotFoundError(\"Missing metadata: docs.json\")\n\n    # Load FAISS index\n    faiss_index = faiss.read_index(index_path)\n\n    # Load metadata\n    with open(docs_path, \"r\") as f:\n        doc_dicts = json.load(f)\n    docs = [Document(**d) for d in doc_dicts]\n\n    docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(docs)})\n    index_to_docstore_id = {i: str(i) for i in range(len(docs))}\n\n    embeddings = HuggingFaceEmbeddings()\n\n    vectorstore = FAISS(\n        embeddings,\n        faiss_index,\n        docstore,\n        index_to_docstore_id=index_to_docstore_id\n    )\n\n    # Return full Document objects\n    return vectorstore.similarity_search(query, k=num_chunks)\n\ntemplates/base.html: \n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>{% block title %}Video Transcription App{% endblock %}</title>\n    <script src=\"https://cdn.tailwindcss.com\"></script>\n    <link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\" rel=\"stylesheet\">\n    <style>\n        .file-upload-area {\n            border: 2px dashed #e5e7eb;\n            transition: all 0.3s ease;\n        }\n        .file-upload-area:hover {\n            border-color: #3b82f6;\n            background-color: #f8fafc;\n        }\n        .file-upload-area.dragover {\n            border-color: #3b82f6;\n            background-color: #eff6ff;\n        }\n        .progress-bar {\n            background: linear-gradient(90deg, #3b82f6, #1d4ed8);\n        }\n        .status-badge {\n            font-size: 0.75rem;\n            padding: 0.25rem 0.5rem;\n            border-radius: 0.375rem;\n        }\n        .uploaded { background-color: #dbeafe; color: #1e40af; }\n        .transcribing { background-color: #fef3c7; color: #d97706; }\n        .completed { background-color: #dcfce7; color: #15803d; }\n        .error { background-color: #fee2e2; color: #dc2626; }\n    </style>\n</head>\n<body class=\"bg-gray-50 min-h-screen\" data-file-id=\"{{ file_data.id if file_data is defined else '' }}\">\n    <header class=\"bg-white shadow-sm border-b\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8\">\n            <div class=\"flex justify-between items-center py-4\">\n                <div class=\"flex items-center space-x-4\">\n                    <img src=\"{{ url_for('static', filename='images/FishEyeLogo.png') }}\" alt=\"FishEye Logo\" class=\"h-8 w-auto\">\n                    <h1 class=\"text-xl font-semibold text-gray-900\">Helektron by FishEye</h1>\n                </div>\n                <div class=\"flex items-center space-x-2\">\n                    <span class=\"text-sm text-gray-500\">Powered by</span>\n                    <span class=\"text-sm font-medium text-gray-700\">Flask + Whisper.cpp + Ollama</span>\n                </div>\n            </div>\n        </div>\n    </header>\n\n    <main class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8\">\n        {% block content %}{% endblock %}\n    </main>\n\n    <footer class=\"bg-white border-t mt-16\">\n        <div class=\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6\">\n            <div class=\"flex justify-between items-center\">\n                <p class=\"text-sm text-gray-500\">\n                    Built with Flask, Whisper.cpp, and Ollama for AI-powered transcription\n                </p>\n                <div class=\"flex space-x-4 text-sm text-gray-500\">\n                    <span>MP4 Support</span>\n                    <span>\u2022</span>\n                    <span>500MB Max</span>\n                    <span>\u2022</span>\n                    <span>Real-time Processing</span>\n                </div>\n            </div>\n        </div>\n    </footer>\n\n    <script>\n        // Auto-refresh functionality for status updates\n        function startPolling() {\n            setInterval(() => {\n                const statusElements = document.querySelectorAll('[data-status=\"transcribing\"]');\n                if (statusElements.length > 0) {\n                    location.reload();\n                }\n            }, 2000);\n        }\n\n        // File upload drag and drop\n        function setupFileUpload() {\n            const uploadArea = document.getElementById('upload-area');\n            if (!uploadArea) return;\n\n            ['dragenter', 'dragover', 'dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, preventDefaults, false);\n            });\n\n            function preventDefaults(e) {\n                e.preventDefault();\n                e.stopPropagation();\n            }\n\n            ['dragenter', 'dragover'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, highlight, false);\n            });\n\n            ['dragleave', 'drop'].forEach(eventName => {\n                uploadArea.addEventListener(eventName, unhighlight, false);\n            });\n\n            function highlight(e) {\n                uploadArea.classList.add('dragover');\n            }\n\n            function unhighlight(e) {\n                uploadArea.classList.remove('dragover');\n            }\n\n            uploadArea.addEventListener('drop', handleDrop, false);\n\n            function handleDrop(e) {\n                const dt = e.dataTransfer;\n                const files = dt.files;\n                handleFiles(files);\n            }\n\n            function handleFiles(files) {\n                if (files.length > 0) {\n                    const file = files[0];\n                    const allowedExtensions = ['mp4', 'm4a'];\n                    const fileExt = file.name.split('.').pop().toLowerCase();\n\n                    if (allowedExtensions.includes(fileExt)) {\n                        uploadFile(file);\n                    } else {\n                        alert('Please upload an MP4 or M4A file');\n                    }\n\n                }\n            }\n        }\n\n        // Initialize on page load\n        document.addEventListener('DOMContentLoaded', function() {\n            setupFileUpload();\n            startPolling();\n        });\n    </script>\n</body>\n</html>\n\ntemplates/index.html: \n{% extends \"base.html\" %}\n\n{% block content %}\n<div class=\"grid grid-cols-1 lg:grid-cols-1 justify-center\">\n    <!-- Upload Section -->\n    <div class=\"max-w-3xl mx-auto\">\n        <div class=\"bg-white rounded-lg shadow-sm border p-6\">\n            <h2 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-upload mr-2 text-blue-600\"></i>\n                Upload Digital Audio File\n            </h2>\n            \n            <div id=\"upload-area\" class=\"file-upload-area rounded-lg p-8 text-center cursor-pointer\">\n                <div class=\"space-y-3\">\n                    <i class=\"fas fa-cloud-upload-alt text-4xl text-gray-400\"></i>\n                    <div>\n                        <p class=\"text-lg font-medium text-gray-700\">Drag and drop your MP4 or M4A file here</p>\n                        <p class=\"text-sm text-gray-500\">or click to select a file</p>\n                    </div>\n                    <div class=\"text-xs text-gray-400\">\n                        Maximum file size: 500MB \u2022 Supported format: MP4, M4A\n                    </div>\n                </div>\n                <input type=\"file\" id=\"file-input\" accept=\".mp4, .m4a\" class=\"hidden\">\n            </div>\n\n            <!-- Upload Progress -->\n            <div id=\"upload-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">Uploading...</span>\n                    <span class=\"text-sm text-gray-500\" id=\"progress-text\">0%</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full transition-all duration-300\" id=\"progress-bar\" style=\"width: 0%\"></div>\n                </div>\n            </div>\n\n            <!-- Transcription Progress -->\n            <div id=\"transcription-progress\" class=\"hidden mt-4\">\n                <div class=\"flex items-center justify-between mb-2\">\n                    <span class=\"text-sm font-medium text-gray-700\">\n                        <i class=\"fas fa-cog fa-spin mr-2\"></i>\n                        Transcribing audio...\n                    </span>\n                    <span class=\"text-sm text-gray-500\" id=\"transcription-time\">Processing...</span>\n                </div>\n                <div class=\"w-full bg-gray-200 rounded-full h-2\">\n                    <div class=\"progress-bar h-2 rounded-full animate-pulse\" style=\"width: 100%\"></div>\n                </div>\n            </div>\n        </div>\n\n        <!-- Processing Options -->\n        <div id=\"processing-options\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <h3 class=\"text-lg font-semibold text-gray-900 mb-4\">\n                <i class=\"fas fa-robot mr-2 text-green-600\"></i>\n                AI Processing Options\n            </h3>\n            \n            <div class=\"grid grid-cols-1 md:grid-cols-3 gap-4\">\n                <button onclick=\"viewTranscription()\" class=\"processing-btn bg-blue-50 border border-blue-200 rounded-lg p-4 text-left hover:bg-blue-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-file-text text-blue-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">View Transcription</div>\n                            <div class=\"text-sm text-gray-500\">Raw text output</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateSummary()\" class=\"processing-btn bg-green-50 border border-green-200 rounded-lg p-4 text-left hover:bg-green-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fas fa-list-ul text-green-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">Meeting Summary</div>\n                            <div class=\"text-sm text-gray-500\">Bullet point summary</div>\n                        </div>\n                    </div>\n                </button>\n\n                <button onclick=\"generateIssues()\" class=\"processing-btn bg-purple-50 border border-purple-200 rounded-lg p-4 text-left hover:bg-purple-100 transition-colors\">\n                    <div class=\"flex items-center space-x-3\">\n                        <i class=\"fab fa-gitlab text-purple-600\"></i>\n                        <div>\n                            <div class=\"font-medium text-gray-900\">GitLab Issues</div>\n                            <div class=\"text-sm text-gray-500\">Project stories</div>\n                        </div>\n                    </div>\n                </button>\n            </div>\n\n                    <!-- Loading symbol-->\n            <div id=\"ai-loading\" class=\"hidden mt-4 text-sm text-gray-500 flex items-center space-x-2\">\n                <i class=\"fas fa-spinner fa-spin text-blue-600\"></i>\n                <span id=\"ai-loading-text\">Processing request...</span>\n            </div>\n        </div>\n\n        <!-- Results Display -->\n        <div id=\"results-section\" class=\"hidden bg-white rounded-lg shadow-sm border p-6 mt-6\">\n            <div class=\"flex items-center justify-between mb-4\">\n                <h3 class=\"text-lg font-semibold text-gray-900\">\n                    <i class=\"fas fa-eye mr-2 text-indigo-600\"></i>\n                    <span id=\"results-title\">Results</span>\n                </h3>\n                <div class=\"flex space-x-2\">\n                    <button onclick=\"copyResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\"> \n                        <i class=\"fas fa-copy mr-1\"></i>Copy\n                    </button>\n                    <button onclick=\"downloadResults()\" class=\"text-sm text-gray-600 hover:text-gray-800\">\n                        <i class=\"fas fa-download mr-1\"></i>Download\n                    </button>\n                </div>\n            </div>\n            <div id=\"results-content\" class=\"prose max-w-none\">\n                <!-- Results will be displayed here -->\n            </div>\n        </div>\n    </div>\n</div>  \n\n<script>\nlet currentFileId = null;\nlet transcriptionStartTime = null;\n\n// File upload handling\ndocument.getElementById('upload-area').addEventListener('click', () => {\n    document.getElementById('file-input').click();\n});\n\ndocument.getElementById('file-input').addEventListener('change', (e) => {\n    if (e.target.files.length > 0) {\n        uploadFile(e.target.files[0]);\n    }\n});\n\nasync function uploadFile(file) {\n    const allowedExtensions = ['mp4', 'm4a'];\n    const fileExt = file.name.split('.').pop().toLowerCase();\n\n    if (!allowedExtensions.includes(fileExt)) {\n        alert('Please upload an MP4 or M4A file');\n        return;\n    }\n\n    const formData = new FormData();\n    formData.append('file', file);\n\n    // Show upload progress\n    document.getElementById('upload-progress').classList.remove('hidden');\n    \n    try {\n        const response = await fetch('/api/upload', {\n            method: 'POST',\n            body: formData\n        });\n\n        if (!response.ok) {\n            throw new Error('Upload failed');\n        }\n\n        const result = await response.json();\n        currentFileId = result.id;\n        transcriptionStartTime = Date.now();\n\n        // Hide upload progress, show transcription progress\n        document.getElementById('upload-progress').classList.add('hidden');\n        document.getElementById('transcription-progress').classList.remove('hidden');\n\n        // Show progress bar even for cached transcriptions\n        setTimeout(() => {\n            pollTranscriptionStatus();\n        }, 800);\n    } catch (error) {\n        console.error('Upload error:', error);\n        alert('Upload failed. Please try again.');\n        document.getElementById('upload-progress').classList.add('hidden');\n    }\n}\n\nasync function pollTranscriptionStatus() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.status === 'completed') {\n            const elapsed = (Date.now() - transcriptionStartTime) / 1000;\n            const minDisplayTime = 2; // seconds\n\n            if (elapsed < minDisplayTime) {\n                setTimeout(() => {\n                    document.getElementById('transcription-progress').classList.add('hidden');\n                    document.getElementById('processing-options').classList.remove('hidden');\n                }, (minDisplayTime - elapsed) * 1000);\n            } else {\n                document.getElementById('transcription-progress').classList.add('hidden');\n                document.getElementById('processing-options').classList.remove('hidden');\n            }\n        } else if (data.status === 'error') {\n            document.getElementById('transcription-progress').classList.add('hidden');\n            alert('Transcription failed: ' + (data.error || 'Unknown error'));\n        } else {\n            // Update time display\n            if (transcriptionStartTime) {\n                const elapsed = Math.floor((Date.now() - transcriptionStartTime) / 1000);\n                document.getElementById('transcription-time').textContent = `${elapsed}s elapsed`;\n            }\n            // Continue polling\n            setTimeout(pollTranscriptionStatus, 2000);\n        }\n    } catch (error) {\n        console.error('Status polling error:', error);\n        setTimeout(pollTranscriptionStatus, 2000);\n    }\n}\n\nasync function viewTranscription() {\n    if (!currentFileId) return;\n\n    try {\n        const response = await fetch(`/api/transcription/${currentFileId}`);\n        const data = await response.json();\n\n        if (data.transcription) {\n            showResults('Transcription', data.transcription);\n        }\n    } catch (error) {\n        console.error('Error fetching transcription:', error);\n    }\n}\n\nasync function generateSummary() {\n    if (!currentFileId) return;\n\n    // Show loading spinner\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating summary...';\n\n    try {\n        const response = await fetch('/api/summary', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Summary generation failed');\n        }\n\n        const data = await response.json();\n        showResults('Meeting Summary', data.summary);\n    } catch (error) {\n        console.error('Error generating summary:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nasync function generateIssues() {\n    if (!currentFileId) return;\n\n    document.getElementById('ai-loading').classList.remove('hidden');\n    document.getElementById('ai-loading-text').textContent = 'Generating GitLab issues...';\n\n    try {\n        const response = await fetch('/api/issues', {\n            method: 'POST',\n            headers: { 'Content-Type': 'application/json' },\n            body: JSON.stringify({ file_id: currentFileId })\n        });\n\n        if (!response.ok) {\n            const errorData = await response.json();\n            throw new Error(errorData.error || 'Issue generation failed');\n        }\n\n        const data = await response.json();\n        showResults('GitLab Issues', data.issues);\n    } catch (error) {\n        console.error('Error generating issues:', error);\n        alert(error.message);\n    } finally {\n        document.getElementById('ai-loading').classList.add('hidden');\n    }\n}\n\nfunction showResults(title, content) {\n    document.getElementById('results-title').textContent = title;\n    document.getElementById('results-content').innerHTML = `<pre class=\"whitespace-pre-wrap text-sm\">${content}</pre>`;\n    document.getElementById('results-section').classList.remove('hidden');\n}\n\nfunction copyResults() {\n    const content = document.getElementById('results-content').textContent;\n\n    // Modern Clipboard API \n    if (navigator.clipboard && window.isSecureContext) {\n        navigator.clipboard.writeText(content)\n            .then(() => {\n                alert('Copied to clipboard!');\n            })\n            .catch((err) => {\n                console.error('Clipboard copy failed:', err);\n                fallbackCopy(content);\n            });\n    } else {\n        // Fallback for HTTP \n        fallbackCopy(content);\n    }\n\n    function fallbackCopy(text) {\n        const textarea = document.createElement('textarea');\n        textarea.value = text;\n        textarea.style.position = 'fixed';\n        textarea.style.top = 0;\n        textarea.style.left = 0;\n        document.body.appendChild(textarea);\n        textarea.focus();\n        textarea.select();\n\n        try {\n            const success = document.execCommand('copy');\n            alert(success ? 'Copied to clipboard!' : 'Copy failed');\n        } catch (err) {\n            console.error('Fallback copy failed:', err);\n        } finally {\n            document.body.removeChild(textarea);\n        }\n    }\n}\n\nfunction downloadResults() {\n    const content = document.getElementById('results-content').textContent;\n    const title = document.getElementById('results-title').textContent;\n    const blob = new Blob([content], { type: 'text/plain' });\n    const url = URL.createObjectURL(blob);\n    const a = document.createElement('a');\n    a.href = url;\n    a.download = `${title.toLowerCase().replace(/\\s+/g, '_')}.txt`;\n    a.click();\n    URL.revokeObjectURL(url);\n}\n</script>\n{% endblock %}\n\nsetup.sh: \n#!/bin/bash\n\n# setup.sh \u2014 Setup script for virtual environment and dependencies\n\necho \"Creating Python virtual environment...\"\npython3 -m venv venv\n\necho \"Activating virtual environment...\"\nsource venv/bin/activate\n\necho \"Upgrading pip...\"\npip install --upgrade pip\n\necho \"Installing Python dependencies from requirements.txt...\"\npip install -r requirements.txt\n\n# --------- OLLAMA SETUP ---------\necho \"Checking for existing Ollama installation...\"\n\n# Make sure installed Ollama verion is 0.9.2+\nREQUIRED_OLLAMA_VERSION=\"0.9.2\"\nMIN_OLLAMA_VERSION_OK=false\n\nif command -v ollama &> /dev/null; then\n    INSTALLED_OLLAMA_VERSION=$(ollama --version | grep -oP '\\d+\\.\\d+\\.\\d+')\n    echo \"Detected Ollama version: $INSTALLED_OLLAMA_VERSION\"\n\n    if [ \"$(printf '%s\\n' \"$REQUIRED_OLLAMA_VERSION\" \"$INSTALLED_OLLAMA_VERSION\" | sort -V | head -n1)\" = \"$REQUIRED_OLLAMA_VERSION\" ]; then\n        echo \"Ollama version is up-to-date (>= $REQUIRED_OLLAMA_VERSION)\"\n        MIN_OLLAMA_VERSION_OK=true\n    else\n        echo \"WARNING: Ollama version is too old. Reinstalling...\"\n    fi\nelse\n    echo \"Ollama not found. Installing...\"\nfi\n\nif [ \"$MIN_OLLAMA_VERSION_OK\" = false ]; then\n    curl -fsSL https://ollama.com/install.sh | sh\nfi\n\necho \"Cleaning up any leftover local ollama binaries...\"\nrm -f ./ollama\n\n# --------- MODEL SETUP ---------\n\necho \"Pulling LLaMA 3 model...\"\nollama pull llama3:latest\n\n# --------- WHISPER SETUP ---------\n\n# Clone and build whisper.cpp\nif [ ! -d whisper.cpp ]; then\n    echo \"Cloning whisper.cpp repository...\"\n    git clone https://github.com/ggerganov/whisper.cpp\nfi\n\necho \"Compiling whisper.cpp...\"\ncd whisper.cpp && make && cd ..\n\n# --------- FFMPEG INSTALLATION ---------\n\necho \"Installing FFmpeg...\"\nsudo dnf install -y ffmpeg\n\n# --------- CONFIRMATION ---------\n\necho \"Setup complete\"\necho \"To activate the environment later, run: source venv/bin/activate\"\n\nREADME.md: \n# Helektron by Fisheye\n\n## Overview\n\nWhisperTranscriber is a Flask web app that lets users:\n\n- Upload `.mp4` or `.m4a` audio/video files\n- Transcribe them locally using [whisper.cpp](https://github.com/ggerganov/whisper.cpp)\n- Generate AI-powered:\n  - Bullet-point meeting summaries\n  - GitLab-style project issues\n- Retrieve relevant past transcripts via vector search (RAG)\n\n\n## Getting Started\n\n### 1. Clone the repository\n\n```bash\ngit clone git@pelagic-gitlab.fisheye.net:helektron/helektron.git\ncd WhisperTranscriber2\n```\n\n### 2. Run setup script\n\nThis script will:\n- Create and activate a Python virtual environment\n- Install all required Python packages\n- Install Ollama (if not already installed)\n- Install and compile `whisper.cpp`\n- Install `ffmpeg`\n\n```bash\nchmod +x setup.sh\n./setup.sh\n```\n### 3. Run the Flask app\n\nActivate your environment and launch the server:\n\n```bash\nsource venv/bin/activate\npython flask_app.py\n```\n\nVisit `http://192.168.168.54:5000` in your browser.\n\n---\n\n## UI/UX Stack\n\n- **Frontend**: \n  - **HTML** is used to structure the content of the pages.\n  - **Jinja2** templates are used to inject dynamic content from Flask into the HTML pages. \n    - Files: `index.html` (file upload interface), and `base.html` (shared layout).\n- **Styling**: Basic CSS for layout and design\n- **Interactivity**:\n  - Drag & drop uploads\n  - Real-time transcription status polling\n  - Copy/download actions\n\n---\n\n## Backend Stack\n\n| Component        | Description                                                           |\n|------------------|-----------------------------------------------------------------------|\n| **Flask**        | Python 3.11 web framework                                              |\n| **whisper.cpp**  | Fast, local speech-to-text (no OpenAI keys required)                  |\n| **Ollama**       | Local LLM (default: `llama3`) for summaries and issues                |\n| **FAISS**        | Vector search for retrieving similar transcript chunks                |\n| **Threading**    | Transcription runs in background thread per upload                    |\n| **JSON cache**   | `transcription_cache.json` stores all file metadata and AI outputs    |\n| **JSON hash cache**   | `transcription_hash_cache.json` maps file hashes to transcription strings    |\n| **RAG Integration**| Automatic retrieval of relevant transcript chunks for GitLab Issue generation |\n---\n\n## App Workflow\n\n1. User uploads `.mp4` or `.m4a` via drag/drop\n\n2. `ffmpeg` converts to `.wav`, then `whisper.cpp` generates `.txt`\n\n3. Transcription is saved to:\n   - `upload/data/` (for vector search)\n   - `transcription_cache.json`\n\n4. User can click:\n   - \"View Transcription\"\n   - \"Generate Summary\"\n   - \"Generate GitLab Issues\"\n\n5. Summaries and issues are generated using Ollama (llama3) via local API\n\n6. All data is cached to avoid recomputation\n\n7. The vector store is automatically rebuilt after every new transcription\n\n\n\n## API Endpoints\n\n\n| Endpoint                  | Description                              |\n|---------------------------|------------------------------------------|\n| `/`                       | Upload interface                         |\n| `/transcription/<id>`     | View results per file                    |\n| `/api/upload`             | Accept file uploads (POST)               |\n| `/api/files`              | Return recent uploaded files             |\n| `/api/transcription/<id>` | Get status/transcript for a file         |\n| `/api/summary`            | Generate summary via Ollama              |\n| `/api/issues`             | Generate GitLab issues via Ollama        |\n\n\n\n--- \n\n## Notes\n\n- Output files are cached in `transcription_cache.json` to avoid regeneration.\n- Whisper output goes to `upload/data/` and uploaded files live in `upload/vs/`.\n\nrequirements.txt: \naiohappyeyeballs==2.6.1\naiohttp==3.12.14\naiosignal==1.4.0\nannotated-types==0.7.0\nanyio==4.9.0\nasync-timeout==4.0.3\nattrs==25.3.0\nblinker==1.9.0\ncertifi==2025.7.9\ncharset-normalizer==3.4.2\nclick==8.1.8\ndataclasses-json==0.6.7\nexceptiongroup==1.3.0\nfaiss-cpu==1.11.0.post1\nfilelock==3.18.0\nFlask==3.1.1\nflask-cors==6.0.1\nfrozenlist==1.7.0\nfsspec==2025.7.0\ngreenlet==3.2.3\nh11==0.16.0\nhf-xet==1.1.5\nhttpcore==1.0.9\nhttpx==0.28.1\nhttpx-sse==0.4.1\nhuggingface-hub==0.33.4\nidna==3.10\nimportlib_metadata==8.7.0\nitsdangerous==2.2.0\nJinja2==3.1.6\njoblib==1.5.1\njsonpatch==1.33\njsonpointer==3.0.0\nlangchain==0.3.26\nlangchain-community==0.3.27\nlangchain-core==0.3.72\nlangchain-huggingface==0.3.0\nlangchain-ollama==0.3.6\nlangchain-text-splitters==0.3.8\nlangsmith==0.4.6\nMarkupSafe==3.0.2\nmarshmallow==3.26.1\nmpmath==1.3.0\nmultidict==6.6.3\nmypy_extensions==1.1.0\nnetworkx==3.2.1\nnumpy==1.26.4\nollama==0.5.1\norjson==3.11.0\npackaging==24.2\npillow==11.3.0\npropcache==0.3.2\npydantic==2.11.7\npydantic-settings==2.10.1\npydantic_core==2.33.2\npython-dotenv==1.1.1\nPyYAML==6.0.2\nregex==2024.11.6\nrequests==2.32.4\nrequests-toolbelt==1.0.0\nsafetensors==0.5.3\nscikit-learn==1.6.1\nscipy==1.13.1\nsentence-transformers==5.0.0\nsniffio==1.3.1\nSQLAlchemy==2.0.41\nsympy==1.14.0\ntenacity==9.1.2\nthreadpoolctl==3.6.0\ntokenizers==0.21.2\ntorch==2.2.2\ntorchaudio==2.2.2\ntorchvision==0.17.2\ntqdm==4.67.1\ntransformers==4.53.2\ntriton==2.2.0\ntyping-inspect==0.9.0\ntyping-inspection==0.4.1\ntyping_extensions==4.14.1\nurllib3==2.5.0\nWerkzeug==3.1.3\nyarl==1.20.1\nzipp==3.23.0\nzstandard==0.23.0",
        "added_at": "2025-11-29T04:46:39.555872"
      }
    ]
  },
  "05a2d245-5d53-46c7-ade4-8ebe4d99cb2e": {
    "created_at": "2025-11-29T04:56:20.450143",
    "files": [
      {
        "name": "styles.css",
        "type": "document",
        "text": "[Unsupported file type: css]",
        "added_at": "2025-11-29T04:56:20.454873"
      }
    ]
  },
  "83be6559-bd3d-4156-a628-5192d625e480": {
    "created_at": "2025-11-29T04:59:52.047149",
    "files": [
      {
        "name": "styles.css",
        "type": "document",
        "text": "[Unsupported file type: css]",
        "added_at": "2025-11-29T04:59:52.050108"
      },
      {
        "name": "styles.css",
        "type": "document",
        "text": "[Unsupported file type: css]",
        "added_at": "2025-11-29T05:04:53.895368"
      }
    ]
  },
  "a7735f7f-1a5c-4031-b97d-5ab025b83b6c": {
    "created_at": "2025-11-29T05:08:15.193558",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n\n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n\n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n\n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n\n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \n\nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n\n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n\n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=HTMLResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/summary.html\", \n\n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/keyterms/{session_id}\", response_class=HTMLResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/keyterms.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/questions/{session_id}\", response_class=HTMLResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/questions.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/resources/{session_id}\", response_class=HTMLResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/resources.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n\nPython\nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n\n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n\nPython\n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n\n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n\n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n\nHTML\n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n    <!-- LEFT: Upload + Materials --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n\n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n            <form \n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\" \n            > \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <div id=\"materials-panel\"> \n            <!-- filled by upload_status fragment --> \n            <div id=\"materials-list\"> \n                <em>No materials uploaded yet.</em> \n            </div> \n        </div> \n    </div> \n \n    <!-- RIGHT: Study Tools --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555; margin-top:0;\"> \n            Once you've uploaded all relevant materials, choose what you want \nthe AI to generate. \n\n        </p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n    </div> \n</div> \n \n\n<script> \nlet currentSessionId = null; \n \n// Called from upload_status fragment \nfunction setSessionId(id) { \n    currentSessionId = id; \n    document.getElementById(\"session_id_input\").value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn = document.getElementById(\"btn-keyterms\"); \n    const qBtn = document.getElementById(\"btn-questions\"); \n    const rBtn = document.getElementById(\"btn-resources\"); \n \n    summaryBtn.disabled = keyBtn.disabled = qBtn.disabled = rBtn.disabled = \nfalse; \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \n// -------- Live recording with MediaRecorder -------- \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        // Start recording \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n\n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        // Stop recording \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n</script> \n \n{% endblock %} \n\nHTML\nHTML\n \n2.\u200b base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n    <script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</head> \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b upload_status.html  \n \n\nHTML\nHTML\nHTML\n<h2>{{ title }}</h2> \n<div class=\"output-content\"> \n    {{ content | safe }} \n</div> \n \nc.\u200b upload_list.html  \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li>{{ f.name }} ({{ f.size_kb }} KB)</li> \n    {% endfor %} \n</ul> \n{% else %} \n<p>No files uploaded yet.</p> \n{% endif %} \n \nd.\u200b output.html  \n \n<div id=\"materials-list\"> \n    {% if files %} \n    <ul> \n        {% for f in files %} \n        <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n        {% endfor %} \n    </ul> \n    {% else %} \n    <em>No materials uploaded yet.</em> \n    {% endif %} \n</div> \n \n<script> \n    setSessionId(\"{{ session_id }}\"); \n</script> \n \n \n \n\nCSS\nStatic:  \n1.\u200b styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n\n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n\n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n2.\u200b  \n",
        "added_at": "2025-11-29T05:08:15.253991"
      }
    ]
  },
  "a017f018-1e4c-4735-8b17-ff4217c43a58": {
    "created_at": "2025-11-29T05:08:39.467148",
    "files": [
      {
        "name": "styles.css",
        "type": "document",
        "text": "[Unsupported file type: css]",
        "added_at": "2025-11-29T05:08:39.475290"
      }
    ]
  },
  "443b3f6a-d3ed-4080-bf4d-37a658ce8fd1": {
    "created_at": "2025-11-29T05:09:21.584862",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n\n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n\n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n\n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n\n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \n\nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n\n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n\n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=HTMLResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/summary.html\", \n\n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/keyterms/{session_id}\", response_class=HTMLResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/keyterms.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/questions/{session_id}\", response_class=HTMLResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/questions.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/resources/{session_id}\", response_class=HTMLResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/resources.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n\nPython\nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n\n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n\nPython\n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n\n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n\n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n\nHTML\n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n    <!-- LEFT: Upload + Materials --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n\n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n            <form \n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\" \n            > \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <div id=\"materials-panel\"> \n            <!-- filled by upload_status fragment --> \n            <div id=\"materials-list\"> \n                <em>No materials uploaded yet.</em> \n            </div> \n        </div> \n    </div> \n \n    <!-- RIGHT: Study Tools --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555; margin-top:0;\"> \n            Once you've uploaded all relevant materials, choose what you want \nthe AI to generate. \n\n        </p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n    </div> \n</div> \n \n\n<script> \nlet currentSessionId = null; \n \n// Called from upload_status fragment \nfunction setSessionId(id) { \n    currentSessionId = id; \n    document.getElementById(\"session_id_input\").value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn = document.getElementById(\"btn-keyterms\"); \n    const qBtn = document.getElementById(\"btn-questions\"); \n    const rBtn = document.getElementById(\"btn-resources\"); \n \n    summaryBtn.disabled = keyBtn.disabled = qBtn.disabled = rBtn.disabled = \nfalse; \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \n// -------- Live recording with MediaRecorder -------- \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        // Start recording \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n\n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        // Stop recording \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n</script> \n \n{% endblock %} \n\nHTML\nHTML\n \n2.\u200b base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n    <script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</head> \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b upload_status.html  \n \n\nHTML\nHTML\nHTML\n<h2>{{ title }}</h2> \n<div class=\"output-content\"> \n    {{ content | safe }} \n</div> \n \nc.\u200b upload_list.html  \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li>{{ f.name }} ({{ f.size_kb }} KB)</li> \n    {% endfor %} \n</ul> \n{% else %} \n<p>No files uploaded yet.</p> \n{% endif %} \n \nd.\u200b output.html  \n \n<div id=\"materials-list\"> \n    {% if files %} \n    <ul> \n        {% for f in files %} \n        <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n        {% endfor %} \n    </ul> \n    {% else %} \n    <em>No materials uploaded yet.</em> \n    {% endif %} \n</div> \n \n<script> \n    setSessionId(\"{{ session_id }}\"); \n</script> \n \n \n \n\nCSS\nStatic:  \n1.\u200b styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n\n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n\n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n2.\u200b  \n",
        "added_at": "2025-11-29T05:09:21.675441"
      }
    ]
  },
  "b55d819e-7b78-46c6-b394-3a6f200a2efe": {
    "created_at": "2025-11-29T05:20:29.366320",
    "files": [
      {
        "name": "326 Proj.pdf",
        "type": "document",
        "text": "Python\nmain.py:  \n \nimport os \nimport json \nimport uuid \nimport subprocess \nfrom datetime import datetime \nfrom typing import Dict, Any \n \nimport fitz  # PyMuPDF \nfrom pptx import Presentation \nfrom fastapi import FastAPI, Request, UploadFile, File, Form \nfrom fastapi.responses import HTMLResponse \nfrom fastapi.staticfiles import StaticFiles \nfrom fastapi.templating import Jinja2Templates \nimport google.generativeai as genai \n \n# Ai model  \ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\")) \nGEM_MODEL = genai.GenerativeModel(\"gemini-1.5-flash\") \n \n# ----------------- CONFIG ----------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \nUPLOAD_DIR = os.path.join(BASE_DIR, \"upload\") \nAUDIO_DIR = os.path.join(UPLOAD_DIR, \"audio\") \nDOC_DIR = os.path.join(UPLOAD_DIR, \"docs\") \nSESSIONS_PATH = os.path.join(BASE_DIR, \"sessions.json\") \n \nos.makedirs(UPLOAD_DIR, exist_ok=True) \nos.makedirs(AUDIO_DIR, exist_ok=True) \nos.makedirs(DOC_DIR, exist_ok=True) \n \n#client = (api_key=os.getenv(\"GEMINI_API_KEY\")) \n \napp = FastAPI(title=\"Helektron Study Assistant\") \napp.mount(\"/static\", StaticFiles(directory=os.path.join(BASE_DIR, \"static\")), \nname=\"static\") \ntemplates = Jinja2Templates(directory=os.path.join(BASE_DIR, \"templates\")) \n \n# ----------------- SESSION STORAGE ----------------- \ndef load_sessions() -> Dict[str, Any]: \n    if not os.path.exists(SESSIONS_PATH): \n        return {} \n    with open(SESSIONS_PATH, \"r\") as f: \n\n        try: \n            return json.load(f) \n        except json.JSONDecodeError: \n            return {} \n \ndef save_sessions(sessions: Dict[str, Any]) -> None: \n    with open(SESSIONS_PATH, \"w\") as f: \n        json.dump(sessions, f, indent=2) \n \nsessions: Dict[str, Any] = load_sessions() \n \ndef get_or_create_session(session_id: str | None) -> str: \n    global sessions \n    if session_id and session_id in sessions: \n        return session_id \n    new_id = str(uuid.uuid4()) \n    sessions[new_id] = { \n        \"created_at\": datetime.utcnow().isoformat(), \n        \"files\": [],   # list of {\"name\",\"type\",\"text\"} \n    } \n    save_sessions(sessions) \n    return new_id \n \ndef add_text_to_session(session_id: str, filename: str, file_type: str, text: \nstr) -> None: \n    global sessions \n    if session_id not in sessions: \n        session_id = get_or_create_session(None) \n    sessions[session_id][\"files\"].append({ \n        \"name\": filename, \n        \"type\": file_type, \n        \"text\": text, \n        \"added_at\": datetime.utcnow().isoformat() \n    }) \n    save_sessions(sessions) \n \ndef get_session_text(session_id: str) -> str: \n    session = sessions.get(session_id) \n    if not session: \n        return \"\" \n    blocks = [f\"--- {f['name']} ({f['type']}) ---\\n{f['text']}\" for f in \nsession[\"files\"] if f.get(\"text\")] \n    return \"\\n\\n\".join(blocks) \n \n\n# ----------------- HELPERS: FILE TEXT EXTRACTION ----------------- \ndef extract_txt(path: str) -> str: \n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        return f.read() \n \ndef extract_pdf(path: str) -> str: \n    doc = fitz.open(path) \n    texts = [] \n    for page in doc: \n        texts.append(page.get_text()) \n    doc.close() \n    return \"\\n\".join(texts) \n \ndef extract_pptx(path: str) -> str: \n    prs = Presentation(path) \n    texts = [] \n    for slide in prs.slides: \n        for shape in slide.shapes: \n            if hasattr(shape, \"text\"): \n                texts.append(shape.text) \n    return \"\\n\".join(texts) \n \ndef transcribe_audio(path: str) -> str: \n    # Find whisper binary \n    candidates = [ \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper.cpp\", \"build\", \"bin\", \"main\"), \n        os.path.join(BASE_DIR, \"whisper-main\", \"build\", \"bin\", \"whisper-cli\"), \n    ] \n    whisper_bin = next((c for c in candidates if os.path.exists(c)), None) \n    if not whisper_bin: \n        raise RuntimeError(\"Whisper binary not found. Build whisper.cpp \nfirst.\") \n \n    wav_path = path + \".wav\" \n    subprocess.run( \n        [\"ffmpeg\", \"-y\", \"-i\", path, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \n\"-ac\", \"1\", wav_path], \n        check=True \n    ) \n \n    out_prefix = path + \"_out\" \n    subprocess.run( \n\n        [whisper_bin, \"-m\", os.path.join(BASE_DIR, \"whisper.cpp\", \"models\", \n\"ggml-base.en.bin\"), \n         \"-f\", wav_path, \"--output-txt\", \"--output-file\", out_prefix], \n        check=True \n    ) \n \n    txt_path = out_prefix + \".txt\" \n    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n        transcript = f.read() \n \n    # cleanup \n    for p in [wav_path, txt_path]: \n        if os.path.exists(p): \n            os.remove(p) \n \n    return transcript \n \n# ----------------- GPT PROMPTS ----------------- \ndef get_summary_prompt(transcript: str) -> str: \n    return f\"\"\" \nBased on the following combined study materials (lectures, slides, PDFs, notes, \ntranscripts), \ncreate a **structured, detailed, and academically accurate study summary**. \n \nOrganize the output into the following clearly labeled sections: \n \n- **Overview**: Briefly describe the overall topic and goals. \n- **Key Concepts Introduced**: List and explain major ideas, theories, \nformulas, or processes. \n- **Detailed Topic Breakdown**: Group related ideas and summarize explanations, \nreasoning steps, and relationships. \n- **Important Definitions**: Provide concise definitions for technical terms or \ndomain-specific vocabulary. \n- **Examples or Applications**: Summarize any examples, demonstrations, or \nreal-world applications. \n- **Main Takeaways**: What a student should remember after studying this \nmaterial. \n \nExpectations: \n- Use clear, concise bullet points. \n- Prioritize conceptual correctness. \n- Use simple academic language suitable for undergraduate study. \n- If information is unclear or missing, label it as `TBD`. \n \n\n---BEGIN MATERIAL--- \n{transcript} \n---END MATERIAL--- \n\"\"\" \n \ndef get_keyterms_prompt(text: str) -> str: \n    return f\"\"\" \nFrom the combined study materials below, extract **10\u201320 key terms** with short \ndefinitions. \n \nFor each term, include: \n- The term \n- A 1\u20132 sentence definition \n- (Optional) A quick note on why it matters in this context. \n \nFormat as a bulleted list. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_questions_prompt(text: str) -> str: \n    return f\"\"\" \nCreate **8\u201312 practice questions** based on the combined study materials below. \n \nInclude a mix of: \n- Conceptual understanding questions \n- Short-answer questions \n- Application/problem-style questions (where possible) \n \nDo NOT provide answers, only questions. \nGroup them into sections if appropriate (e.g., 'Conceptual', 'Application'). \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef get_resources_prompt(text: str) -> str: \n    return f\"\"\" \nBased on the topics and concepts in the combined study materials below, \nrecommend **3\u20137 high-quality external resources** for further study. \n \n\nEach resource should include: \n- Title \n- Type (e.g., textbook chapter, .edu article, video lecture) \n- Source (e.g., university, well-known platform) \n- 1\u20132 sentences on why it is relevant. \n \nPrefer: \n- .edu domains \n- Reputable textbooks \n- Well-known educational channels \n \nYou do NOT need to provide URLs, just clear, identifiable references. \n \n---BEGIN MATERIAL--- \n{text} \n---END MATERIAL--- \n\"\"\" \n \ndef call_gemini(prompt: str) -> str: \n    try: \n        response = GEM_MODEL.generate_content(prompt) \n        return response.text.strip() \n    except Exception as e: \n        return f\"[Gemini API Error: {e}]\" \n \n# ----------------- ROUTES ----------------- \n@app.get(\"/\", response_class=HTMLResponse) \ndef index(request: Request): \n    return templates.TemplateResponse(\"index.html\", {\"request\": request}) \n \n# Upload any material (txt, pdf, pptx, mp4, m4a) \n@app.post(\"/upload\", response_class=HTMLResponse) \nasync def upload_material( \n    request: Request, \n    file: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    global sessions \n \n    session_id = get_or_create_session(session_id) \n    filename = file.filename \n    ext = filename.split(\".\")[-1].lower() \n \n    # Save file \n\n    if ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n        save_dir = AUDIO_DIR \n        kind = \"audio\" \n    else: \n        save_dir = DOC_DIR \n        kind = \"document\" \n \n    os.makedirs(save_dir, exist_ok=True) \n    saved_path = os.path.join(save_dir, f\"{uuid.uuid4().hex}_{filename}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await file.read()) \n \n    # Extract text \n    try: \n        if ext == \"txt\": \n            text = extract_txt(saved_path) \n        elif ext == \"pdf\": \n            text = extract_pdf(saved_path) \n        elif ext in [\"pptx\"]: \n            text = extract_pptx(saved_path) \n        elif ext in [\"mp4\", \"m4a\", \"wav\", \"webm\"]: \n            text = transcribe_audio(saved_path) \n        else: \n            text = f\"[Unsupported file type: {ext}]\" \n    except Exception as e: \n        text = f\"[Error processing file {filename}: {e}]\" \n \n    add_text_to_session(session_id, filename, kind, text) \n \n    # Render updated materials list (fragment) \n    session = sessions[session_id] \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# Live transcription: audio blob from the browser \n@app.post(\"/upload_live_audio\", response_class=HTMLResponse) \nasync def upload_live_audio( \n\n    request: Request, \n    audio: UploadFile = File(...), \n    session_id: str | None = Form(None), \n): \n    session_id = get_or_create_session(session_id) \n    filename = audio.filename or \"live_recording.webm\" \n    ext = filename.split(\".\")[-1].lower() \n \n    os.makedirs(AUDIO_DIR, exist_ok=True) \n    saved_path = os.path.join(AUDIO_DIR, f\"live_{uuid.uuid4().hex}.{ext}\") \n \n    with open(saved_path, \"wb\") as f: \n        f.write(await audio.read()) \n \n    try: \n        text = transcribe_audio(saved_path) \n    except Exception as e: \n        text = f\"[Error transcribing live audio: {e}]\" \n \n    add_text_to_session(session_id, filename, \"audio-live\", text) \n    session = sessions[session_id] \n \n    return templates.TemplateResponse( \n        \"fragments/upload_status.html\", \n        { \n            \"request\": request, \n            \"session_id\": session_id, \n            \"files\": session[\"files\"], \n        }, \n    ) \n \n# ---- Study tools (summary, key terms, questions, resources) ---- \n \n@app.get(\"/summary/{session_id}\", response_class=HTMLResponse) \ndef generate_summary(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_summary_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/summary.html\", \n\n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/keyterms/{session_id}\", response_class=HTMLResponse) \ndef generate_keyterms(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_keyterms_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/keyterms.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/questions/{session_id}\", response_class=HTMLResponse) \ndef generate_questions_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_questions_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/questions.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n \n@app.get(\"/resources/{session_id}\", response_class=HTMLResponse) \ndef generate_resources_view(request: Request, session_id: str): \n    text = get_session_text(session_id) \n    if not text.strip(): \n        content = \"No material uploaded yet.\" \n    else: \n        prompt = get_resources_prompt(text) \n        content = call_gemini(prompt) \n \n    return templates.TemplateResponse( \n        \"fragments/resources.html\", \n        {\"request\": request, \"content\": content}, \n    ) \n\nPython\nRag Folder:  \n1.\u200b build_vector_store.py  \n \nimport os \nimport json \nimport faiss \nfrom langchain_community.vectorstores import FAISS \nfrom langchain_huggingface import HuggingFaceEmbeddings \nfrom langchain_core.documents import Document \nfrom langchain_text_splitters import RecursiveCharacterTextSplitter \n \n# -------------------------------------------- \n# Correct project paths \n# -------------------------------------------- \nBASE_DIR = os.path.dirname(os.path.abspath(__file__)) \n \n# Uploaded document text is stored here by your FastAPI app \nDOC_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"docs\") \n \n# Vector store output directory \nVS_DIR = os.path.join(BASE_DIR, \"..\", \"upload\", \"vs\") \n \nos.makedirs(VS_DIR, exist_ok=True) \n \n \n# -------------------------------------------- \n# Load documents from upload/docs/ \n# -------------------------------------------- \ndef load_documents(): \n    docs = [] \n    if not os.path.exists(DOC_DIR): \n        print(f\"Missing directory {DOC_DIR}. No documents found.\") \n        return docs \n \n    for filename in os.listdir(DOC_DIR): \n        path = os.path.join(DOC_DIR, filename) \n        if not os.path.isfile(path): \n            continue \n \n        try: \n            with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f: \n                text = f.read().strip() \n                if text: \n\n                    docs.append(Document(page_content=text, metadata={\"source\": \nfilename})) \n        except Exception as e: \n            print(f\"Error loading {filename}: {e}\") \n \n    return docs \n \n \n# -------------------------------------------- \n# Build vector store \n# -------------------------------------------- \ndef build_vector_store(): \n    print(f\"Building vector store from {DOC_DIR}...\") \n \n    docs = load_documents() \n \n    if not docs: \n        print(\"No documents found in upload/docs/. Upload files in the web app \nfirst.\") \n        return \n \n    print(f\"Loaded {len(docs)} raw documents.\") \n \n    # ------------------------------- \n    # Chunk documents \n    # ------------------------------- \n    splitter = RecursiveCharacterTextSplitter( \n        chunk_size=1000, # ~750 tokens \n        chunk_overlap=200, # contextual overlap \n        length_function=len \n    ) \n \n    split_docs = [] \n    for doc in docs: \n        chunks = splitter.split_text(doc.page_content) \n        for chunk in chunks: \n            split_docs.append( \n                Document( \n                    page_content=chunk, \n                    metadata=doc.metadata  # keep source info \n                ) \n            ) \n \n    print(f\"Created {len(split_docs)} total chunks.\") \n\nPython\n \n    # ------------------------------- \n    # Generate embeddings \n    # ------------------------------- \n    embeddings = HuggingFaceEmbeddings() \n \n    print(\"Generating embeddings...\") \n    vectorstore = FAISS.from_documents(split_docs, embeddings) \n \n    # ------------------------------- \n    # Save FAISS index + metadata \n    # ------------------------------- \n    print(\"Saving FAISS index and metadata...\") \n \n    faiss.write_index(vectorstore.index, os.path.join(VS_DIR, \"vs.faiss\")) \n \n    with open(os.path.join(VS_DIR, \"docs.json\"), \"w\") as f: \n        json.dump([d.dict() for d in split_docs], f, indent=2) \n \n    print(\"\\n\ud83c\udf89 Vector store built successfully!\") \n    print(f\"Saved to: {VS_DIR}\") \n \n \nif __name__ == \"__main__\": \n    build_vector_store() \n \n2.\u200b rag_utils.py \n \nimport os \nimport json \nimport numpy as np \nfrom google import genai \n \n# Load Gemini client \nclient = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\")) \n \n \n# ----------------------------- \n# Helper: Load vector store \n# ----------------------------- \ndef load_store(path): \n\n    if not os.path.exists(path): \n        return [] \n    with open(path, \"r\") as f: \n        return json.load(f) \n \n \n# ----------------------------- \n# Helper: Save vector store \n# ----------------------------- \ndef save_store(path, data): \n    with open(path, \"w\") as f: \n        json.dump(data, f, indent=4) \n \n \n# ----------------------------- \n# Embed text using Gemini \n# ----------------------------- \ndef embed_text(text: str): \n    \"\"\" \n    Uses Gemini embeddings. \n    Returns list[float]. \n    \"\"\" \n \n    result = client.models.embed_content( \n        model=\"text-embedding-004\", \n        contents=text, \n    ) \n \n    # Gemini wraps embeddings inside .output_embedding \n    return result.output_embedding \n \n \n# ----------------------------- \n# Add text to class vector store \n# ----------------------------- \ndef add_to_vector_store(class_id: str, text: str): \n    \"\"\" \n    Creates or appends to the vector store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    store = load_store(store_path) \n\n \n    embedding = embed_text(text) \n \n    store.append({ \n        \"text\": text, \n        \"embedding\": embedding, \n    }) \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Build store from scratch (optional) \n# ----------------------------- \ndef build_vector_store(class_id: str, text: str): \n    \"\"\" \n    Clears and rebuilds the entire store for a class. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    os.makedirs(os.path.join(\"upload\", class_id), exist_ok=True) \n \n    embedding = embed_text(text) \n \n    store = [ \n        { \n            \"text\": text, \n            \"embedding\": embedding, \n        } \n    ] \n \n    save_store(store_path, store) \n \n \n# ----------------------------- \n# Search RAG store using cosine similarity \n# ----------------------------- \ndef search_vector_store(class_id: str, query: str): \n    \"\"\" \n    Returns the most relevant text chunk for the given query. \n    \"\"\" \n \n    store_path = os.path.join(\"upload\", class_id, \"vs.json\") \n    store = load_store(store_path) \n\nHTML\n \n    if not store: \n        return None \n \n    query_embedding = embed_text(query) \n    query_vec = np.array(query_embedding) \n \n    best_score = -999 \n    best_text = None \n \n    for item in store: \n        vec = np.array(item[\"embedding\"]) \n \n        # cosine similarity \n        score = np.dot(query_vec, vec) / ( \n            np.linalg.norm(query_vec) * np.linalg.norm(vec) \n        ) \n \n        if score > best_score: \n            best_score = score \n            best_text = item[\"text\"] \n \n    return best_text \n \n \nTemplates \n1.\u200b index.html  \n \n{% extends \"base.html\" %} \n{% block content %} \n \n<h1>Helektron Study Assistant</h1> \n<p class=\"subtitle\"> \n    Upload lecture materials (PDF, slides, audio, notes), then generate \nsummaries, key terms, questions, and more. \n</p> \n \n<div class=\"layout\"> \n    <!-- LEFT: Upload + Materials --> \n    <div class=\"card\"> \n        <h2>1. Upload Materials</h2> \n\n \n        <div class=\"upload-box\"> \n            <p><strong>Upload files:</strong> .txt, .pdf, .pptx, .mp4, .m4a</p> \n            <form \n                hx-post=\"/upload\" \n                hx-target=\"#materials-panel\" \n                hx-indicator=\"#upload-loading\" \n                enctype=\"multipart/form-data\" \n            > \n                <input type=\"file\" name=\"file\" required> \n                <input type=\"hidden\" name=\"session_id\" id=\"session_id_input\"> \n                <br><br> \n                <button class=\"primary\" type=\"submit\">Upload</button> \n            </form> \n        </div> \n \n        <div class=\"upload-box\"> \n            <p><strong>Live Transcription:</strong> record audio and transcribe \nwith Whisper.</p> \n            <button class=\"secondary\" id=\"record-btn\">\ud83c\udf99 Start \nRecording</button> \n            <p id=\"record-status\" style=\"font-size: 12px; color:#555; \nmargin-top:6px;\"></p> \n        </div> \n \n        <div id=\"upload-loading\" class=\"htmx-indicator\"> \n            Uploading/processing material... \n        </div> \n \n        <div id=\"materials-panel\"> \n            <!-- filled by upload_status fragment --> \n            <div id=\"materials-list\"> \n                <em>No materials uploaded yet.</em> \n            </div> \n        </div> \n    </div> \n \n    <!-- RIGHT: Study Tools --> \n    <div class=\"card\"> \n        <h2>2. Generate Study Material</h2> \n \n        <p style=\"font-size:13px; color:#555; margin-top:0;\"> \n            Once you've uploaded all relevant materials, choose what you want \nthe AI to generate. \n\n        </p> \n \n        <div style=\"display:flex; gap:8px; flex-wrap:wrap; \nmargin-bottom:10px;\"> \n            <button class=\"primary\" \n                    id=\"btn-summary\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Summary \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-keyterms\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Key Terms \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-questions\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                Practice Questions \n            </button> \n            <button class=\"secondary\" \n                    id=\"btn-resources\" \n                    disabled \n                    hx-target=\"#results-panel\" \n                    hx-indicator=\"#generate-loading\"> \n                External Resources \n            </button> \n        </div> \n \n        <div id=\"generate-loading\" class=\"htmx-indicator\"> \n            Generating material... \n        </div> \n \n        <div id=\"results-panel\"> \n            <em>Your generated study content will appear here.</em> \n        </div> \n    </div> \n</div> \n \n\n<script> \nlet currentSessionId = null; \n \n// Called from upload_status fragment \nfunction setSessionId(id) { \n    currentSessionId = id; \n    document.getElementById(\"session_id_input\").value = id; \n    enableStudyButtons(); \n} \n \nfunction enableStudyButtons() { \n    if (!currentSessionId) return; \n \n    const summaryBtn = document.getElementById(\"btn-summary\"); \n    const keyBtn = document.getElementById(\"btn-keyterms\"); \n    const qBtn = document.getElementById(\"btn-questions\"); \n    const rBtn = document.getElementById(\"btn-resources\"); \n \n    summaryBtn.disabled = keyBtn.disabled = qBtn.disabled = rBtn.disabled = \nfalse; \n \n    summaryBtn.setAttribute(\"hx-get\", `/summary/${currentSessionId}`); \n    keyBtn.setAttribute(\"hx-get\", `/keyterms/${currentSessionId}`); \n    qBtn.setAttribute(\"hx-get\", `/questions/${currentSessionId}`); \n    rBtn.setAttribute(\"hx-get\", `/resources/${currentSessionId}`); \n} \n \n// -------- Live recording with MediaRecorder -------- \nlet mediaRecorder = null; \nlet recordedChunks = []; \n \nconst recordBtn = document.getElementById(\"record-btn\"); \nconst recordStatus = document.getElementById(\"record-status\"); \n \nrecordBtn.addEventListener(\"click\", async () => { \n    if (!mediaRecorder || mediaRecorder.state === \"inactive\") { \n        // Start recording \n        try { \n            const stream = await navigator.mediaDevices.getUserMedia({ audio: \ntrue }); \n            mediaRecorder = new MediaRecorder(stream); \n            recordedChunks = []; \n \n            mediaRecorder.ondataavailable = (e) => { \n\n                if (e.data.size > 0) recordedChunks.push(e.data); \n            }; \n \n            mediaRecorder.onstop = async () => { \n                const blob = new Blob(recordedChunks, { type: \"audio/webm\" }); \n                await uploadLiveAudio(blob); \n                stream.getTracks().forEach(t => t.stop()); \n                recordStatus.textContent = \"Recording finished. \nTranscribing...\"; \n            }; \n \n            mediaRecorder.start(); \n            recordBtn.textContent = \"\u23f9 Stop Recording\"; \n            recordStatus.textContent = \"Recording...\"; \n        } catch (err) { \n            console.error(err); \n            alert(\"Could not access microphone.\"); \n        } \n    } else if (mediaRecorder.state === \"recording\") { \n        // Stop recording \n        mediaRecorder.stop(); \n        recordBtn.textContent = \"\ud83c\udf99 Start Recording\"; \n    } \n}); \n \nasync function uploadLiveAudio(blob) { \n    const formData = new FormData(); \n    formData.append(\"audio\", blob, \"live_recording.webm\"); \n    if (currentSessionId) { \n        formData.append(\"session_id\", currentSessionId); \n    } \n \n    const resp = await fetch(\"/upload_live_audio\", { \n        method: \"POST\", \n        body: formData \n    }); \n \n    const html = await resp.text(); \n    document.getElementById(\"materials-panel\").innerHTML = html; \n    recordStatus.textContent = \"Live audio processed.\"; \n} \n</script> \n \n{% endblock %} \n\nHTML\nHTML\n \n2.\u200b base.html  \n \n<!DOCTYPE html> \n<html lang=\"en\"> \n<head> \n    <meta charset=\"UTF-8\"> \n    <title>Helektron Study Assistant</title> \n    <link rel=\"stylesheet\" href=\"/static/styles.css\"> \n    <script src=\"https://unpkg.com/htmx.org@1.9.10\"></script> \n</head> \n<body> \n<div class=\"app-shell\"> \n    {% block content %}{% endblock %} \n</div> \n</body> \n</html> \n \n3.\u200b Fragments folder \na.\u200b class_list.html  \n \n<ul> \n    {% for cls in classes %} \n    <li> \n        <button  \n            hx-post=\"/set_active_class\"  \n            hx-vals='{\"class_id\": \"{{ cls.id }}\"}' \n            hx-target=\"#upload-list\" \n            hx-swap=\"innerHTML\" \n            class=\"{% if cls.id == active_class_id %}active{% endif %}\" \n        > \n            {{ cls.name }} \n        </button> \n    </li> \n    {% endfor %} \n</ul> \n \nb.\u200b upload_status.html  \n \n\nHTML\nHTML\nHTML\n<h2>{{ title }}</h2> \n<div class=\"output-content\"> \n    {{ content | safe }} \n</div> \n \nc.\u200b upload_list.html  \n \n{% if files %} \n<ul> \n    {% for f in files %} \n    <li>{{ f.name }} ({{ f.size_kb }} KB)</li> \n    {% endfor %} \n</ul> \n{% else %} \n<p>No files uploaded yet.</p> \n{% endif %} \n \nd.\u200b output.html  \n \n<div id=\"materials-list\"> \n    {% if files %} \n    <ul> \n        {% for f in files %} \n        <li><strong>{{ f.name }}</strong> ({{ f.type }})</li> \n        {% endfor %} \n    </ul> \n    {% else %} \n    <em>No materials uploaded yet.</em> \n    {% endif %} \n</div> \n \n<script> \n    setSessionId(\"{{ session_id }}\"); \n</script> \n \n \n \n\nCSS\nStatic:  \n1.\u200b styles.css \n \nbody { \n    margin: 0; \n    font-family: system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", \nsans-serif; \n    background: #f4f4fb; \n} \n \n.app-shell { \n    max-width: 1100px; \n    margin: 0 auto; \n    padding: 24px; \n} \n \nh1 { \n    font-size: 28px; \n    margin-bottom: 8px; \n} \n \n.subtitle { \n    color: #555; \n    margin-bottom: 24px; \n} \n \n.layout { \n    display: grid; \n    grid-template-columns: 1.2fr 1.8fr; \n    gap: 20px; \n} \n \n.card { \n    background: white; \n    border-radius: 12px; \n    padding: 16px 20px; \n    box-shadow: 0 1px 3px rgba(15,23,42,0.08); \n    border: 1px solid #e2e8f0; \n} \n \n.card h2 { \n    font-size: 18px; \n    margin-top: 0; \n    margin-bottom: 12px; \n\n} \n \n.upload-box { \n    border: 2px dashed #cbd5f5; \n    border-radius: 10px; \n    padding: 16px; \n    text-align: center; \n    background: #f9fafb; \n    margin-bottom: 12px; \n} \n \n.upload-box input[type=\"file\"] { \n    margin-top: 8px; \n} \n \nbutton.primary { \n    background: #4f46e5; \n    color: white; \n    border: none; \n    border-radius: 8px; \n    padding: 10px 16px; \n    font-weight: 600; \n    cursor: pointer; \n    font-size: 14px; \n} \n \nbutton.primary:disabled { \n    opacity: 0.6; \n    cursor: not-allowed; \n} \n \nbutton.primary:hover:not(:disabled) { \n    background: #4338ca; \n} \n \nbutton.secondary { \n    background: white; \n    color: #4f46e5; \n    border: 1px solid #cbd5f5; \n    border-radius: 8px; \n    padding: 8px 14px; \n    font-weight: 500; \n    cursor: pointer; \n    font-size: 13px; \n\n} \n \nbutton.secondary:hover { \n    background: #eef2ff; \n} \n \n#materials-list { \n    font-size: 14px; \n    max-height: 200px; \n    overflow-y: auto; \n} \n \n#materials-list ul { \n    padding-left: 18px; \n} \n \n#results-panel { \n    min-height: 250px; \n    font-size: 14px; \n    white-space: pre-wrap; \n} \n \n/* htmx loading indicator */ \n.htmx-indicator { \n    display: none; \n    margin-bottom: 8px; \n    font-size: 13px; \n    color: #6b21a8; \n} \n \n.htmx-request .htmx-indicator { \n    display: block; \n} \n \nFix:  \n1.\u200b The following buttons do not work on panel 2. Generate Study Material: \na.\u200b Summary \nb.\u200b Key Terms \nc.\u200b Practice Questions \nd.\u200b External Resources \n2.\u200b  \n",
        "added_at": "2025-11-29T05:20:29.417745"
      }
    ]
  }
}